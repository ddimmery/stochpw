{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"stochpw - Permutation Weighting for Causal Inference","text":"<p>Permutation weighting learns importance weights for causal inference by training a discriminator to distinguish between observed treatment-covariate pairs and artificially permuted pairs.</p>"},{"location":"#installation","title":"Installation","text":"<pre><code>pip install stochpw\n</code></pre> <p>For development: <pre><code>git clone https://github.com/ddimmery/stochpw.git\ncd stochpw\nuv sync\n</code></pre></p>"},{"location":"#quick-start","title":"Quick Start","text":"<pre><code>import jax.numpy as jnp\nfrom stochpw import PermutationWeighter\n\n# Your observational data\nX = jnp.array(...)  # Covariates, shape (n_samples, n_features)\nA = jnp.array(...)  # Treatments, shape (n_samples,) or (n_samples, n_treatments)\n\n# Fit permutation weighter (sklearn-style API)\nweighter = PermutationWeighter(\n    num_epochs=100,\n    batch_size=256,\n    random_state=42\n)\nweighter.fit(X, A)\n\n# Predict importance weights\nweights = weighter.predict(X, A)\n\n# Use weights for downstream task\n# (tools for causal estimation not provided)\n# ate = weighted_estimator(Y, A, weights)\n</code></pre>"},{"location":"#how-it-works","title":"How It Works","text":"<p>Permutation weighting estimates density ratios by:</p> <ol> <li>Training a discriminator to distinguish:</li> <li>Permuted pairs: (X, A') with label C=1 (treatments shuffled)</li> <li> <p>Observed pairs: (X, A) with label C=0 (original data)</p> </li> <li> <p>Extracting weights from discriminator probabilities:    <pre><code>w(a, x) = \u03b7(a, x) / (1 - \u03b7(a, x))\n</code></pre>    where \u03b7(a, x) = p(C=1 | a, x)</p> </li> <li> <p>Using weights for balancing weights in causal effect estimation</p> </li> </ol>"},{"location":"#composable-design","title":"Composable Design","text":"<p>The package exposes low-level components for integration into larger models:</p> <pre><code>from stochpw import (\n    BaseDiscriminator,\n    LinearDiscriminator,\n    MLPDiscriminator,\n    create_training_batch,\n    logistic_loss,\n    extract_weights,\n)\n\n# Use in your custom architecture (e.g., DragonNet)\nbatch = create_training_batch(X, A, batch_indices, rng_key)\nlogits = my_discriminator(params, batch.A, batch.X, batch.AX)\nloss = logistic_loss(logits, batch.C)\n</code></pre>"},{"location":"#features","title":"Features","text":"<ul> <li>JAX-based: Fast, GPU-compatible, auto-differentiable</li> <li>Sklearn-style API: Familiar <code>.fit()</code> and <code>.predict()</code> interface</li> <li>Composable: All components exposed for integration</li> <li>Flexible: Supports binary, continuous, and multi-dimensional treatments</li> <li>Diagnostic tools: ESS, SMD, and balance checks included</li> </ul>"},{"location":"#references","title":"References","text":"<p>Arbour, D., Dimmery, D., &amp; Sondhi, A. (2021). Permutation Weighting. In Proceedings of the 38th International Conference on Machine Learning, PMLR 139:331-341.</p>"},{"location":"#license","title":"License","text":"<p>Apache-2.0 License - see LICENSE file for details.</p>"},{"location":"#contributing","title":"Contributing","text":"<p>Contributions are welcome! Please feel free to submit a Pull Request.</p>"},{"location":"#citation","title":"Citation","text":"<p>If you use this package, please cite the original paper:</p> <pre><code>@InProceedings{arbour21permutation,\n  title = {Permutation Weighting},\n  author = {Arbour, David and Dimmery, Drew and Sondhi, Arjun},\n  booktitle = {Proceedings of the 38th International Conference on Machine Learning},\n  pages = {331--341},\n  year = {2021},\n  editor = {Meila, Marina and Zhang, Tong},\n  volume = {139},\n  series = {Proceedings of Machine Learning Research},\n  month = {18--24 Jul},\n  publisher = {PMLR},\n  pdf = {http://proceedings.mlr.press/v139/arbour21a/arbour21a.pdf},\n  url = {https://proceedings.mlr.press/v139/arbour21a.html}\n}\n</code></pre>"},{"location":"about/","title":"About","text":""},{"location":"about/#about-the-package","title":"About the package","text":"<p>This package implements permutation weighting using stochastic gradient descent (SGD) with JAX.</p>"},{"location":"about/#what-is-permutation-weighting","title":"What is Permutation Weighting?","text":"<p>Permutation weighting is a method for learning importance weights for causal inference in observational studies. It was introduced in:</p> <p>Arbour, D., Dimmery, D., &amp; Sondhi, A. (2021). Permutation Weighting. In Proceedings of the 38th International Conference on Machine Learning, PMLR 139:331-341.</p> <p>The key insight is to train a discriminator to distinguish between observed treatment-covariate pairs (X, A) and artificially permuted pairs (X, A'). The discriminator probabilities are then transformed into importance weights that can be used for causal effect estimation.</p>"},{"location":"about/#implementation-approach","title":"Implementation Approach","text":"<p>This implementation uses:</p> <ul> <li>JAX for high-performance computing, automatic differentiation, and GPU acceleration</li> <li>Optax for flexible gradient-based optimization algorithms</li> <li>Sklearn-style API for familiar <code>.fit()</code> and <code>.predict()</code> interface</li> <li>Class-based discriminators allowing users to easily define custom architectures</li> <li>Mini-batch SGD enabling scalable training on large datasets</li> </ul>"},{"location":"about/#applications","title":"Applications","text":"<p>Permutation weighting is particularly useful for:</p> <ul> <li>Learning importance weights for causal effect estimation in observational studies</li> <li>Handling arbitrary treatment types (binary, continuous, or multivariate)</li> <li>Balancing treatment and covariate distributions without parametric assumptions</li> <li>Situations where traditional propensity score methods may be insufficient</li> <li>Large-scale causal inference problems requiring scalable training</li> </ul>"},{"location":"about/#key-features","title":"Key Features","text":"<ul> <li>Flexible discriminators: Linear and MLP architectures included, with easy extension to custom models</li> <li>Composable design: Low-level components exposed for integration into larger systems</li> <li>Diagnostic tools: Effective sample size (ESS) and standardized mean difference (SMD) for balance checking</li> <li>Type-safe: Full type annotations with pyright validation</li> <li>Well-tested: Comprehensive test suite with &gt;99% coverage</li> </ul>"},{"location":"about/#original-paper-authors","title":"Original Paper Authors","text":"<ul> <li>David Arbour</li> <li>Drew Dimmery</li> <li>Arjun Sondhi</li> </ul>"},{"location":"code-of-conduct/","title":"Contributor Covenant Code of Conduct","text":""},{"location":"code-of-conduct/#our-pledge","title":"Our Pledge","text":"<p>We as members, contributors, and leaders pledge to make participation in our community a harassment-free experience for everyone, regardless of age, body size, visible or invisible disability, ethnicity, sex characteristics, gender identity and expression, level of experience, education, socio-economic status, nationality, personal appearance, race, caste, color, religion, or sexual identity and orientation.</p> <p>We pledge to act and interact in ways that contribute to an open, welcoming, diverse, inclusive, and healthy community.</p>"},{"location":"code-of-conduct/#our-standards","title":"Our Standards","text":"<p>Examples of behavior that contributes to a positive environment for our community include:</p> <ul> <li>Demonstrating empathy and kindness toward other people</li> <li>Being respectful of differing opinions, viewpoints, and experiences</li> <li>Giving and gracefully accepting constructive feedback</li> <li>Accepting responsibility and apologizing to those affected by our mistakes,   and learning from the experience</li> <li>Focusing on what is best not just for us as individuals, but for the overall   community</li> </ul> <p>Examples of unacceptable behavior include:</p> <ul> <li>The use of sexualized language or imagery, and sexual attention or advances of   any kind</li> <li>Trolling, insulting or derogatory comments, and personal or political attacks</li> <li>Public or private harassment</li> <li>Publishing others' private information, such as a physical or email address,   without their explicit permission</li> <li>Other conduct which could reasonably be considered inappropriate in a   professional setting</li> </ul>"},{"location":"code-of-conduct/#enforcement-responsibilities","title":"Enforcement Responsibilities","text":"<p>Community leaders are responsible for clarifying and enforcing our standards of acceptable behavior and will take appropriate and fair corrective action in response to any behavior that they deem inappropriate, threatening, offensive, or harmful.</p> <p>Community leaders have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, and will communicate reasons for moderation decisions when appropriate.</p>"},{"location":"code-of-conduct/#scope","title":"Scope","text":"<p>This Code of Conduct applies within all community spaces, and also applies when an individual is officially representing the community in public spaces. Examples of representing our community include using an official email address, posting via an official social media account, or acting as an appointed representative at an online or offline event.</p>"},{"location":"code-of-conduct/#enforcement","title":"Enforcement","text":"<p>Instances of abusive, harassing, or otherwise unacceptable behavior may be reported to the community leaders responsible for enforcement at pypi@ddimmery.com. All complaints will be reviewed and investigated promptly and fairly.</p> <p>All community leaders are obligated to respect the privacy and security of the reporter of any incident.</p>"},{"location":"code-of-conduct/#enforcement-guidelines","title":"Enforcement Guidelines","text":"<p>Community leaders will follow these Community Impact Guidelines in determining the consequences for any action they deem in violation of this Code of Conduct:</p>"},{"location":"code-of-conduct/#1-correction","title":"1. Correction","text":"<p>Community Impact: Use of inappropriate language or other behavior deemed unprofessional or unwelcome in the community.</p> <p>Consequence: A private, written warning from community leaders, providing clarity around the nature of the violation and an explanation of why the behavior was inappropriate. A public apology may be requested.</p>"},{"location":"code-of-conduct/#2-warning","title":"2. Warning","text":"<p>Community Impact: A violation through a single incident or series of actions.</p> <p>Consequence: A warning with consequences for continued behavior. No interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, for a specified period of time. This includes avoiding interactions in community spaces as well as external channels like social media. Violating these terms may lead to a temporary or permanent ban.</p>"},{"location":"code-of-conduct/#3-temporary-ban","title":"3. Temporary Ban","text":"<p>Community Impact: A serious violation of community standards, including sustained inappropriate behavior.</p> <p>Consequence: A temporary ban from any sort of interaction or public communication with the community for a specified period of time. No public or private interaction with the people involved, including unsolicited interaction with those enforcing the Code of Conduct, is allowed during this period. Violating these terms may lead to a permanent ban.</p>"},{"location":"code-of-conduct/#4-permanent-ban","title":"4. Permanent Ban","text":"<p>Community Impact: Demonstrating a pattern of violation of community standards, including sustained inappropriate behavior, harassment of an individual, or aggression toward or disparagement of classes of individuals.</p> <p>Consequence: A permanent ban from any sort of public interaction within the community.</p>"},{"location":"code-of-conduct/#attribution","title":"Attribution","text":"<p>This Code of Conduct is adapted from the Contributor Covenant, version 2.1, available at https://www.contributor-covenant.org/version/2/1/code_of_conduct.html.</p> <p>Community Impact Guidelines were inspired by Mozilla's code of conduct enforcement ladder.</p> <p>For answers to common questions about this code of conduct, see the FAQ at https://www.contributor-covenant.org/faq. Translations are available at https://www.contributor-covenant.org/translations.</p>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing-to-this-project","title":"Contributing to this project","text":""},{"location":"contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>Before starting out, please take a look at our Code of Conduct. Participation means that you agree to engage constructively with the community as per the Code.</p>"},{"location":"contributing/#development","title":"Development","text":"<p>The first step is to clone this repository and install all dependencies. You can do this with uv:</p> <pre><code>uv sync\n</code></pre> <p>or just with <code>pip</code>: <pre><code>pip install .\n</code></pre></p> <p>Using uv is particularly nice because it will keep all dependencies in a virtual environment without confusing your local setup.</p> <p>A few tools are provided to make things easier. A basic <code>Makefile</code> provides the necessary commands to build the entire package and documentation. Running <code>make</code> will build everything necessary for local testing.</p>"},{"location":"contributing/#getting-started","title":"Getting Started","text":"<p>Contributions are made to this repo via Issues and Pull Requests (PRs), primarily the former.</p>"},{"location":"contributing/#issues","title":"Issues","text":"<p>Please try to provide a minimal reproducible example. If that isn't possible, explain as clearly and simply why that is, along with all of the relevant debugging steps you've already taken.</p>"},{"location":"contributing/#pull-requests-prs","title":"Pull Requests (PRs)","text":"<p>Since this is a stripped down implementation, it seems unlikely we will accept substantial PRs or feature requests. If you believe there is fundamental functionality that is missing, feel free to open an Issue and we can discuss.</p>"},{"location":"coverage/","title":"Coverage report","text":""},{"location":"examples/","title":"Examples","text":"<p>Comprehensive examples demonstrating stochpw's features and usage patterns.</p>"},{"location":"examples/#basic-usage","title":"Basic Usage","text":""},{"location":"examples/#basic-usage_1","title":"Basic Usage","text":"<p>Introduction to fitting permutation weighters and assessing balance improvement</p>"},{"location":"examples/#mlp-discriminator","title":"MLP Discriminator","text":"<p>Using multilayer perceptron discriminators for complex confounding patterns</p>"},{"location":"examples/#diagnostics-evaluation","title":"Diagnostics &amp; Evaluation","text":""},{"location":"examples/#comprehensive-diagnostics","title":"Comprehensive Diagnostics","text":"<p>Complete diagnostic workflow with ROC curves, calibration, and balance reports</p>"},{"location":"examples/#advanced-features","title":"Advanced Features","text":""},{"location":"examples/#advanced-features_1","title":"Advanced Features","text":"<p>Alternative loss functions, regularization, and early stopping</p>"},{"location":"examples/#lalonde-experiment","title":"Lalonde Experiment","text":"<p>Real-world causal inference on the classic Lalonde dataset</p>"},{"location":"examples/advanced_features/","title":"Advanced Features Demo","text":"<p>This example demonstrates advanced features in stochpw:</p> <ol> <li>Alternative loss functions (exponential, Brier)</li> <li>Weight-based regularization (entropy penalty)</li> <li>Early stopping</li> </ol> <pre><code>import time\n\nimport jax\nimport jax.numpy as jnp\n\nfrom stochpw import (\n    MLPDiscriminator,\n    PermutationWeighter,\n    brier_loss,\n    entropy_penalty,\n    exponential_loss,\n    standardized_mean_difference,\n)\n</code></pre>"},{"location":"examples/advanced_features/#generate-synthetic-data","title":"Generate Synthetic Data","text":"<pre><code>def generate_data(n: int = 1000, seed: int = 42):\n    \"\"\"Generate synthetic data with treatment-covariate confounding.\"\"\"\n    key = jax.random.PRNGKey(seed)\n    key1, key2, _key3 = jax.random.split(key, 3)\n\n    # Generate covariates\n    X = jax.random.normal(key1, (n, 5))\n\n    # Treatment depends on covariates (confounding)\n    propensity = jax.nn.sigmoid(X[:, 0] + 0.5 * X[:, 1])\n    A = (jax.random.uniform(key2, (n,)) &lt; propensity).astype(float)[:, None]\n\n    return X, A\n\n\nstart_time = time.time()\n\nprint(\"=\" * 60)\nprint(\"Advanced Features Demo\")\nprint(\"=\" * 60)\n\n# Generate data\nX, A = generate_data(n=1000, seed=42)\nprint(f\"\\nGenerated data: X.shape={X.shape}, A.shape={A.shape}\")\n\n# Compute initial imbalance\ninitial_smd = standardized_mean_difference(X, A, jnp.ones(X.shape[0]))\nprint(f\"Initial SMD: {jnp.max(jnp.abs(initial_smd)):.4f}\")\n</code></pre> <pre><code>============================================================\nAdvanced Features Demo\n============================================================\n\n\n\nGenerated data: X.shape=(1000, 5), A.shape=(1000, 1)\n\n\nInitial SMD: 0.8575\n</code></pre>"},{"location":"examples/advanced_features/#1-default-configuration-logistic-loss","title":"1. Default Configuration (Logistic Loss)","text":"<pre><code>print(\"\\n\" + \"=\" * 60)\nprint(\"1. Default Configuration (Logistic Loss)\")\nprint(\"=\" * 60)\n\nweighter_default = PermutationWeighter(\n    num_epochs=50,\n    batch_size=128,\n    random_state=42,\n)\n_ = weighter_default.fit(X, A)\nweights_default = weighter_default.predict(X, A)\nsmd_default = standardized_mean_difference(X, A, weights_default)\n\nprint(f\"Final SMD: {jnp.max(jnp.abs(smd_default)):.4f}\")\nassert weighter_default.history_ is not None\nprint(f\"Training epochs: {len(weighter_default.history_['loss'])}\")\nprint(f\"Final loss: {weighter_default.history_['loss'][-1]:.4f}\")\n</code></pre> <pre><code>============================================================\n1. Default Configuration (Logistic Loss)\n============================================================\n\n\nFinal SMD: 0.8955\nTraining epochs: 50\nFinal loss: 0.7840\n</code></pre>"},{"location":"examples/advanced_features/#2-alternative-loss-exponential-loss","title":"2. Alternative Loss: Exponential Loss","text":"<pre><code>print(\"\\n\" + \"=\" * 60)\nprint(\"2. Alternative Loss: Exponential Loss\")\nprint(\"=\" * 60)\n\nweighter_exp = PermutationWeighter(\n    loss_fn=exponential_loss,\n    num_epochs=50,\n    batch_size=128,\n    random_state=42,\n)\n_ = weighter_exp.fit(X, A)\nweights_exp = weighter_exp.predict(X, A)\nsmd_exp = standardized_mean_difference(X, A, weights_exp)\n\nprint(f\"Final SMD: {jnp.max(jnp.abs(smd_exp)):.4f}\")\nassert weighter_exp.history_ is not None\nprint(f\"Final loss: {weighter_exp.history_['loss'][-1]:.4f}\")\n</code></pre> <pre><code>============================================================\n2. Alternative Loss: Exponential Loss\n============================================================\n\n\nFinal SMD: 0.9809\nFinal loss: 1.7735\n</code></pre>"},{"location":"examples/advanced_features/#3-alternative-loss-brier-score","title":"3. Alternative Loss: Brier Score","text":"<pre><code>print(\"\\n\" + \"=\" * 60)\nprint(\"3. Alternative Loss: Brier Score\")\nprint(\"=\" * 60)\n\nweighter_brier = PermutationWeighter(\n    loss_fn=brier_loss,\n    num_epochs=50,\n    batch_size=128,\n    random_state=42,\n)\n_ = weighter_brier.fit(X, A)\nweights_brier = weighter_brier.predict(X, A)\nsmd_brier = standardized_mean_difference(X, A, weights_brier)\n\nprint(f\"Final SMD: {jnp.max(jnp.abs(smd_brier)):.4f}\")\nassert weighter_brier.history_ is not None\nprint(f\"Final loss: {weighter_brier.history_['loss'][-1]:.4f}\")\n</code></pre> <pre><code>============================================================\n3. Alternative Loss: Brier Score\n============================================================\n\n\nFinal SMD: 0.8714\nFinal loss: 0.2809\n</code></pre>"},{"location":"examples/advanced_features/#4-with-entropy-regularization","title":"4. With Entropy Regularization","text":"<pre><code>print(\"\\n\" + \"=\" * 60)\nprint(\"4. With Entropy Regularization\")\nprint(\"=\" * 60)\n\nmlp = MLPDiscriminator(hidden_dims=[64, 32])\nweighter_entropy_reg = PermutationWeighter(\n    discriminator=mlp,\n    regularization_fn=entropy_penalty,\n    regularization_strength=0.01,\n    num_epochs=50,\n    batch_size=128,\n    random_state=42,\n)\n_ = weighter_entropy_reg.fit(X, A)\nweights_entropy_reg = weighter_entropy_reg.predict(X, A)\nsmd_entropy_reg = standardized_mean_difference(X, A, weights_entropy_reg)\n\n# Compare weight entropy with and without regularization\nmlp_no_reg = MLPDiscriminator(hidden_dims=[64, 32])\nweighter_no_reg = PermutationWeighter(\n    discriminator=mlp_no_reg,\n    num_epochs=50,\n    batch_size=128,\n    random_state=42,\n)\n_ = weighter_no_reg.fit(X, A)\nweights_no_reg = weighter_no_reg.predict(X, A)\n\n# Compute negative entropy (penalty) for comparison\nentropy_with_reg = -entropy_penalty(weights_entropy_reg)\nentropy_without_reg = -entropy_penalty(weights_no_reg)\n\nprint(f\"Final SMD: {jnp.max(jnp.abs(smd_entropy_reg)):.4f}\")\nprint(f\"Weight entropy (with regularization): {entropy_with_reg:.2f}\")\nprint(f\"Weight entropy (without regularization): {entropy_without_reg:.2f}\")\nprint(\"Higher entropy = more uniform weights (better ESS)\")\n</code></pre> <pre><code>============================================================\n4. With Entropy Regularization\n============================================================\n\n\nFinal SMD: 0.1016\nWeight entropy (with regularization): 6.78\nWeight entropy (without regularization): 6.77\nHigher entropy = more uniform weights (better ESS)\n</code></pre>"},{"location":"examples/advanced_features/#5-with-early-stopping","title":"5. With Early Stopping","text":"<pre><code>print(\"\\n\" + \"=\" * 60)\nprint(\"5. With Early Stopping\")\nprint(\"=\" * 60)\n\nweighter_early = PermutationWeighter(\n    early_stopping=True,\n    patience=10,\n    min_delta=0.001,\n    num_epochs=200,  # Set high, but will stop early\n    batch_size=128,\n    random_state=42,\n)\n_ = weighter_early.fit(X, A)\nweights_early = weighter_early.predict(X, A)\nsmd_early = standardized_mean_difference(X, A, weights_early)\n\nprint(f\"Final SMD: {jnp.max(jnp.abs(smd_early)):.4f}\")\nassert weighter_early.history_ is not None\nprint(f\"Stopped at epoch: {len(weighter_early.history_['loss'])}/200\")\nprint(f\"Epochs saved: {200 - len(weighter_early.history_['loss'])}\")\n</code></pre> <pre><code>============================================================\n5. With Early Stopping\n============================================================\n\n\nFinal SMD: 0.6395\nStopped at epoch: 106/200\nEpochs saved: 94\n</code></pre>"},{"location":"examples/advanced_features/#6-all-features-combined","title":"6. All Features Combined","text":"<pre><code>print(\"\\n\" + \"=\" * 60)\nprint(\"6. All Features Combined\")\nprint(\"=\" * 60)\n\nmlp_combined = MLPDiscriminator(hidden_dims=[128, 64, 32], activation=\"tanh\")\nweighter_combined = PermutationWeighter(\n    discriminator=mlp_combined,\n    loss_fn=brier_loss,\n    regularization_fn=entropy_penalty,\n    regularization_strength=0.005,\n    early_stopping=True,\n    patience=15,\n    min_delta=0.001,\n    num_epochs=200,\n    batch_size=128,\n    random_state=42,\n)\n_ = weighter_combined.fit(X, A)\nweights_combined = weighter_combined.predict(X, A)\nsmd_combined = standardized_mean_difference(X, A, weights_combined)\n\nprint(f\"Final SMD: {jnp.max(jnp.abs(smd_combined)):.4f}\")\nassert weighter_combined.history_ is not None\nassert weighter_combined.params_ is not None\nprint(f\"Stopped at epoch: {len(weighter_combined.history_['loss'])}/200\")\nprint(f\"Final loss: {weighter_combined.history_['loss'][-1]:.4f}\")\nentropy_combined = -entropy_penalty(weights_combined)\nprint(f\"Weight entropy: {entropy_combined:.2f}\")\n</code></pre> <pre><code>============================================================\n6. All Features Combined\n============================================================\n\n\nFinal SMD: 0.1120\nStopped at epoch: 29/200\nFinal loss: 0.2120\nWeight entropy: 6.79\n</code></pre>"},{"location":"examples/advanced_features/#summary","title":"Summary","text":"<pre><code>print(\"\\n\" + \"=\" * 60)\nprint(\"Summary: Balance Improvement\")\nprint(\"=\" * 60)\nprint(f\"Initial:                    {jnp.max(jnp.abs(initial_smd)):.4f}\")\nprint(f\"Default (logistic):         {jnp.max(jnp.abs(smd_default)):.4f}\")\nprint(f\"Exponential loss:           {jnp.max(jnp.abs(smd_exp)):.4f}\")\nprint(f\"Brier loss:                 {jnp.max(jnp.abs(smd_brier)):.4f}\")\nprint(f\"With entropy regularization: {jnp.max(jnp.abs(smd_entropy_reg)):.4f}\")\nprint(f\"With early stopping:        {jnp.max(jnp.abs(smd_early)):.4f}\")\nprint(f\"All features combined:      {jnp.max(jnp.abs(smd_combined)):.4f}\")\n\nprint(\"\\n\" + \"=\" * 60)\nprint(\"\u2713 Example completed successfully!\")\nelapsed_time = time.time() - start_time\nprint(f\"\u23f1  Total execution time: {elapsed_time:.2f} seconds\")\nprint(\"=\" * 60)\n</code></pre> <pre><code>============================================================\nSummary: Balance Improvement\n============================================================\nInitial:                    0.8575\nDefault (logistic):         0.8955\nExponential loss:           0.9809\nBrier loss:                 0.8714\nWith entropy regularization: 0.1016\nWith early stopping:        0.6395\nAll features combined:      0.1120\n\n============================================================\n\u2713 Example completed successfully!\n\u23f1  Total execution time: 30.65 seconds\n============================================================\n</code></pre> <p>View source on GitHub</p>"},{"location":"examples/basic_usage/","title":"Basic Usage Example for stochpw","text":"<p>This example demonstrates the basic workflow of permutation weighting: 1. Generate synthetic confounded data 2. Fit a permutation weighter 3. Extract importance weights 4. Assess balance improvement</p> <pre><code>import time\n\nimport jax\nimport jax.numpy as jnp\nimport optax\n\nfrom stochpw import PermutationWeighter, effective_sample_size, standardized_mean_difference\n\nstart_time = time.time()\n</code></pre>"},{"location":"examples/basic_usage/#generate-synthetic-data-with-confounding","title":"Generate Synthetic Data with Confounding","text":"<pre><code># Generate synthetic observational data with confounding\nkey = jax.random.PRNGKey(420)\nn = 250\n\n# Generate confounders\nX_key, A_key = jax.random.split(key)\nX = jax.random.normal(X_key, (n, 5))  # 5 covariates\n\n# Treatment depends on covariates (confounding)\npropensity = jax.nn.sigmoid(0.5 * X[:, 0] - 0.3 * X[:, 1] + 0.2)\nA = jax.random.bernoulli(A_key, propensity, (n,)).astype(jnp.float32).reshape(-1, 1)\n\nprint(f\"Generated data: {n} samples, {X.shape[1]} covariates\")\nprint(f\"Treatment distribution: {A.mean():.2%} treated\")\n</code></pre> <pre><code>Generated data: 250 samples, 5 covariates\nTreatment distribution: 63.60% treated\n</code></pre>"},{"location":"examples/basic_usage/#fit-permutation-weighter","title":"Fit Permutation Weighter","text":"<pre><code>opt = optax.rmsprop(learning_rate=0.1)\nweighter = PermutationWeighter(num_epochs=20, batch_size=250 // 4, random_state=42, optimizer=opt)\n\n_ = weighter.fit(X, A)\nweights = weighter.predict(X, A)\n\nprint(\"Fitting complete!\")\n</code></pre> <pre><code>Fitting complete!\n</code></pre>"},{"location":"examples/basic_usage/#weight-statistics","title":"Weight Statistics","text":"<pre><code>print(\"Weight statistics:\")\nprint(f\"  Range: [{weights.min():.3f}, {weights.max():.3f}]\")\nprint(f\"  Mean: {weights.mean():.3f}\")\nprint(f\"  Std: {weights.std():.3f}\")\n</code></pre> <pre><code>Weight statistics:\n  Range: [0.181, 3.177]\n  Mean: 0.896\n  Std: 0.369\n</code></pre>"},{"location":"examples/basic_usage/#effective-sample-size","title":"Effective Sample Size","text":"<pre><code>ess = effective_sample_size(weights)\ness_ratio = ess / len(weights)\nprint(\"Effective sample size:\")\nprint(f\"  ESS: {ess:.1f} / {len(weights)} ({ess_ratio:.1%})\")\n</code></pre> <pre><code>Effective sample size:\n  ESS: 213.8 / 250 (85.5%)\n</code></pre>"},{"location":"examples/basic_usage/#balance-assessment","title":"Balance Assessment","text":"<pre><code>smd_unweighted = standardized_mean_difference(X, A, jnp.ones_like(weights))\nsmd_weighted = standardized_mean_difference(X, A, weights)\n\nprint(\"Standardized Mean Difference (SMD):\")\nprint(f\"  Max |SMD| (unweighted): {jnp.abs(smd_unweighted).max():.3f}\")\nprint(f\"  Max |SMD| (weighted):   {jnp.abs(smd_weighted).max():.3f}\")\n\nmax_smd_unweighted = jnp.abs(smd_unweighted).max()\nmax_smd_weighted = jnp.abs(smd_weighted).max()\nimprovement = (1 - max_smd_weighted / max_smd_unweighted) * 100\nprint(f\"  Improvement: {improvement:.1f}%\")\n</code></pre> <pre><code>Standardized Mean Difference (SMD):\n  Max |SMD| (unweighted): 0.542\n  Max |SMD| (weighted):   0.086\n  Improvement: 84.1%\n</code></pre>"},{"location":"examples/basic_usage/#training-history","title":"Training History","text":"<pre><code>assert weighter.history_ is not None  # Guaranteed after fit()\nloss_history = weighter.history_[\"loss\"]\ninitial_loss = loss_history[0]\nfinal_loss = loss_history[-1]\n\nprint(\"Training history:\")\nprint(f\"  Initial loss: {initial_loss:.4f}\")\nprint(f\"  Final loss: {final_loss:.4f}\")\nprint(f\"  Loss reduction: {(initial_loss - final_loss) / initial_loss * 100:.1f}%\")\nprint(f\"  Epochs: {len(loss_history)}\")\nprint(f\"\\n  First 10 losses: {[f'{loss:.2f}' for loss in loss_history[:10]]}\")\nprint(f\"  Last 10 losses: {[f'{loss:.2f}' for loss in loss_history[-10:]]}\")\n</code></pre> <pre><code>Training history:\n  Initial loss: 0.8255\n  Final loss: 0.6854\n  Loss reduction: 17.0%\n  Epochs: 20\n\n  First 10 losses: ['0.83', '0.72', '0.69', '0.68', '0.70', '0.68', '0.68', '0.69', '0.70', '0.69']\n  Last 10 losses: ['0.68', '0.69', '0.69', '0.70', '0.70', '0.69', '0.70', '0.69', '0.69', '0.69']\n</code></pre>"},{"location":"examples/basic_usage/#summary","title":"Summary","text":"<pre><code>elapsed_time = time.time() - start_time\nprint(\"\u2713 Example completed successfully!\")\nprint(f\"\u23f1  Total execution time: {elapsed_time:.2f} seconds\")\n</code></pre> <pre><code>\u2713 Example completed successfully!\n\u23f1  Total execution time: 5.83 seconds\n</code></pre> <p>View source on GitHub</p>"},{"location":"examples/diagnostics_demo/","title":"Comprehensive Diagnostics Demo","text":"<p>This example demonstrates comprehensive diagnostics in stochpw:</p> <ol> <li>Balance reports</li> <li>Weight statistics</li> <li>ROC curves (most important discriminator diagnostic)</li> <li>Calibration curves</li> <li>Visualization with plotnine</li> </ol> <pre><code>import time\n\nimport jax\nimport jax.numpy as jnp\nimport optax\n\nfrom stochpw import (\n    PermutationWeighter,\n    balance_report,\n    calibration_curve,\n    roc_curve,\n    standardized_mean_difference,\n    weight_statistics,\n)\nfrom stochpw.plotting import (\n    plot_balance_diagnostics,\n    plot_calibration_curve,\n    plot_roc_curve,\n    plot_weight_distribution,\n)\n</code></pre>"},{"location":"examples/diagnostics_demo/#generate-data-with-confounding","title":"Generate Data with Confounding","text":"<pre><code>def generate_confounded_data(n: int = 1000, seed: int = 42):\n    \"\"\"Generate synthetic data with treatment-covariate confounding.\"\"\"\n    key = jax.random.PRNGKey(seed)\n    key1, key2, _key3 = jax.random.split(key, 3)\n\n    # Generate covariates\n    X = jax.random.normal(key1, (n, 5))\n\n    # Treatment depends strongly on first two covariates (confounding)\n    propensity = jax.nn.sigmoid(1.5 * X[:, 0] + X[:, 1] - 0.5)\n    A = (jax.random.uniform(key2, (n,)) &lt; propensity).astype(float)\n\n    return X, A\n\n\nstart_time = time.time()\n\nprint(\"=\" * 70)\nprint(\"Comprehensive Diagnostics Demo\")\nprint(\"=\" * 70)\n\n# Generate data\nX, A = generate_confounded_data(n=1000, seed=42)\nprint(f\"\\nGenerated data: X.shape={X.shape}, A.shape={A.shape}\")\nprint(f\"Treatment balance: {jnp.mean(A):.2%} treated\")\n</code></pre> <pre><code>======================================================================\nComprehensive Diagnostics Demo\n======================================================================\n\n\n\nGenerated data: X.shape=(1000, 5), A.shape=(1000,)\nTreatment balance: 44.10% treated\n</code></pre>"},{"location":"examples/diagnostics_demo/#step-1-initial-balance-assessment","title":"Step 1: Initial Balance Assessment","text":"<pre><code>print(\"\\n\" + \"=\" * 70)\nprint(\"Step 1: Initial Balance Assessment\")\nprint(\"=\" * 70)\n\nuniform_weights = jnp.ones(X.shape[0])\ninitial_smd = standardized_mean_difference(X, A, uniform_weights)\nprint(f\"\\nInitial max SMD: {jnp.max(jnp.abs(initial_smd)):.4f}\")\nprint(f\"Initial mean SMD: {jnp.mean(jnp.abs(initial_smd)):.4f}\")\n\n# Get full balance report\ninitial_report = balance_report(X, A, uniform_weights)\nprint(f\"\\nTreatment type: {initial_report['treatment_type']}\")\nprint(f\"Number of features: {initial_report['n_features']}\")\nprint(f\"Number of samples: {initial_report['n_samples']}\")\n</code></pre> <pre><code>======================================================================\nStep 1: Initial Balance Assessment\n======================================================================\n\n\n\nInitial max SMD: 1.1802\nInitial mean SMD: 0.4308\n\n\n\nTreatment type: binary\nNumber of features: 5\nNumber of samples: 1000\n</code></pre>"},{"location":"examples/diagnostics_demo/#step-2-fit-permutation-weighter","title":"Step 2: Fit Permutation Weighter","text":"<pre><code>print(\"\\n\" + \"=\" * 70)\nprint(\"Step 2: Fit Permutation Weighter\")\nprint(\"=\" * 70)\n\nopt = optax.rmsprop(learning_rate=0.1)\nweighter = PermutationWeighter(num_epochs=50, batch_size=250, random_state=42, optimizer=opt)\n\n_ = weighter.fit(X, A)\nweights = weighter.predict(X, A)\n\nassert weighter.history_ is not None\nprint(f\"\\nTraining completed in {len(weighter.history_['loss'])} epochs\")\nprint(f\"Final training loss: {weighter.history_['loss'][-1]:.4f}\")\n</code></pre> <pre><code>======================================================================\nStep 2: Fit Permutation Weighter\n======================================================================\n\n\n\nTraining completed in 50 epochs\nFinal training loss: 0.6573\n</code></pre>"},{"location":"examples/diagnostics_demo/#step-3-balance-after-weighting","title":"Step 3: Balance After Weighting","text":"<pre><code>print(\"\\n\" + \"=\" * 70)\nprint(\"Step 3: Balance After Weighting\")\nprint(\"=\" * 70)\n\nfinal_smd = standardized_mean_difference(X, A, weights)\nprint(f\"\\nFinal max SMD: {jnp.max(jnp.abs(final_smd)):.4f}\")\nprint(f\"Final mean SMD: {jnp.mean(jnp.abs(final_smd)):.4f}\")\nsmd_improvement = (1 - jnp.max(jnp.abs(final_smd)) / jnp.max(jnp.abs(initial_smd))) * 100\nprint(f\"SMD improvement: {smd_improvement:.1f}%\")\n\n# Get comprehensive balance report\nfinal_report = balance_report(X, A, weights)\n\nprint(f\"\\nEffective Sample Size: {final_report['ess']:.0f} / {final_report['n_samples']}\")\nprint(f\"ESS Ratio: {final_report['ess_ratio']:.2%}\")\n</code></pre> <pre><code>======================================================================\nStep 3: Balance After Weighting\n======================================================================\n\nFinal max SMD: 0.4051\nFinal mean SMD: 0.1781\nSMD improvement: 65.7%\n\nEffective Sample Size: 724 / 1000\nESS Ratio: 72.38%\n</code></pre>"},{"location":"examples/diagnostics_demo/#step-4-weight-distribution-analysis","title":"Step 4: Weight Distribution Analysis","text":"<pre><code>print(\"\\n\" + \"=\" * 70)\nprint(\"Step 4: Weight Distribution Analysis\")\nprint(\"=\" * 70)\n\nw_stats = weight_statistics(weights)\n\nprint(\"\\nWeight Statistics:\")\nprint(f\"  Mean: {w_stats['mean']:.3f}\")\nprint(f\"  Std: {w_stats['std']:.3f}\")\nprint(f\"  Min: {w_stats['min']:.3f}\")\nprint(f\"  Max: {w_stats['max']:.3f}\")\nprint(f\"  CV (std/mean): {w_stats['cv']:.3f}\")\nprint(f\"  Max/Min ratio: {w_stats['max_ratio']:.1f}\")\nprint(f\"  Entropy: {w_stats['entropy']:.3f}\")\nprint(f\"  N extreme (&gt;10x mean): {w_stats['n_extreme']}\")\n</code></pre> <pre><code>======================================================================\nStep 4: Weight Distribution Analysis\n======================================================================\n\nWeight Statistics:\n  Mean: 0.896\n  Std: 0.554\n  Min: 0.054\n  Max: 4.255\n  CV (std/mean): 0.618\n  Max/Min ratio: 79.5\n  Entropy: 6.741\n  N extreme (&gt;10x mean): 0\n</code></pre>"},{"location":"examples/diagnostics_demo/#step-5-roc-curve-analysis","title":"Step 5: ROC Curve Analysis","text":"<p>ROC curve is the most important discriminator diagnostic.</p> <pre><code>print(\"\\n\" + \"=\" * 70)\nprint(\"Step 5: ROC Curve Analysis\")\nprint(\"=\" * 70)\n\n# Create permuted data for ROC analysis\nkey = jax.random.PRNGKey(123)\nA_perm = A[jax.random.permutation(key, len(A))]\n\n# Get weights for both observed and permuted data\nweights_obs = weights\nweights_perm = weighter.predict(X, A_perm)\n\n# Combine weights and create labels (0=observed, 1=permuted)\nall_weights = jnp.concatenate([weights_obs, weights_perm])\nall_labels = jnp.concatenate([jnp.zeros(len(weights_obs)), jnp.ones(len(weights_perm))])\n\n# Compute ROC curve\nfpr, tpr, thresholds = roc_curve(all_weights, all_labels)\n# Compute AUC using trapezoidal rule\nauc = float(jnp.trapezoid(tpr, fpr))\n\nprint(f\"\\nROC AUC: {auc:.4f}\")\nprint(\n    \"\\nInterpretation: AUC measures discriminator's ability to distinguish observed from permuted.\"\n)\nprint(\"  AUC = 0.5: Random guessing (poor discriminator)\")\nprint(\"  AUC = 1.0: Perfect discrimination\")\nprint(f\"  Current AUC = {auc:.4f}: \", end=\"\")\nif auc &gt; 0.9:\n    print(\"Excellent discriminator quality\")\nelif auc &gt; 0.8:\n    print(\"Good discriminator quality\")\nelif auc &gt; 0.7:\n    print(\"Moderate discriminator quality\")\nelse:\n    print(\"Poor discriminator quality - consider more epochs or larger model\")\n</code></pre> <pre><code>======================================================================\nStep 5: ROC Curve Analysis\n======================================================================\n\n\n\nROC AUC: 0.6853\n\nInterpretation: AUC measures discriminator's ability to distinguish observed from permuted.\n  AUC = 0.5: Random guessing (poor discriminator)\n  AUC = 1.0: Perfect discrimination\n  Current AUC = 0.6853: Poor discriminator quality - consider more epochs or larger model\n</code></pre>"},{"location":"examples/diagnostics_demo/#step-6-discriminator-calibration","title":"Step 6: Discriminator Calibration","text":"<pre><code>print(\"\\n\" + \"=\" * 70)\nprint(\"Step 6: Discriminator Calibration\")\nprint(\"=\" * 70)\n\n# Generate discriminator predictions on training data\nassert weighter.params_ is not None\nAX = jnp.einsum(\"bi,bj-&gt;bij\", A[:, None] if A.ndim == 1 else A, X).reshape(X.shape[0], -1)\nlogits = weighter.discriminator.apply(weighter.params_, A[:, None] if A.ndim == 1 else A, X, AX)\nprobs = jax.nn.sigmoid(logits)\n\n# Use same permuted data from ROC analysis\nA_perm_reshaped = A_perm[:, None] if A_perm.ndim == 1 else A_perm\nAX_perm = jnp.einsum(\"bi,bj-&gt;bij\", A_perm_reshaped, X).reshape(X.shape[0], -1)\nlogits_perm = weighter.discriminator.apply(weighter.params_, A_perm_reshaped, X, AX_perm)\nprobs_perm = jax.nn.sigmoid(logits_perm)\n\n# Combine for calibration analysis\nall_probs = jnp.concatenate([probs, probs_perm])\ncal_labels = jnp.concatenate([jnp.zeros(len(probs)), jnp.ones(len(probs_perm))])\n\nbin_centers, true_freqs, counts = calibration_curve(all_probs, cal_labels, num_bins=10)\n\nprint(\"\\nCalibration Analysis (10 bins):\")\nprint(f\"{'Predicted':&lt;12} {'Observed':&lt;12} {'Count':&lt;10} {'Error':&lt;10}\")\nprint(\"-\" * 44)\nfor pred, obs, count in zip(bin_centers, true_freqs, counts):\n    if count &gt; 0:\n        error = abs(pred - obs)\n        print(f\"{pred:&gt;10.3f}   {obs:&gt;10.3f}   {int(count):&gt;8}   {error:&gt;8.3f}\")\n</code></pre> <pre><code>======================================================================\nStep 6: Discriminator Calibration\n======================================================================\n\n\n\nCalibration Analysis (10 bins):\nPredicted    Observed     Count      Error     \n--------------------------------------------\n     0.050        0.375          8      0.325\n     0.150        0.333         63      0.183\n     0.250        0.311        180      0.061\n     0.350        0.358        344      0.008\n     0.450        0.421        518      0.029\n     0.550        0.535        400      0.015\n     0.650        0.687        297      0.037\n     0.750        0.808        146      0.058\n     0.850        0.970         33      0.120\n     0.950        1.000         11      0.050\n</code></pre>"},{"location":"examples/diagnostics_demo/#step-7-beforeafter-comparison","title":"Step 7: Before/After Comparison","text":"<pre><code>print(\"\\n\" + \"=\" * 70)\nprint(\"Step 7: Before/After Comparison\")\nprint(\"=\" * 70)\n\nprint(f\"\\n{'Metric':&lt;30} {'Before':&lt;15} {'After':&lt;15} {'Improvement':&lt;15}\")\nprint(\"-\" * 75)\n# Type ignore needed because balance_report returns a union type\nmax_smd_imp = (1 - float(final_report[\"max_smd\"]) / float(initial_report[\"max_smd\"])) * 100  # type: ignore[arg-type]\nmean_smd_imp = (1 - float(final_report[\"mean_smd\"]) / float(initial_report[\"mean_smd\"])) * 100  # type: ignore[arg-type]\ness_change = (float(final_report[\"ess\"]) / float(initial_report[\"ess\"]) - 1) * 100  # type: ignore[arg-type]\nprint(\n    f\"{'Max SMD':&lt;30} {initial_report['max_smd']:&gt;13.4f}  \"\n    + f\"{final_report['max_smd']:&gt;13.4f}  {max_smd_imp:&gt;12.1f}%\"\n)\nprint(\n    f\"{'Mean SMD':&lt;30} {initial_report['mean_smd']:&gt;13.4f}  \"\n    + f\"{final_report['mean_smd']:&gt;13.4f}  {mean_smd_imp:&gt;12.1f}%\"\n)\nprint(\n    f\"{'ESS':&lt;30} {initial_report['ess']:&gt;13.0f}  \"\n    + f\"{final_report['ess']:&gt;13.0f}  {ess_change:&gt;12.1f}%\"\n)\nprint(\n    f\"{'ESS Ratio':&lt;30} {initial_report['ess_ratio']:&gt;13.2%}  \"\n    + f\"{final_report['ess_ratio']:&gt;13.2%}  {'-':&gt;15}\"\n)\n</code></pre> <pre><code>======================================================================\nStep 7: Before/After Comparison\n======================================================================\n\nMetric                         Before          After           Improvement    \n---------------------------------------------------------------------------\nMax SMD                               1.1802         0.4051          65.7%\nMean SMD                              0.4308         0.1781          58.7%\nESS                                     1000            724         -27.6%\nESS Ratio                            100.00%         72.38%                -\n</code></pre>"},{"location":"examples/diagnostics_demo/#step-8-create-visualizations","title":"Step 8: Create Visualizations","text":"<pre><code>print(\"\\n\" + \"=\" * 70)\nprint(\"Step 8: Creating Visualizations\")\nprint(\"=\" * 70)\n\n# Plot ROC curve (most important!)\nplot_roc = plot_roc_curve(fpr, tpr, auc)\nplot_roc.save(\"roc_curve.png\", dpi=150, width=8, height=8)\nprint(\"\\n\u2713 Saved: roc_curve.png (MOST IMPORTANT DIAGNOSTIC)\")\n\n# Plot balance diagnostics with standard errors\nplot_balance = plot_balance_diagnostics(\n    X, A, weights, feature_names=[f\"X{i}\" for i in range(X.shape[1])]\n)\nplot_balance.save(\"balance_diagnostics.png\", dpi=150, width=10, height=6)\nprint(\"\u2713 Saved: balance_diagnostics.png (with 95% confidence intervals)\")\n\n# Plot weight distribution\nplot_weights = plot_weight_distribution(weights)\nplot_weights.save(\"weight_distribution.png\", dpi=150, width=8, height=6)\nprint(\"\u2713 Saved: weight_distribution.png\")\n\n# Plot calibration\nplot_cal = plot_calibration_curve(bin_centers, true_freqs, counts)\nplot_cal.save(\"calibration_curve.png\", dpi=150, width=8, height=8)\nprint(\"\u2713 Saved: calibration_curve.png\")\n\nprint(\"\\nVisualization files saved to current directory\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"Demo Complete!\")\nelapsed_time = time.time() - start_time\nprint(f\"Total execution time: {elapsed_time:.2f} seconds\")\nprint(\"=\" * 70)\n</code></pre> <pre><code>======================================================================\nStep 8: Creating Visualizations\n======================================================================\n\n\n/Users/drewd/GitHub/_packages/stochpw/.venv/lib/python3.12/site-packages/plotnine/ggplot.py:623: PlotnineWarning: Saving 8 x 8 in image.\n/Users/drewd/GitHub/_packages/stochpw/.venv/lib/python3.12/site-packages/plotnine/ggplot.py:624: PlotnineWarning: Filename: roc_curve.png\n\n\n\n\u2713 Saved: roc_curve.png (MOST IMPORTANT DIAGNOSTIC)\n\n\n/Users/drewd/GitHub/_packages/stochpw/.venv/lib/python3.12/site-packages/plotnine/ggplot.py:623: PlotnineWarning: Saving 10 x 6 in image.\n/Users/drewd/GitHub/_packages/stochpw/.venv/lib/python3.12/site-packages/plotnine/ggplot.py:624: PlotnineWarning: Filename: balance_diagnostics.png\n\n\n\u2713 Saved: balance_diagnostics.png (with 95% confidence intervals)\n\u2713 Saved: weight_distribution.png\n\n\n/Users/drewd/GitHub/_packages/stochpw/.venv/lib/python3.12/site-packages/plotnine/ggplot.py:623: PlotnineWarning: Saving 8 x 6 in image.\n/Users/drewd/GitHub/_packages/stochpw/.venv/lib/python3.12/site-packages/plotnine/ggplot.py:624: PlotnineWarning: Filename: weight_distribution.png\n\n\n\u2713 Saved: calibration_curve.png\n\nVisualization files saved to current directory\n\n======================================================================\nDemo Complete!\nTotal execution time: 16.03 seconds\n======================================================================\n\n\n/Users/drewd/GitHub/_packages/stochpw/.venv/lib/python3.12/site-packages/plotnine/ggplot.py:623: PlotnineWarning: Saving 8 x 8 in image.\n/Users/drewd/GitHub/_packages/stochpw/.venv/lib/python3.12/site-packages/plotnine/ggplot.py:624: PlotnineWarning: Filename: calibration_curve.png\n</code></pre> <p>View source on GitHub</p>"},{"location":"examples/lalonde_experiment/","title":"Lalonde Experiment: Permutation Weighting for ATE Estimation","text":"<p>This example demonstrates using permutation weighting on the classic Lalonde (1986) observational dataset to estimate the average treatment effect (ATE) of a job training program on earnings.</p> <p>The dataset combines experimental treatment units from the NSW program with non-experimental control units, creating confounding and selection bias that must be addressed to recover the experimental benchmark ATE of $1,794.</p> <p>Reference: LaLonde, R. J. (1986). \"Evaluating the Econometric Evaluations of Training Programs with Experimental Data\". The American Economic Review, 76(4), 604-620.</p> <pre><code>import time\nfrom pathlib import Path\nfrom typing import TypedDict\n\nimport jax.numpy as jnp\nimport numpy as np\nfrom jax import Array\n\nfrom stochpw import (\n    MLPDiscriminator,\n    PermutationWeighter,\n    effective_sample_size,\n    standardized_mean_difference,\n)\n\n\nclass LalondeData(TypedDict):\n    \"\"\"Type definition for Lalonde dataset.\"\"\"\n\n    X: Array\n    A: Array\n    Y: Array\n    feature_names: list[str]\n    ate_benchmark: float\n</code></pre>"},{"location":"examples/lalonde_experiment/#helper-functions","title":"Helper Functions","text":"<pre><code>def load_lalonde_nsw() -&gt; LalondeData:\n    \"\"\"\n    Load the Lalonde NSW (National Supported Work) observational dataset.\n\n    This dataset contains observational (non-experimental) data from the LaLonde study,\n    where NSW experimental treatment units are combined with non-experimental control\n    units, creating confounding and selection bias.\n\n    The dataset contains:\n    - Treatment: participation in job training program (1=treated, 0=control)\n    - Outcome: real earnings in 1978 (RE78)\n    - Covariates: age, education, race, marital status, pre-treatment earnings\n\n    Returns:\n        dict: Dictionary with keys:\n            - X: Covariates array, shape (n, d_x)\n            - A: Treatment array, shape (n, 1)\n            - Y: Outcome (earnings) array, shape (n, 1)\n            - feature_names: List of covariate names\n            - ate_benchmark: Experimental ATE estimate from RCT ($1,794)\n    \"\"\"\n    # Find the data file - try multiple locations to handle different execution contexts\n    # In notebooks, __file__ may not be defined, so use current working directory\n    try:\n        current_dir = Path(__file__).parent\n    except NameError:\n        # Running in notebook context, use current working directory\n        current_dir = Path.cwd()\n\n    # Try several possible locations\n    possible_paths = [\n        current_dir / \"nsw_data.csv\",  # Same directory as script or notebook\n        current_dir / \"examples\" / \"nsw_data.csv\",  # When run from project root\n        Path(\"examples\") / \"nsw_data.csv\",  # Relative to project root\n    ]\n\n    data_file = None\n    for path in possible_paths:\n        if path.exists():\n            data_file = path\n            break\n\n    if data_file is None:\n        raise FileNotFoundError(\n            \"Data file not found. Tried:\\n\"\n            + \"\\n\".join(f\"  - {p}\" for p in possible_paths)\n            + \"\\n\\nPlease ensure nsw_data.csv is in the examples/ directory.\"\n        )\n\n    # Load data\n    data = np.genfromtxt(data_file, delimiter=\",\", skip_header=1)\n\n    # Extract treatment, outcome, and covariates\n    A = data[:, 0]  # Treatment indicator (first column)\n    Y = data[:, -1]  # RE78 earnings (last column)\n    X = data[:, 1:9]  # All other columns are covariates\n\n    # Feature names (from dataset documentation)\n    feature_names = [\n        \"age\",\n        \"education\",\n        \"black\",\n        \"hispanic\",\n        \"married\",\n        \"nodegree\",\n        \"RE74\",  # earnings in 1974\n        \"RE75\",  # earnings in 1975\n    ]\n\n    # Experimental ATE benchmark (from LaLonde 1986 paper)\n    # This is the \"true\" ATE estimated from the randomized experiment\n    ate_benchmark = 1794.0\n\n    return {\n        \"X\": jnp.array(X),\n        \"A\": jnp.array(A),\n        \"Y\": jnp.array(Y),\n        \"feature_names\": feature_names,\n        \"ate_benchmark\": ate_benchmark,\n    }\n\n\ndef estimate_ate(Y: Array, A: Array, weights: Array) -&gt; float:\n    \"\"\"\n    Estimate the average treatment effect (ATE) using weighted means.\n\n    ATE = E[Y(1) - Y(0)] = E[Y|A=1] - E[Y|A=0]\n\n    Args:\n        Y: Outcome array, shape (n, 1) or (n,)\n        A: Treatment array, shape (n, 1) or (n,)\n        weights: Importance weights, shape (n,)\n\n    Returns:\n        float: Estimated ATE\n    \"\"\"\n    y_flat = Y.flatten()\n    a_flat = A.flatten()\n\n    treated_mask = a_flat == 1\n    control_mask = a_flat == 0\n\n    # Weighted mean for treated\n    weighted_y1 = jnp.sum(y_flat[treated_mask] * weights[treated_mask])\n    weighted_n1 = jnp.sum(weights[treated_mask])\n    mean_y1 = weighted_y1 / weighted_n1\n\n    # Weighted mean for control\n    weighted_y0 = jnp.sum(y_flat[control_mask] * weights[control_mask])\n    weighted_n0 = jnp.sum(weights[control_mask])\n    mean_y0 = weighted_y0 / weighted_n0\n\n    ate = mean_y1 - mean_y0\n    return float(ate)\n</code></pre>"},{"location":"examples/lalonde_experiment/#load-dataset","title":"Load Dataset","text":"<pre><code>start_time = time.time()\n\nprint(\"=\" * 70)\nprint(\"Lalonde Experiment: Permutation Weighting for ATE Estimation\")\nprint(\"=\" * 70)\n\n# Load observational data\nprint(\"\\nLoading Lalonde NSW observational dataset...\")\ndata = load_lalonde_nsw()\nX, A, Y = data[\"X\"], data[\"A\"], data[\"Y\"]\nfeature_names = data[\"feature_names\"]\nate_benchmark = data[\"ate_benchmark\"]\n\nn_treated = int(A.sum())\nn_control = len(A) - n_treated\n\nprint(\"\\nDataset statistics:\")\nprint(f\"  Total samples: {len(X)}\")\nprint(f\"  Treated: {n_treated}\")\nprint(f\"  Control: {n_control}\")\nprint(f\"  Covariates: {X.shape[1]}\")\nprint(f\"  Covariate names: {', '.join(feature_names)}\")\n</code></pre> <pre><code>======================================================================\nLalonde Experiment: Permutation Weighting for ATE Estimation\n======================================================================\n\nLoading Lalonde NSW observational dataset...\n\nDataset statistics:\n  Total samples: 458\n  Treated: 177\n  Control: 281\n  Covariates: 8\n  Covariate names: age, education, black, hispanic, married, nodegree, RE74, RE75\n</code></pre>"},{"location":"examples/lalonde_experiment/#experimental-benchmark-ground-truth","title":"Experimental Benchmark (Ground Truth)","text":"<pre><code>print(f\"\\n{'=' * 70}\")\nprint(\"Experimental Benchmark (Ground Truth)\")\nprint(f\"{'=' * 70}\")\nprint(f\"  Experimental ATE: ${ate_benchmark:.2f}\")\nprint(\"  (From the original randomized controlled trial)\")\n</code></pre> <pre><code>======================================================================\nExperimental Benchmark (Ground Truth)\n======================================================================\n  Experimental ATE: $1794.00\n  (From the original randomized controlled trial)\n</code></pre>"},{"location":"examples/lalonde_experiment/#naive-estimate-no-adjustment","title":"Naive Estimate (No Adjustment)","text":"<pre><code>print(f\"\\n{'=' * 70}\")\nprint(\"Naive Estimate (No Adjustment)\")\nprint(f\"{'=' * 70}\")\n\nweights_naive = jnp.ones(len(X))\nate_naive = estimate_ate(Y, A, weights_naive)\nnaive_error = ate_naive - ate_benchmark\nnaive_pct_error = (naive_error / ate_benchmark) * 100\n\nprint(f\"  Naive ATE: ${ate_naive:.2f}\")\nprint(f\"  Error: ${naive_error:.2f} ({naive_pct_error:+.1f}%)\")\n\n# Check initial balance\nsmd_naive = standardized_mean_difference(X, A, weights_naive)\nprint(\"\\n  Covariate balance:\")\nprint(f\"    Max |SMD|: {jnp.abs(smd_naive).max():.3f}\")\nprint(\"    (Values &gt; 0.1 indicate imbalance)\")\n\nprint(\"\\n  Per-covariate imbalance:\")\nfor i, (name, smd_val) in enumerate(zip(feature_names, smd_naive)):\n    print(f\"    {name:12s}: {smd_val:+.3f}\")\n</code></pre> <pre><code>======================================================================\nNaive Estimate (No Adjustment)\n======================================================================\n\n\n  Naive ATE: $4224.48\n  Error: $2430.48 (+135.5%)\n\n\n\n  Covariate balance:\n    Max |SMD|: 0.899\n    (Values &gt; 0.1 indicate imbalance)\n\n  Per-covariate imbalance:\n    age         : +0.175\n    education   : +0.323\n    black       : +0.279\n    hispanic    : -0.087\n    married     : -0.220\n    nodegree    : -0.086\n    RE74        : -0.884\n    RE75        : -0.899\n</code></pre>"},{"location":"examples/lalonde_experiment/#permutation-weighting-with-simple-mlp","title":"Permutation Weighting with Simple MLP","text":"<pre><code>print(f\"\\n{'=' * 70}\")\nprint(\"Permutation Weighting (Simple MLP)\")\nprint(f\"{'=' * 70}\")\n\n# Fit with a simple MLP architecture\nmlp_simple = MLPDiscriminator(hidden_dims=[3])\nweighter_simple = PermutationWeighter(\n    discriminator=mlp_simple,\n    num_epochs=500,\n    batch_size=len(X),  # Full batch\n    random_state=42,\n)\n\nprint(\"\\nFitting weighter...\")\n_ = weighter_simple.fit(X, A)\nweights_simple = weighter_simple.predict(X, A)\n\n# Estimate ATE\nate_pw_simple = estimate_ate(Y, A, weights_simple)\npw_error_simple = ate_pw_simple - ate_benchmark\npw_pct_error_simple = (pw_error_simple / ate_benchmark) * 100\n\nprint(f\"\\n  Permutation-weighted ATE: ${ate_pw_simple:.2f}\")\nprint(f\"  Error: ${pw_error_simple:.2f} ({pw_pct_error_simple:+.1f}%)\")\n\n# Check balance improvement\nsmd_pw_simple = standardized_mean_difference(X, A, weights_simple)\nprint(\"\\n  Covariate balance after weighting:\")\nprint(f\"    Max |SMD|: {jnp.abs(smd_pw_simple).max():.3f}\")\nbalance_improvement = (1 - jnp.abs(smd_pw_simple).max() / jnp.abs(smd_naive).max()) * 100\nprint(f\"    Balance improvement: {balance_improvement:.1f}%\")\n\n# ESS\ness_simple = effective_sample_size(weights_simple)\ness_ratio_simple = ess_simple / len(weights_simple)\nprint(\"\\n  Effective sample size:\")\nprint(f\"    ESS: {ess_simple:.0f} / {len(weights_simple)} ({ess_ratio_simple:.1%})\")\n</code></pre> <pre><code>======================================================================\nPermutation Weighting (Simple MLP)\n======================================================================\n\nFitting weighter...\n\n\n\n  Permutation-weighted ATE: $2512.67\n  Error: $718.67 (+40.1%)\n\n  Covariate balance after weighting:\n    Max |SMD|: 3.033\n    Balance improvement: -237.5%\n\n  Effective sample size:\n    ESS: 56 / 458 (12.2%)\n</code></pre>"},{"location":"examples/lalonde_experiment/#permutation-weighting-with-larger-mlp","title":"Permutation Weighting with Larger MLP","text":"<pre><code>print(f\"\\n{'=' * 70}\")\nprint(\"Permutation Weighting (Larger MLP)\")\nprint(f\"{'=' * 70}\")\n\n# Try a larger architecture\nmlp_large = MLPDiscriminator(hidden_dims=[32, 16])\nweighter_large = PermutationWeighter(\n    discriminator=mlp_large,\n    num_epochs=500,\n    batch_size=len(X),\n    random_state=42,\n)\n\nprint(\"\\nFitting weighter...\")\n_ = weighter_large.fit(X, A)\nweights_large = weighter_large.predict(X, A)\n\n# Estimate ATE\nate_pw_large = estimate_ate(Y, A, weights_large)\npw_error_large = ate_pw_large - ate_benchmark\npw_pct_error_large = (pw_error_large / ate_benchmark) * 100\n\nprint(f\"\\n  Permutation-weighted ATE: ${ate_pw_large:.2f}\")\nprint(f\"  Error: ${pw_error_large:.2f} ({pw_pct_error_large:+.1f}%)\")\n\n# Check balance\nsmd_pw_large = standardized_mean_difference(X, A, weights_large)\nprint(\"\\n  Covariate balance after weighting:\")\nprint(f\"    Max |SMD|: {jnp.abs(smd_pw_large).max():.3f}\")\nbalance_improvement_large = (1 - jnp.abs(smd_pw_large).max() / jnp.abs(smd_naive).max()) * 100\nprint(f\"    Balance improvement: {balance_improvement_large:.1f}%\")\n\n# ESS\ness_large = effective_sample_size(weights_large)\ness_ratio_large = ess_large / len(weights_large)\nprint(\"\\n  Effective sample size:\")\nprint(f\"    ESS: {ess_large:.0f} / {len(weights_large)} ({ess_ratio_large:.1%})\")\n</code></pre> <pre><code>======================================================================\nPermutation Weighting (Larger MLP)\n======================================================================\n\nFitting weighter...\n\n\n\n  Permutation-weighted ATE: $-633.69\n  Error: $-2427.69 (-135.3%)\n\n  Covariate balance after weighting:\n    Max |SMD|: 7.756\n    Balance improvement: -763.2%\n\n  Effective sample size:\n    ESS: 2 / 458 (0.5%)\n</code></pre>"},{"location":"examples/lalonde_experiment/#summary-comparison","title":"Summary Comparison","text":"<pre><code>print(f\"\\n{'=' * 70}\")\nprint(\"Summary Comparison\")\nprint(f\"{'=' * 70}\")\n\nprint(f\"\\n{'Method':&lt;30} {'ATE Estimate':&lt;15} {'Error':&lt;15} {'% Error':&lt;12}\")\nprint(\"-\" * 72)\nprint(f\"{'Experimental (Benchmark)':&lt;30} ${ate_benchmark:&gt;12.2f}   {'---':&gt;12}   {'---':&gt;12}\")\nprint(\n    f\"{'Naive (Unadjusted)':&lt;30} ${ate_naive:&gt;12.2f}  \"\n    + f\"${naive_error:&gt;12.2f}  {naive_pct_error:&gt;10.1f}%\"\n)\nprint(\n    f\"{'PW (Simple MLP)':&lt;30} ${ate_pw_simple:&gt;12.2f}  \"\n    + f\"${pw_error_simple:&gt;12.2f}  {pw_pct_error_simple:&gt;10.1f}%\"\n)\nprint(\n    f\"{'PW (Larger MLP)':&lt;30} ${ate_pw_large:&gt;12.2f}  \"\n    + f\"${pw_error_large:&gt;12.2f}  {pw_pct_error_large:&gt;10.1f}%\"\n)\n\nprint(\"\\n  Improvement over naive:\")\nimprovement_over_naive = abs(naive_error) - abs(pw_error_simple)\nprint(f\"\\n  Improvement over naive: ${improvement_over_naive:.2f}\")\n\nprint(f\"\\n{'=' * 70}\")\nprint(\"\u2713 Lalonde experiment completed successfully!\")\nelapsed_time = time.time() - start_time\nprint(f\"\u23f1  Total execution time: {elapsed_time:.2f} seconds\")\nprint(f\"{'=' * 70}\")\n</code></pre> <pre><code>======================================================================\nSummary Comparison\n======================================================================\n\nMethod                         ATE Estimate    Error           % Error     \n------------------------------------------------------------------------\nExperimental (Benchmark)       $     1794.00            ---            ---\nNaive (Unadjusted)             $     4224.48  $     2430.48       135.5%\nPW (Simple MLP)                $     2512.67  $      718.67        40.1%\nPW (Larger MLP)                $     -633.69  $    -2427.69      -135.3%\n\n  Improvement over naive:\n\n  Improvement over naive: $1711.81\n\n======================================================================\n\u2713 Lalonde experiment completed successfully!\n\u23f1  Total execution time: 14.95 seconds\n======================================================================\n</code></pre> <p>View source on GitHub</p>"},{"location":"examples/mlp_discriminator/","title":"MLP Discriminator Example","text":"<p>This example demonstrates using MLP (multilayer perceptron) discriminators for handling complex nonlinear confounding patterns.</p> <p>We compare: - Linear discriminator (default) - MLP with different architectures - MLP with different activation functions</p> <pre><code>import time\n\nimport jax\nimport jax.numpy as jnp\n\nfrom stochpw import MLPDiscriminator, PermutationWeighter, standardized_mean_difference\n</code></pre>"},{"location":"examples/mlp_discriminator/#generate-data-with-complex-nonlinear-confounding","title":"Generate Data with Complex Nonlinear Confounding","text":"<pre><code>start_time = time.time()\n\n# Generate synthetic data with complex confounding\nkey = jax.random.PRNGKey(123)\nn = 500\n\n# Generate confounders with nonlinear relationships\nX_key, A_key = jax.random.split(key)\nX = jax.random.normal(X_key, (n, 5))\n\n# Complex nonlinear propensity function\npropensity = jax.nn.sigmoid(\n    0.5 * X[:, 0] ** 2  # Nonlinear effect\n    - 0.3 * X[:, 1] * X[:, 2]  # Interaction\n    + jnp.sin(X[:, 3])  # Nonlinearity\n    + 0.1\n)\nA = jax.random.bernoulli(A_key, propensity, (n,)).astype(jnp.float32).reshape(-1, 1)\n\nprint(\"=\" * 70)\nprint(\"MLP Discriminator Example\")\nprint(\"=\" * 70)\nprint(f\"\\nData: {n} samples, {X.shape[1]} covariates\")\nprint(f\"Treatment: {A.mean():.1%} treated (nonlinear confounding)\")\n</code></pre> <pre><code>======================================================================\nMLP Discriminator Example\n======================================================================\n\nData: 500 samples, 5 covariates\nTreatment: 62.8% treated (nonlinear confounding)\n</code></pre>"},{"location":"examples/mlp_discriminator/#compare-linear-vs-mlp-discriminators","title":"Compare Linear vs MLP Discriminators","text":"<pre><code>print(\"\\n\" + \"=\" * 70)\nprint(\"Comparing Linear vs MLP Discriminators\")\nprint(\"=\" * 70)\n\nconfigs = [\n    (\"Linear\", None),\n    (\"MLP (default)\", MLPDiscriminator()),\n    (\"MLP (small)\", MLPDiscriminator(hidden_dims=[32])),\n    (\"MLP (large)\", MLPDiscriminator(hidden_dims=[128, 64, 32])),\n    (\"MLP (tanh)\", MLPDiscriminator(activation=\"tanh\")),\n]\n\nresults = []\n\nfor name, discriminator in configs:\n    print(f\"\\n{name}:\")\n    print(\"-\" * 40)\n\n    weighter = PermutationWeighter(\n        discriminator=discriminator,\n        num_epochs=500,\n        batch_size=500,\n        random_state=42,\n    )\n\n    _ = weighter.fit(X, A)\n    weights = weighter.predict(X, A)\n\n    # Calculate balance improvement\n    smd_unweighted = standardized_mean_difference(X, A, jnp.ones_like(weights))\n    smd_weighted = standardized_mean_difference(X, A, weights)\n\n    max_smd_unw = jnp.abs(smd_unweighted).max()\n    max_smd_w = jnp.abs(smd_weighted).max()\n    improvement = (1 - max_smd_w / max_smd_unw) * 100\n\n    print(f\"  Max |SMD| (unweighted): {max_smd_unw:.3f}\")\n    print(f\"  Max |SMD| (weighted):   {max_smd_w:.3f}\")\n    print(f\"  Balance improvement:    {improvement:.1f}%\")\n\n    results.append((name, max_smd_unw, max_smd_w, improvement))\n</code></pre> <pre><code>======================================================================\nComparing Linear vs MLP Discriminators\n======================================================================\n\nLinear:\n----------------------------------------\n\n\n  Max |SMD| (unweighted): 0.338\n  Max |SMD| (weighted):   1.419\n  Balance improvement:    -319.6%\n\nMLP (default):\n----------------------------------------\n\n\n  Max |SMD| (unweighted): 0.338\n  Max |SMD| (weighted):   0.085\n  Balance improvement:    74.8%\n\nMLP (small):\n----------------------------------------\n\n\n  Max |SMD| (unweighted): 0.338\n  Max |SMD| (weighted):   0.034\n  Balance improvement:    89.9%\n\nMLP (large):\n----------------------------------------\n\n\n  Max |SMD| (unweighted): 0.338\n  Max |SMD| (weighted):   0.275\n  Balance improvement:    18.6%\n\nMLP (tanh):\n----------------------------------------\n\n\n  Max |SMD| (unweighted): 0.338\n  Max |SMD| (weighted):   0.047\n  Balance improvement:    86.0%\n</code></pre>"},{"location":"examples/mlp_discriminator/#summary-comparison","title":"Summary Comparison","text":"<pre><code>print(\"\\n\" + \"=\" * 70)\nprint(\"Summary Comparison\")\nprint(\"=\" * 70)\nprint(f\"{'Model':&lt;20} {'Unweighted SMD':&lt;18} {'Weighted SMD':&lt;15} {'Improvement':&lt;12}\")\nprint(\"-\" * 70)\nfor name, unw, w, imp in results:\n    print(f\"{name:&lt;20} {unw:&gt;15.3f}   {w:&gt;13.3f}   {imp:&gt;10.1f}%\")\n\nprint(\"\\n\" + \"=\" * 70)\nprint(\"\u2713 Example completed successfully!\")\nelapsed_time = time.time() - start_time\nprint(f\"\u23f1  Total execution time: {elapsed_time:.2f} seconds\")\nprint(\"=\" * 70)\n</code></pre> <pre><code>======================================================================\nSummary Comparison\n======================================================================\nModel                Unweighted SMD     Weighted SMD    Improvement \n----------------------------------------------------------------------\nLinear                         0.338           1.419       -319.6%\nMLP (default)                  0.338           0.085         74.8%\nMLP (small)                    0.338           0.034         89.9%\nMLP (large)                    0.338           0.275         18.6%\nMLP (tanh)                     0.338           0.047         86.0%\n\n======================================================================\n\u2713 Example completed successfully!\n\u23f1  Total execution time: 32.12 seconds\n======================================================================\n</code></pre> <p>View source on GitHub</p>"},{"location":"reference/SUMMARY/","title":"SUMMARY","text":"<ul> <li>stochpw<ul> <li>core</li> <li>data</li> <li>diagnostics<ul> <li>advanced</li> <li>balance</li> <li>weights</li> </ul> </li> <li>models<ul> <li>base</li> <li>linear</li> <li>mlp</li> </ul> </li> <li>plotting</li> <li>training<ul> <li>batch</li> <li>loop</li> <li>losses</li> <li>regularization</li> </ul> </li> <li>types</li> <li>utils</li> <li>weights</li> </ul> </li> </ul>"},{"location":"reference/stochpw/","title":"stochpw","text":"<p>stochpw - Permutation weighting for causal inference.</p> <p>This package implements permutation weighting, a method for learning density ratios via discriminative classification. It trains a discriminator to distinguish between observed (X, A) pairs and permuted (X, A') pairs, then extracts importance weights from the discriminator's predictions.</p> <p>The package provides both a high-level sklearn-style API and low-level composable components for integration into larger causal inference models.</p>"},{"location":"reference/stochpw/#stochpw.BaseDiscriminator","title":"<code>BaseDiscriminator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for discriminator models in permutation weighting.</p> <p>All discriminator models must implement the <code>init_params</code> and <code>apply</code> methods to define how parameters are initialized and how logits are computed from treatments (A), covariates (X), and their interactions (AX).</p> <p>The discriminator's role is to distinguish between: - Observed pairs (X, A) with label C=0 - Permuted pairs (X, A') with label C=1</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyDiscriminator(BaseDiscriminator):\n...     def init_params(self, rng_key, d_a, d_x):\n...         # Initialize parameters\n...         return {\"w\": jax.random.normal(rng_key, (d_a + d_x,))}\n...\n...     def apply(self, params, a, x, ax):\n...         # Compute logits\n...         return jnp.dot(jnp.concatenate([a, x], axis=-1), params[\"w\"])\n</code></pre>"},{"location":"reference/stochpw/#stochpw.BaseDiscriminator.__call__","title":"<code>__call__(params, a, x, ax)</code>","text":"<p>Convenience method to call apply.</p> <p>Allows discriminator to be called as: discriminator(params, a, x, ax).</p> Source code in <code>src/stochpw/models/base.py</code> <pre><code>def __call__(self, params: PyTree, a: Array, x: Array, ax: Array) -&gt; Array:\n    \"\"\"\n    Convenience method to call apply.\n\n    Allows discriminator to be called as: discriminator(params, a, x, ax).\n    \"\"\"\n    return self.apply(params, a, x, ax)\n</code></pre>"},{"location":"reference/stochpw/#stochpw.BaseDiscriminator.apply","title":"<code>apply(params, a, x, ax)</code>  <code>abstractmethod</code>","text":"<p>Compute discriminator logits.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>PyTree</code> <p>Model parameters (output of init_params)</p> required <code>a</code> <code>(Array, shape(batch_size, d_a) or (batch_size,))</code> <p>Treatment assignments</p> required <code>x</code> <code>(Array, shape(batch_size, d_x))</code> <p>Covariates</p> required <code>ax</code> <code>(Array, shape(batch_size, d_a * d_x))</code> <p>Pre-computed first-order interactions A \u2297 X</p> required <p>Returns:</p> Name Type Description <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Discriminator logits for p(C=1 | a, x)</p> Notes <p>The logits represent the raw output before sigmoid activation. They are used in the logistic loss: BCE(sigmoid(logits), labels).</p> Source code in <code>src/stochpw/models/base.py</code> <pre><code>@abstractmethod\ndef apply(self, params: PyTree, a: Array, x: Array, ax: Array) -&gt; Array:\n    \"\"\"\n    Compute discriminator logits.\n\n    Parameters\n    ----------\n    params : PyTree\n        Model parameters (output of init_params)\n    a : jax.Array, shape (batch_size, d_a) or (batch_size,)\n        Treatment assignments\n    x : jax.Array, shape (batch_size, d_x)\n        Covariates\n    ax : jax.Array, shape (batch_size, d_a * d_x)\n        Pre-computed first-order interactions A \u2297 X\n\n    Returns\n    -------\n    logits : jax.Array, shape (batch_size,)\n        Discriminator logits for p(C=1 | a, x)\n\n    Notes\n    -----\n    The logits represent the raw output before sigmoid activation.\n    They are used in the logistic loss: BCE(sigmoid(logits), labels).\n    \"\"\"\n    raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"reference/stochpw/#stochpw.BaseDiscriminator.init_params","title":"<code>init_params(rng_key, d_a, d_x)</code>  <code>abstractmethod</code>","text":"<p>Initialize discriminator parameters.</p> <p>Parameters:</p> Name Type Description Default <code>rng_key</code> <code>Array</code> <p>Random key for parameter initialization</p> required <code>d_a</code> <code>int</code> <p>Dimension of treatment vector</p> required <code>d_x</code> <code>int</code> <p>Dimension of covariate vector</p> required <p>Returns:</p> Name Type Description <code>params</code> <code>PyTree</code> <p>Initialized parameters (any JAX-compatible PyTree structure)</p> Source code in <code>src/stochpw/models/base.py</code> <pre><code>@abstractmethod\ndef init_params(self, rng_key: Array, d_a: int, d_x: int) -&gt; PyTree:\n    \"\"\"\n    Initialize discriminator parameters.\n\n    Parameters\n    ----------\n    rng_key : jax.Array\n        Random key for parameter initialization\n    d_a : int\n        Dimension of treatment vector\n    d_x : int\n        Dimension of covariate vector\n\n    Returns\n    -------\n    params : PyTree\n        Initialized parameters (any JAX-compatible PyTree structure)\n    \"\"\"\n    raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"reference/stochpw/#stochpw.LinearDiscriminator","title":"<code>LinearDiscriminator</code>","text":"<p>               Bases: <code>BaseDiscriminator</code></p> <p>Linear discriminator using A, X, and A*X interactions.</p> <p>The discriminator computes logits as:     logit = w_a^T A + w_x^T X + w_ax^T (A \u2297 X) + b</p> <p>This allows the model to learn from: - Marginal treatment effects (w_a) - Marginal covariate effects (w_x) - Treatment-covariate interactions (w_ax)</p> <p>The explicit inclusion of A*X interactions is critical for linear models because within-batch permutation ensures P(A) and P(X) are identical in observed vs permuted batches. Only the joint distribution P(A,X) differs, which linear models need explicit interaction terms to capture.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from stochpw.models import LinearDiscriminator\n&gt;&gt;&gt; import jax\n&gt;&gt;&gt;\n&gt;&gt;&gt; discriminator = LinearDiscriminator()\n&gt;&gt;&gt; params = discriminator.init_params(jax.random.PRNGKey(0), d_a=1, d_x=3)\n&gt;&gt;&gt; # params contains: w_a, w_x, w_ax, b\n</code></pre>"},{"location":"reference/stochpw/#stochpw.LinearDiscriminator.apply","title":"<code>apply(params, a, x, ax)</code>","text":"<p>Compute linear discriminator logits using A, X, and A*X.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters with keys 'w_a', 'w_x', 'w_ax', and 'b'</p> required <code>a</code> <code>(Array, shape(batch_size, d_a) or (batch_size,))</code> <p>Treatment assignments</p> required <code>x</code> <code>(Array, shape(batch_size, d_x))</code> <p>Covariates</p> required <code>ax</code> <code>(Array, shape(batch_size, d_a * d_x))</code> <p>Pre-computed first-order interactions A \u2297 X</p> required <p>Returns:</p> Name Type Description <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Discriminator logits for p(C=1 | a, x)</p> Source code in <code>src/stochpw/models/linear.py</code> <pre><code>@override\ndef apply(self, params: PyTree, a: Array, x: Array, ax: Array) -&gt; Array:  # type: ignore[override]\n    \"\"\"\n    Compute linear discriminator logits using A, X, and A*X.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters with keys 'w_a', 'w_x', 'w_ax', and 'b'\n    a : jax.Array, shape (batch_size, d_a) or (batch_size,)\n        Treatment assignments\n    x : jax.Array, shape (batch_size, d_x)\n        Covariates\n    ax : jax.Array, shape (batch_size, d_a * d_x)\n        Pre-computed first-order interactions A \u2297 X\n\n    Returns\n    -------\n    logits : jax.Array, shape (batch_size,)\n        Discriminator logits for p(C=1 | a, x)\n    \"\"\"\n    # Ensure a is 2D\n    if a.ndim == 1:\n        a = a.reshape(-1, 1)\n\n    # Cast params to expected dict type for type checker\n    params_dict = cast(LinearParams, params)\n\n    # Linear transformation: w_a^T A + w_x^T X + w_ax^T (A*X) + b\n    logits = (\n        jnp.dot(a, params_dict[\"w_a\"])\n        + jnp.dot(x, params_dict[\"w_x\"])\n        + jnp.dot(ax, params_dict[\"w_ax\"])\n        + params_dict[\"b\"]\n    )\n\n    return logits\n</code></pre>"},{"location":"reference/stochpw/#stochpw.LinearDiscriminator.init_params","title":"<code>init_params(rng_key, d_a, d_x)</code>","text":"<p>Initialize linear discriminator parameters.</p> <p>Uses Xavier/Glorot initialization for weights and small random initialization for bias.</p> <p>Parameters:</p> Name Type Description Default <code>rng_key</code> <code>Array</code> <p>Random key for parameter initialization</p> required <code>d_a</code> <code>int</code> <p>Dimension of treatment vector</p> required <code>d_x</code> <code>int</code> <p>Dimension of covariate vector</p> required <p>Returns:</p> Name Type Description <code>params</code> <code>dict</code> <p>Dictionary with keys: - 'w_a': Array of shape (d_a,) - treatment weights - 'w_x': Array of shape (d_x,) - covariate weights - 'w_ax': Array of shape (d_a * d_x,) - interaction weights - 'b': scalar - bias term</p> Source code in <code>src/stochpw/models/linear.py</code> <pre><code>@override\ndef init_params(self, rng_key: Array, d_a: int, d_x: int) -&gt; LinearParams:\n    \"\"\"\n    Initialize linear discriminator parameters.\n\n    Uses Xavier/Glorot initialization for weights and small random\n    initialization for bias.\n\n    Parameters\n    ----------\n    rng_key : jax.Array\n        Random key for parameter initialization\n    d_a : int\n        Dimension of treatment vector\n    d_x : int\n        Dimension of covariate vector\n\n    Returns\n    -------\n    params : dict\n        Dictionary with keys:\n        - 'w_a': Array of shape (d_a,) - treatment weights\n        - 'w_x': Array of shape (d_x,) - covariate weights\n        - 'w_ax': Array of shape (d_a * d_x,) - interaction weights\n        - 'b': scalar - bias term\n    \"\"\"\n    interaction_dim = d_a * d_x\n    total_dim = d_a + d_x + interaction_dim\n\n    w_key, b_key = jax.random.split(rng_key)\n\n    # Xavier/Glorot initialization\n    std = jnp.sqrt(2.0 / total_dim)\n    w_a = jax.random.normal(jax.random.fold_in(w_key, 0), (d_a,)) * std\n    w_x = jax.random.normal(jax.random.fold_in(w_key, 1), (d_x,)) * std\n    w_ax = jax.random.normal(jax.random.fold_in(w_key, 2), (interaction_dim,)) * std\n    b = jax.random.normal(b_key, ()) * 0.01\n\n    return {\"w_a\": w_a, \"w_x\": w_x, \"w_ax\": w_ax, \"b\": b}\n</code></pre>"},{"location":"reference/stochpw/#stochpw.MLPDiscriminator","title":"<code>MLPDiscriminator(hidden_dims=None, activation='relu')</code>","text":"<p>               Bases: <code>BaseDiscriminator</code></p> <p>Multi-layer perceptron (MLP) discriminator using A, X, and A*X interactions.</p> <p>The MLP processes concatenated features [A, X, A*X] through configurable hidden layers with specified activation functions, outputting a scalar logit.</p> <p>This provides more expressive power than linear discriminators for capturing complex relationships between treatments and covariates.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_dims</code> <code>list[int]</code> <p>List of hidden layer sizes. Default is [64, 32]</p> <code>None</code> <code>activation</code> <code>(relu, tanh, elu, sigmoid)</code> <p>Activation function to use between layers. Default is 'relu'</p> <code>'relu'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from stochpw.models import MLPDiscriminator\n&gt;&gt;&gt; import jax\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Default: 2-layer MLP with ReLU\n&gt;&gt;&gt; discriminator = MLPDiscriminator()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Custom: 3-layer MLP with tanh\n&gt;&gt;&gt; discriminator = MLPDiscriminator(hidden_dims=[128, 64, 32], activation=\"tanh\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; params = discriminator.init_params(jax.random.PRNGKey(0), d_a=1, d_x=3)\n</code></pre> Source code in <code>src/stochpw/models/mlp.py</code> <pre><code>def __init__(\n    self,\n    hidden_dims: list[int] | None = None,\n    activation: ActivationType = \"relu\",\n):\n    if hidden_dims is None:\n        hidden_dims = [64, 32]\n    self.hidden_dims: list[int] = hidden_dims\n    self.activation: ActivationType = activation\n    self._activation_fn: Callable[[Array], Array] = _get_activation(activation)\n    self._use_he_init: bool = activation in (\"relu\", \"elu\")\n</code></pre>"},{"location":"reference/stochpw/#stochpw.MLPDiscriminator.apply","title":"<code>apply(params, a, x, ax)</code>","text":"<p>Compute MLP discriminator logits using A, X, and A*X.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters with key 'layers' containing list of layer dicts</p> required <code>a</code> <code>(Array, shape(batch_size, d_a) or (batch_size,))</code> <p>Treatment assignments</p> required <code>x</code> <code>(Array, shape(batch_size, d_x))</code> <p>Covariates</p> required <code>ax</code> <code>(Array, shape(batch_size, d_a * d_x))</code> <p>Pre-computed first-order interactions A \u2297 X</p> required <p>Returns:</p> Name Type Description <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Discriminator logits for p(C=1 | a, x)</p> Source code in <code>src/stochpw/models/mlp.py</code> <pre><code>@override\ndef apply(self, params: PyTree, a: Array, x: Array, ax: Array) -&gt; Array:  # type: ignore[override]\n    \"\"\"\n    Compute MLP discriminator logits using A, X, and A*X.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters with key 'layers' containing list of layer dicts\n    a : jax.Array, shape (batch_size, d_a) or (batch_size,)\n        Treatment assignments\n    x : jax.Array, shape (batch_size, d_x)\n        Covariates\n    ax : jax.Array, shape (batch_size, d_a * d_x)\n        Pre-computed first-order interactions A \u2297 X\n\n    Returns\n    -------\n    logits : jax.Array, shape (batch_size,)\n        Discriminator logits for p(C=1 | a, x)\n    \"\"\"\n    # Ensure a is 2D\n    if a.ndim == 1:\n        a = a.reshape(-1, 1)\n\n    # Concatenate all features: [A, X, A*X]\n    h = jnp.concatenate([a, x, ax], axis=-1)\n\n    # Cast params to expected dict type for type checker\n    params_dict = cast(MLPParams, params)\n\n    # Forward pass through hidden layers\n    for i, layer in enumerate(params_dict[\"layers\"]):\n        h = jnp.dot(h, layer[\"w\"]) + layer[\"b\"]\n\n        # Apply activation to all layers except the last (output layer)\n        if i &lt; len(params_dict[\"layers\"]) - 1:\n            h = self._activation_fn(h)\n\n    # Output is shape (batch_size, 1), squeeze to (batch_size,)\n    logits = h.squeeze(-1)\n\n    return logits\n</code></pre>"},{"location":"reference/stochpw/#stochpw.MLPDiscriminator.init_params","title":"<code>init_params(rng_key, d_a, d_x)</code>","text":"<p>Initialize MLP discriminator parameters.</p> <p>Uses He initialization for ReLU-family activations and Xavier initialization for tanh/sigmoid activations. Biases are initialized to zero.</p> <p>Parameters:</p> Name Type Description Default <code>rng_key</code> <code>Array</code> <p>Random key for parameter initialization</p> required <code>d_a</code> <code>int</code> <p>Dimension of treatment vector</p> required <code>d_x</code> <code>int</code> <p>Dimension of covariate vector</p> required <p>Returns:</p> Name Type Description <code>params</code> <code>dict</code> <p>Dictionary with key 'layers' containing a list of layer dicts, each with keys 'w' (weight matrix) and 'b' (bias vector)</p> Source code in <code>src/stochpw/models/mlp.py</code> <pre><code>@override\ndef init_params(self, rng_key: Array, d_a: int, d_x: int) -&gt; MLPParams:\n    \"\"\"\n    Initialize MLP discriminator parameters.\n\n    Uses He initialization for ReLU-family activations and Xavier\n    initialization for tanh/sigmoid activations. Biases are initialized to zero.\n\n    Parameters\n    ----------\n    rng_key : jax.Array\n        Random key for parameter initialization\n    d_a : int\n        Dimension of treatment vector\n    d_x : int\n        Dimension of covariate vector\n\n    Returns\n    -------\n    params : dict\n        Dictionary with key 'layers' containing a list of layer dicts,\n        each with keys 'w' (weight matrix) and 'b' (bias vector)\n    \"\"\"\n    interaction_dim = d_a * d_x\n    input_dim = d_a + d_x + interaction_dim\n\n    params = {\"layers\": []}\n    layer_dims = [input_dim] + self.hidden_dims + [1]  # Output is scalar logit\n    current_key = rng_key\n\n    for i in range(len(layer_dims) - 1):\n        current_key, layer_key = jax.random.split(current_key)\n        w_key, _b_key = jax.random.split(layer_key)\n\n        in_dim = layer_dims[i]\n        out_dim = layer_dims[i + 1]\n\n        # He initialization for ReLU, Xavier for others\n        if self._use_he_init:\n            std = jnp.sqrt(2.0 / in_dim)\n        else:\n            std = jnp.sqrt(2.0 / (in_dim + out_dim))\n\n        w = jax.random.normal(w_key, (in_dim, out_dim)) * std\n        b = jnp.zeros((out_dim,))  # Initialize biases to zero\n\n        params[\"layers\"].append({\"w\": w, \"b\": b})\n\n    return params\n</code></pre>"},{"location":"reference/stochpw/#stochpw.NotFittedError","title":"<code>NotFittedError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when predict is called before fit.</p>"},{"location":"reference/stochpw/#stochpw.PermutationWeighter","title":"<code>PermutationWeighter(discriminator=None, optimizer=None, num_epochs=100, batch_size=256, random_state=None, loss_fn=logistic_loss, regularization_fn=None, regularization_strength=0.0, early_stopping=False, patience=10, min_delta=0.0001)</code>","text":"<p>Main class for permutation weighting (sklearn-style API).</p> <p>Permutation weighting learns importance weights by training a discriminator to distinguish between observed (X, A) pairs and permuted (X, A') pairs, where treatments are randomly shuffled to break the association with covariates.</p> <p>Parameters:</p> Name Type Description Default <code>discriminator</code> <code>BaseDiscriminator</code> <p>Discriminator model instance. If None, uses LinearDiscriminator(). Can be any subclass of BaseDiscriminator (e.g., LinearDiscriminator, MLPDiscriminator, or custom discriminator).</p> <code>None</code> <code>optimizer</code> <code>GradientTransformation</code> <p>Optax optimizer. If None, uses Adam with learning rate 1e-3.</p> <code>None</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs</p> <code>100</code> <code>batch_size</code> <code>int</code> <p>Mini-batch size for training</p> <code>256</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility</p> <code>None</code> <code>loss_fn</code> <code>Callable</code> <p>Loss function (logits, labels) -&gt; loss. Can be logistic_loss, exponential_loss, or brier_loss.</p> <code>logistic_loss</code> <code>regularization_fn</code> <code>Callable</code> <p>Regularization function on weights (weights) -&gt; penalty. Use entropy_penalty or lp_weight_penalty (with p=1 or p=2).</p> <code>None</code> <code>regularization_strength</code> <code>float</code> <p>Strength of regularization penalty</p> <code>0.0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to use early stopping based on training loss</p> <code>False</code> <code>patience</code> <code>int</code> <p>Number of epochs to wait for improvement before early stopping</p> <code>10</code> <code>min_delta</code> <code>float</code> <p>Minimum change in loss to qualify as improvement for early stopping</p> <code>1e-4</code> <p>Attributes:</p> Name Type Description <code>params_</code> <code>dict</code> <p>Fitted discriminator parameters (set after fit)</p> <code>history_</code> <code>dict</code> <p>Training history with 'loss' key (set after fit)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from stochpw import PermutationWeighter, LinearDiscriminator, MLPDiscriminator\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Generate synthetic data\n&gt;&gt;&gt; X = jnp.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n&gt;&gt;&gt; A = jnp.array([[0.0], [1.0], [0.0]])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Default: Linear discriminator\n&gt;&gt;&gt; weighter = PermutationWeighter(num_epochs=50, random_state=42)\n&gt;&gt;&gt; weighter.fit(X, A)\n&gt;&gt;&gt; weights = weighter.predict(X, A)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # MLP discriminator with custom architecture\n&gt;&gt;&gt; mlp_disc = MLPDiscriminator(hidden_dims=[128, 64], activation=\"tanh\")\n&gt;&gt;&gt; weighter = PermutationWeighter(discriminator=mlp_disc, num_epochs=50, random_state=42)\n&gt;&gt;&gt; weighter.fit(X, A)\n&gt;&gt;&gt; weights = weighter.predict(X, A)\n</code></pre> Source code in <code>src/stochpw/core.py</code> <pre><code>def __init__(\n    self,\n    discriminator: BaseDiscriminator | None = None,\n    optimizer: optax.GradientTransformation | None = None,\n    num_epochs: int = 100,\n    batch_size: int = 256,\n    random_state: int | None = None,\n    loss_fn: LossFn = logistic_loss,\n    regularization_fn: Callable[[Array], Array] | None = None,\n    regularization_strength: float = 0.0,\n    early_stopping: bool = False,\n    patience: int = 10,\n    min_delta: float = 1e-4,\n):\n    self.discriminator: BaseDiscriminator = (\n        discriminator if discriminator is not None else LinearDiscriminator()\n    )\n    self.optimizer: optax.GradientTransformation | None = optimizer\n    self.num_epochs: int = num_epochs\n    self.batch_size: int = batch_size\n    self.random_state: int | None = random_state\n    self.loss_fn: LossFn = loss_fn\n    self.regularization_fn: Callable[[Array], Array] | None = regularization_fn\n    self.regularization_strength: float = regularization_strength\n    self.early_stopping: bool = early_stopping\n    self.patience: int = patience\n    self.min_delta: float = min_delta\n\n    # Fitted attributes (set by fit())\n    self.params_: PyTree | None = None\n    self.history_: dict[str, list[float]] | None = None\n    self._input_dim: int | None = None\n</code></pre>"},{"location":"reference/stochpw/#stochpw.PermutationWeighter.fit","title":"<code>fit(X, A)</code>","text":"<p>Fit discriminator on data (sklearn-style).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(array - like, shape(n_samples) or (n_samples, n_treatments))</code> <p>Treatment assignments</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>PermutationWeighter</code> <p>Fitted estimator (for method chaining)</p> Source code in <code>src/stochpw/core.py</code> <pre><code>def fit(\n    self,\n    X: Array | NDArray[Any],  # type: ignore[misc]\n    A: Array | NDArray[Any],  # type: ignore[misc]\n) -&gt; \"PermutationWeighter\":\n    \"\"\"\n    Fit discriminator on data (sklearn-style).\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Covariates\n    A : array-like, shape (n_samples,) or (n_samples, n_treatments)\n        Treatment assignments\n\n    Returns\n    -------\n    self : PermutationWeighter\n        Fitted estimator (for method chaining)\n    \"\"\"\n    # Validate and convert inputs\n    x_val, a_val = validate_inputs(X, A)\n\n    # Set up RNG key\n    if self.random_state is not None:\n        rng_key = jax.random.PRNGKey(self.random_state)\n    else:\n        rng_key = jax.random.PRNGKey(0)\n\n    # Determine dimensions\n    d_a = a_val.shape[1]\n    d_x = x_val.shape[1]\n\n    # Initialize discriminator parameters\n    init_key, train_key = jax.random.split(rng_key)\n    init_params = self.discriminator.init_params(init_key, d_a, d_x)\n\n    # Set up optimizer\n    if self.optimizer is None:\n        optimizer = optax.adam(1e-3)\n    else:\n        optimizer = self.optimizer\n\n    # Fit discriminator\n    self.params_, self.history_ = fit_discriminator(\n        X=x_val,\n        A=a_val,\n        discriminator_fn=self.discriminator.apply,\n        init_params=init_params,\n        optimizer=optimizer,\n        num_epochs=self.num_epochs,\n        batch_size=self.batch_size,\n        rng_key=train_key,\n        loss_fn=self.loss_fn,\n        regularization_fn=self.regularization_fn,\n        regularization_strength=self.regularization_strength,\n        early_stopping=self.early_stopping,\n        patience=self.patience,\n        min_delta=self.min_delta,\n    )\n\n    return self\n</code></pre>"},{"location":"reference/stochpw/#stochpw.PermutationWeighter.predict","title":"<code>predict(X, A)</code>","text":"<p>Predict importance weights for given data (sklearn-style).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(array - like, shape(n_samples) or (n_samples, n_treatments))</code> <p>Treatment assignments</p> required <p>Returns:</p> Name Type Description <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Importance weights</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If called before fit()</p> Source code in <code>src/stochpw/core.py</code> <pre><code>def predict(\n    self,\n    X: Array | NDArray[Any],  # type: ignore[misc]\n    A: Array | NDArray[Any],  # type: ignore[misc]\n) -&gt; Array:\n    \"\"\"\n    Predict importance weights for given data (sklearn-style).\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Covariates\n    A : array-like, shape (n_samples,) or (n_samples, n_treatments)\n        Treatment assignments\n\n    Returns\n    -------\n    weights : jax.Array, shape (n_samples,)\n        Importance weights\n\n    Raises\n    ------\n    NotFittedError\n        If called before fit()\n    \"\"\"\n    if self.params_ is None:\n        raise NotFittedError(\n            \"This PermutationWeighter instance is not fitted yet. \"\n            + \"Call 'fit' with appropriate arguments before using 'predict'.\"\n        )\n\n    # Validate and convert inputs\n    x_val, a_val = validate_inputs(X, A)\n\n    # Compute interactions\n    ax = jnp.einsum(\"bi,bj-&gt;bij\", a_val, x_val).reshape(x_val.shape[0], -1)\n\n    # Extract weights\n    weights = extract_weights(self.discriminator.apply, self.params_, x_val, a_val, ax)\n\n    return weights\n</code></pre>"},{"location":"reference/stochpw/#stochpw.TrainingBatch","title":"<code>TrainingBatch(X, A, C, AX)</code>  <code>dataclass</code>","text":"<p>A batch of data for discriminator training.</p>"},{"location":"reference/stochpw/#stochpw.TrainingState","title":"<code>TrainingState(params, opt_state, rng_key, epoch, history)</code>  <code>dataclass</code>","text":"<p>State of the training process.</p>"},{"location":"reference/stochpw/#stochpw.TrainingStepResult","title":"<code>TrainingStepResult(state, loss)</code>  <code>dataclass</code>","text":"<p>Result of a single training step.</p>"},{"location":"reference/stochpw/#stochpw.WeightedData","title":"<code>WeightedData(X, A, weights)</code>  <code>dataclass</code>","text":"<p>Data with computed importance weights.</p>"},{"location":"reference/stochpw/#stochpw.balance_report","title":"<code>balance_report(X, A, weights)</code>","text":"<p>Generate comprehensive balance report.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n_samples, 1) or (n_samples,))</code> <p>Treatments</p> required <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>report</code> <code>dict</code> <p>Comprehensive balance report with: - smd: Array of SMD per covariate - max_smd: Maximum absolute SMD across covariates - mean_smd: Mean absolute SMD across covariates - ess: Effective sample size - ess_ratio: ESS / n_samples - weight_stats: Dictionary of weight distribution statistics - n_samples: Number of samples - n_features: Number of features - treatment_type: 'binary' or 'continuous'</p> Notes <p>This function provides a complete overview of balance quality after weighting, useful for reporting and model diagnostics.</p> Source code in <code>src/stochpw/diagnostics/advanced.py</code> <pre><code>def balance_report(\n    X: Array, A: Array, weights: Array\n) -&gt; dict[str, float | Array | dict[str, float] | str | int]:\n    \"\"\"\n    Generate comprehensive balance report.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n_samples, n_features)\n        Covariates\n    A : jax.Array, shape (n_samples, 1) or (n_samples,)\n        Treatments\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    report : dict\n        Comprehensive balance report with:\n        - smd: Array of SMD per covariate\n        - max_smd: Maximum absolute SMD across covariates\n        - mean_smd: Mean absolute SMD across covariates\n        - ess: Effective sample size\n        - ess_ratio: ESS / n_samples\n        - weight_stats: Dictionary of weight distribution statistics\n        - n_samples: Number of samples\n        - n_features: Number of features\n        - treatment_type: 'binary' or 'continuous'\n\n    Notes\n    -----\n    This function provides a complete overview of balance quality after\n    weighting, useful for reporting and model diagnostics.\n    \"\"\"\n    from .balance import standardized_mean_difference\n    from .weights import effective_sample_size as ess_fn\n\n    # Ensure A is 1D for type detection\n    A_flat = A.squeeze() if A.ndim == 2 else A\n\n    # Detect treatment type\n    unique_a = jnp.unique(A_flat)\n    is_binary = len(unique_a) == 2\n    treatment_type = \"binary\" if is_binary else \"continuous\"\n\n    # Compute SMD\n    smd = standardized_mean_difference(X, A, weights)\n    max_smd = float(jnp.max(jnp.abs(smd)))\n    mean_smd = float(jnp.mean(jnp.abs(smd)))\n\n    # Compute ESS\n    ess = float(ess_fn(weights))\n    n_samples = len(weights)\n    ess_ratio = ess / n_samples\n\n    # Weight statistics\n    w_stats = weight_statistics(weights)\n\n    return {\n        \"smd\": smd,\n        \"max_smd\": max_smd,\n        \"mean_smd\": mean_smd,\n        \"ess\": ess,\n        \"ess_ratio\": ess_ratio,\n        \"weight_stats\": w_stats,\n        \"n_samples\": n_samples,\n        \"n_features\": X.shape[1],\n        \"treatment_type\": treatment_type,\n    }\n</code></pre>"},{"location":"reference/stochpw/#stochpw.brier_loss","title":"<code>brier_loss(logits, labels)</code>","text":"<p>Brier score loss for probabilistic predictions.</p> <p>The Brier score is the mean squared error between predicted probabilities and true labels. It's a proper scoring rule that encourages well-calibrated predictions.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Raw discriminator outputs</p> required <code>labels</code> <code>(Array, shape(batch_size))</code> <p>Binary labels (0 or 1)</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>Scalar loss value</p> Notes <p>The Brier score is defined as:     BS = (1/n) * \u03a3(p_i - y_i)\u00b2 where p_i = \u03c3(logits_i) is the predicted probability and y_i is the true label.</p> Source code in <code>src/stochpw/training/losses.py</code> <pre><code>@jax.jit\ndef brier_loss(logits: Array, labels: Array) -&gt; Array:\n    \"\"\"\n    Brier score loss for probabilistic predictions.\n\n    The Brier score is the mean squared error between predicted probabilities\n    and true labels. It's a proper scoring rule that encourages well-calibrated\n    predictions.\n\n    Parameters\n    ----------\n    logits : jax.Array, shape (batch_size,)\n        Raw discriminator outputs\n    labels : jax.Array, shape (batch_size,)\n        Binary labels (0 or 1)\n\n    Returns\n    -------\n    loss : float\n        Scalar loss value\n\n    Notes\n    -----\n    The Brier score is defined as:\n        BS = (1/n) * \u03a3(p_i - y_i)\u00b2\n    where p_i = \u03c3(logits_i) is the predicted probability and y_i is the true label.\n    \"\"\"\n    # Convert logits to probabilities\n    probs = jax.nn.sigmoid(logits)\n    # Brier score: mean squared error\n    return jnp.mean((probs - labels) ** 2)\n</code></pre>"},{"location":"reference/stochpw/#stochpw.calibration_curve","title":"<code>calibration_curve(discriminator_probs, true_labels, num_bins=10)</code>","text":"<p>Compute calibration curve for discriminator predictions.</p> <p>A well-calibrated discriminator should have predicted probabilities that match the true frequencies of the labels.</p> <p>Parameters:</p> Name Type Description Default <code>discriminator_probs</code> <code>(Array, shape(n_samples))</code> <p>Predicted probabilities from discriminator (values between 0 and 1)</p> required <code>true_labels</code> <code>(Array, shape(n_samples))</code> <p>True binary labels (0 or 1)</p> required <code>num_bins</code> <code>int</code> <p>Number of bins to divide probability range into</p> <code>10</code> <p>Returns:</p> Name Type Description <code>bin_centers</code> <code>(Array, shape(num_bins))</code> <p>Center of each probability bin</p> <code>true_frequencies</code> <code>(Array, shape(num_bins))</code> <p>Actual frequency of positive class in each bin</p> <code>counts</code> <code>(Array, shape(num_bins))</code> <p>Number of samples in each bin</p> Notes <p>Perfect calibration means true_frequencies == bin_centers for all bins.</p> Source code in <code>src/stochpw/diagnostics/advanced.py</code> <pre><code>def calibration_curve(\n    discriminator_probs: Array, true_labels: Array, num_bins: int = 10\n) -&gt; tuple[Array, Array, Array]:\n    \"\"\"\n    Compute calibration curve for discriminator predictions.\n\n    A well-calibrated discriminator should have predicted probabilities\n    that match the true frequencies of the labels.\n\n    Parameters\n    ----------\n    discriminator_probs : jax.Array, shape (n_samples,)\n        Predicted probabilities from discriminator (values between 0 and 1)\n    true_labels : jax.Array, shape (n_samples,)\n        True binary labels (0 or 1)\n    num_bins : int, default=10\n        Number of bins to divide probability range into\n\n    Returns\n    -------\n    bin_centers : jax.Array, shape (num_bins,)\n        Center of each probability bin\n    true_frequencies : jax.Array, shape (num_bins,)\n        Actual frequency of positive class in each bin\n    counts : jax.Array, shape (num_bins,)\n        Number of samples in each bin\n\n    Notes\n    -----\n    Perfect calibration means true_frequencies == bin_centers for all bins.\n    \"\"\"\n    # Create bins\n    bin_edges = jnp.linspace(0, 1, num_bins + 1)\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n    # Digitize predictions into bins\n    bin_indices = jnp.digitize(discriminator_probs, bin_edges[1:-1])\n\n    # Compute true frequency and count per bin\n    true_frequencies = jnp.zeros(num_bins)\n    counts = jnp.zeros(num_bins)\n\n    for i in range(num_bins):\n        mask = bin_indices == i\n        count = jnp.sum(mask)\n        counts = counts.at[i].set(count)\n\n        if count &gt; 0:\n            freq = jnp.sum(true_labels[mask]) / count\n            true_frequencies = true_frequencies.at[i].set(freq)\n        else:\n            # For empty bins, use bin center as default\n            true_frequencies = true_frequencies.at[i].set(bin_centers[i])\n\n    return bin_centers, true_frequencies, counts\n</code></pre>"},{"location":"reference/stochpw/#stochpw.create_training_batch","title":"<code>create_training_batch(X, A, batch_indices, rng_key)</code>","text":"<p>Create a training batch with observed and permuted pairs.</p> <p>Includes first-order interactions (A*X) which are critical for the discriminator to learn the association between treatment and covariates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n, d_x))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n, d_a))</code> <p>Treatments</p> required <code>batch_indices</code> <code>(Array, shape(batch_size))</code> <p>Indices for this batch</p> required <code>rng_key</code> <code>PRNGKey</code> <p>PRNG key for permutation</p> required <p>Returns:</p> Type Description <code>TrainingBatch</code> <p>Batch with concatenated observed and permuted data, including interactions</p> Source code in <code>src/stochpw/training/batch.py</code> <pre><code>def create_training_batch(\n    X: Array, A: Array, batch_indices: Array, rng_key: Array\n) -&gt; TrainingBatch:\n    \"\"\"\n    Create a training batch with observed and permuted pairs.\n\n    Includes first-order interactions (A*X) which are critical for the\n    discriminator to learn the association between treatment and covariates.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n, d_x)\n        Covariates\n    A : jax.Array, shape (n, d_a)\n        Treatments\n    batch_indices : jax.Array, shape (batch_size,)\n        Indices for this batch\n    rng_key : jax.random.PRNGKey\n        PRNG key for permutation\n\n    Returns\n    -------\n    TrainingBatch\n        Batch with concatenated observed and permuted data, including interactions\n    \"\"\"\n    # Sample observed batch\n    X_obs = X[batch_indices]\n    A_obs = A[batch_indices]\n\n    # Create permuted batch by shuffling treatments WITHIN the batch\n    # This creates the product distribution P(A)P(X) within the batch\n    batch_size = len(batch_indices)\n    X_perm = X_obs  # Same covariates (not shuffled)\n    A_perm = permute_treatment(A_obs, rng_key)  # Shuffle treatments within batch\n\n    # Compute interactions: outer product A \u2297 X\n    # For each sample, creates all A_i * X_j combinations\n    interactions_obs = jnp.einsum(\"bi,bj-&gt;bij\", A_obs, X_obs).reshape(batch_size, -1)\n    interactions_perm = jnp.einsum(\"bi,bj-&gt;bij\", A_perm, X_perm).reshape(batch_size, -1)\n\n    # Concatenate and label\n    X_batch = jnp.concatenate([X_obs, X_perm])\n    A_batch = jnp.concatenate([A_obs, A_perm])\n    interactions_batch = jnp.concatenate([interactions_obs, interactions_perm])\n    C_batch = jnp.concatenate(\n        [\n            jnp.zeros(batch_size),  # Observed: C=0\n            jnp.ones(batch_size),  # Permuted: C=1\n        ]\n    )\n\n    return TrainingBatch(X=X_batch, A=A_batch, C=C_batch, AX=interactions_batch)\n</code></pre>"},{"location":"reference/stochpw/#stochpw.effective_sample_size","title":"<code>effective_sample_size(weights)</code>","text":"<p>Compute effective sample size (ESS).</p> <p>ESS = (sum w)^2 / sum(w^2)</p> <p>Lower values indicate more extreme weights (fewer \"effective\" samples). ESS = n means uniform weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>ess</code> <code>Array(scalar)</code> <p>Effective sample size</p> Source code in <code>src/stochpw/diagnostics/weights.py</code> <pre><code>@jax.jit\ndef effective_sample_size(weights: Array) -&gt; Array:\n    \"\"\"\n    Compute effective sample size (ESS).\n\n    ESS = (sum w)^2 / sum(w^2)\n\n    Lower values indicate more extreme weights (fewer \"effective\" samples).\n    ESS = n means uniform weights.\n\n    Parameters\n    ----------\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    ess : jax.Array (scalar)\n        Effective sample size\n    \"\"\"\n    return jnp.sum(weights) ** 2 / jnp.sum(weights**2)\n</code></pre>"},{"location":"reference/stochpw/#stochpw.entropy_penalty","title":"<code>entropy_penalty(weights, eps=1e-07)</code>","text":"<p>Entropy regularization on weights.</p> <p>Penalizes weights that diverge from uniform, encouraging smoother reweighting and better effective sample size.</p> <p>The entropy of normalized weights is computed as:     H = -sum(p * log(p)) where p = weights / sum(weights)</p> <p>We return -H (negative entropy) as a penalty, since lower entropy (more peaked weights) should be penalized.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n))</code> <p>Importance weights</p> required <code>eps</code> <code>float</code> <p>Small constant for numerical stability</p> <code>1e-7</code> <p>Returns:</p> Name Type Description <code>penalty</code> <code>Array</code> <p>Negative entropy penalty (higher = more concentrated weights)</p> Notes <ul> <li>Uniform weights have maximum entropy</li> <li>Highly concentrated weights have low entropy (high penalty)</li> <li>Encourages effective sample size close to n</li> </ul> Source code in <code>src/stochpw/training/regularization.py</code> <pre><code>def entropy_penalty(weights: Array, eps: float = 1e-7) -&gt; Array:\n    \"\"\"\n    Entropy regularization on weights.\n\n    Penalizes weights that diverge from uniform, encouraging smoother\n    reweighting and better effective sample size.\n\n    The entropy of normalized weights is computed as:\n        H = -sum(p * log(p)) where p = weights / sum(weights)\n\n    We return -H (negative entropy) as a penalty, since lower entropy\n    (more peaked weights) should be penalized.\n\n    Parameters\n    ----------\n    weights : Array, shape (n,)\n        Importance weights\n    eps : float, default=1e-7\n        Small constant for numerical stability\n\n    Returns\n    -------\n    penalty : Array\n        Negative entropy penalty (higher = more concentrated weights)\n\n    Notes\n    -----\n    - Uniform weights have maximum entropy\n    - Highly concentrated weights have low entropy (high penalty)\n    - Encourages effective sample size close to n\n    \"\"\"\n    # Normalize weights to probability distribution\n    p = weights / (jnp.sum(weights) + eps)\n    p = jnp.clip(p, eps, 1.0)  # Avoid log(0)\n\n    # Compute entropy: H = -sum(p * log(p))\n    entropy = -jnp.sum(p * jnp.log(p))\n\n    # Return negative entropy as penalty (we want to maximize entropy)\n    return -entropy\n</code></pre>"},{"location":"reference/stochpw/#stochpw.exponential_loss","title":"<code>exponential_loss(logits, labels)</code>","text":"<p>Exponential loss for density ratio estimation.</p> <p>This is a proper scoring rule that can be more robust than logistic loss for density ratio estimation. It directly optimizes the exponential of the log density ratio.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Raw discriminator outputs</p> required <code>labels</code> <code>(Array, shape(batch_size))</code> <p>Binary labels (0 or 1)</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>Scalar loss value</p> Notes <p>The exponential loss is defined as:     L = E[exp(-y * f(x))] where y \u2208 {-1, +1} and f(x) are the logits. We convert labels from {0, 1} to {-1, +1} for this formulation.</p> Source code in <code>src/stochpw/training/losses.py</code> <pre><code>@jax.jit\ndef exponential_loss(logits: Array, labels: Array) -&gt; Array:\n    \"\"\"\n    Exponential loss for density ratio estimation.\n\n    This is a proper scoring rule that can be more robust than logistic loss\n    for density ratio estimation. It directly optimizes the exponential of\n    the log density ratio.\n\n    Parameters\n    ----------\n    logits : jax.Array, shape (batch_size,)\n        Raw discriminator outputs\n    labels : jax.Array, shape (batch_size,)\n        Binary labels (0 or 1)\n\n    Returns\n    -------\n    loss : float\n        Scalar loss value\n\n    Notes\n    -----\n    The exponential loss is defined as:\n        L = E[exp(-y * f(x))]\n    where y \u2208 {-1, +1} and f(x) are the logits.\n    We convert labels from {0, 1} to {-1, +1} for this formulation.\n    \"\"\"\n    # Convert labels from {0, 1} to {-1, +1}\n    y = 2 * labels - 1\n    # Exponential loss: E[exp(-y * logits)]\n    return jnp.mean(jnp.exp(-y * logits))\n</code></pre>"},{"location":"reference/stochpw/#stochpw.extract_weights","title":"<code>extract_weights(discriminator_fn, params, X, A, AX, eps=1e-07)</code>","text":"<p>Extract importance weights from trained discriminator.</p> <p>Converts discriminator probabilities to density ratio weights using the formula: w(a, x) = (1 - \u03b7(a, x)) / \u03b7(a, x) where \u03b7(a, x) = p(C=1 | a, x) is the probability from the permuted distribution.</p> <p>Parameters:</p> Name Type Description Default <code>discriminator_fn</code> <code>Callable</code> <p>Discriminator function (params, a, x) -&gt; logits</p> required <code>params</code> <code>dict</code> <p>Trained discriminator parameters</p> required <code>X</code> <code>(Array, shape(n, d_x))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n, d_a))</code> <p>Treatments</p> required <code>eps</code> <code>float</code> <p>Numerical stability constant (default: 1e-7)</p> <code>1e-07</code> <p>Returns:</p> Name Type Description <code>weights</code> <code>(Array, shape(n))</code> <p>Importance weights</p> Notes <p>The discriminator outputs logits for p(C=1 | a, x), where C=1 indicates the permuted distribution. The weight formula w = (1 - \u03b7) / \u03b7 gives the density ratio dP/dQ where P is the observed distribution and Q is the balanced (permuted) distribution. This up-weights observed pairs that look like they came from the observed distribution (low \u03b7).</p> Source code in <code>src/stochpw/weights.py</code> <pre><code>def extract_weights(\n    discriminator_fn: Callable[[PyTree, Array, Array, Array], Array],\n    params: PyTree,\n    X: Array,\n    A: Array,\n    AX: Array,\n    eps: float = 1e-7,\n) -&gt; Array:\n    \"\"\"\n    Extract importance weights from trained discriminator.\n\n    Converts discriminator probabilities to density ratio weights using\n    the formula: w(a, x) = (1 - \u03b7(a, x)) / \u03b7(a, x)\n    where \u03b7(a, x) = p(C=1 | a, x) is the probability from the permuted distribution.\n\n    Parameters\n    ----------\n    discriminator_fn : Callable\n        Discriminator function (params, a, x) -&gt; logits\n    params : dict\n        Trained discriminator parameters\n    X : jax.Array, shape (n, d_x)\n        Covariates\n    A : jax.Array, shape (n, d_a)\n        Treatments\n    eps : float, optional\n        Numerical stability constant (default: 1e-7)\n\n    Returns\n    -------\n    weights : jax.Array, shape (n,)\n        Importance weights\n\n    Notes\n    -----\n    The discriminator outputs logits for p(C=1 | a, x), where C=1 indicates\n    the permuted distribution. The weight formula w = (1 - \u03b7) / \u03b7 gives the\n    density ratio dP/dQ where P is the observed distribution and Q is the\n    balanced (permuted) distribution. This up-weights observed pairs that\n    look like they came from the observed distribution (low \u03b7).\n    \"\"\"\n    # Get discriminator probabilities\n    logits = discriminator_fn(params, A, X, AX)\n    eta = jax.nn.sigmoid(logits)\n\n    # Clip to avoid division issues\n    eta_clipped = jnp.clip(eta, eps, 1 - eps)\n\n    # Convert to density ratio: w = \u03b7 / (1 - \u03b7)\n    # Up-weight units that look PERMUTED (high \u03b7) to make observed distribution balanced\n    weights = eta_clipped / (1 - eta_clipped)\n\n    return weights\n</code></pre>"},{"location":"reference/stochpw/#stochpw.fit_discriminator","title":"<code>fit_discriminator(X, A, discriminator_fn, init_params, optimizer, num_epochs, batch_size, rng_key, loss_fn=logistic_loss, regularization_fn=None, regularization_strength=0.0, early_stopping=False, patience=10, min_delta=0.0001, eps=1e-07)</code>","text":"<p>Complete training loop for discriminator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n, d_x))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n, d_a))</code> <p>Treatments</p> required <code>discriminator_fn</code> <code>Callable</code> <p>Discriminator function (params, a, x, ax) -&gt; logits</p> required <code>init_params</code> <code>dict</code> <p>Initial parameters</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>Optax optimizer</p> required <code>num_epochs</code> <code>int</code> <p>Number of training epochs</p> required <code>batch_size</code> <code>int</code> <p>Mini-batch size</p> required <code>rng_key</code> <code>PRNGKey</code> <p>Random key for reproducibility</p> required <code>loss_fn</code> <code>Callable</code> <p>Loss function (logits, labels) -&gt; loss</p> <code>logistic_loss</code> <code>regularization_fn</code> <code>Callable</code> <p>Regularization function on weights (weights) -&gt; penalty</p> <code>None</code> <code>regularization_strength</code> <code>float</code> <p>Strength of regularization penalty</p> <code>0.0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to use early stopping based on validation loss</p> <code>False</code> <code>patience</code> <code>int</code> <p>Number of epochs to wait for improvement before stopping</p> <code>10</code> <code>min_delta</code> <code>float</code> <p>Minimum change in loss to qualify as improvement</p> <code>1e-4</code> <code>eps</code> <code>float</code> <p>Numerical stability constant for weight computation</p> <code>1e-7</code> <p>Returns:</p> Name Type Description <code>params</code> <code>dict</code> <p>Fitted discriminator parameters</p> <code>history</code> <code>dict</code> <p>Training history with keys 'loss' (list of losses per epoch)</p> Source code in <code>src/stochpw/training/loop.py</code> <pre><code>def fit_discriminator(\n    X: Array,\n    A: Array,\n    discriminator_fn: Callable[[PyTree, Array, Array, Array], Array],\n    init_params: PyTree,\n    optimizer: optax.GradientTransformation,\n    num_epochs: int,\n    batch_size: int,\n    rng_key: Array,\n    loss_fn: LossFn = logistic_loss,\n    regularization_fn: Callable[[Array], Array] | None = None,\n    regularization_strength: float = 0.0,\n    early_stopping: bool = False,\n    patience: int = 10,\n    min_delta: float = 1e-4,\n    eps: float = 1e-7,\n) -&gt; tuple[PyTree, dict[str, list[float]]]:\n    \"\"\"\n    Complete training loop for discriminator.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n, d_x)\n        Covariates\n    A : jax.Array, shape (n, d_a)\n        Treatments\n    discriminator_fn : Callable\n        Discriminator function (params, a, x, ax) -&gt; logits\n    init_params : dict\n        Initial parameters\n    optimizer : optax.GradientTransformation\n        Optax optimizer\n    num_epochs : int\n        Number of training epochs\n    batch_size : int\n        Mini-batch size\n    rng_key : jax.random.PRNGKey\n        Random key for reproducibility\n    loss_fn : Callable, default=logistic_loss\n        Loss function (logits, labels) -&gt; loss\n    regularization_fn : Callable, optional\n        Regularization function on weights (weights) -&gt; penalty\n    regularization_strength : float, default=0.0\n        Strength of regularization penalty\n    early_stopping : bool, default=False\n        Whether to use early stopping based on validation loss\n    patience : int, default=10\n        Number of epochs to wait for improvement before stopping\n    min_delta : float, default=1e-4\n        Minimum change in loss to qualify as improvement\n    eps : float, default=1e-7\n        Numerical stability constant for weight computation\n\n    Returns\n    -------\n    params : dict\n        Fitted discriminator parameters\n    history : dict\n        Training history with keys 'loss' (list of losses per epoch)\n    \"\"\"\n    n = X.shape[0]\n    opt_state = optimizer.init(init_params)\n\n    # Initialize state\n    state = TrainingState(\n        params=init_params,\n        opt_state=opt_state,\n        rng_key=rng_key,\n        epoch=0,\n        history={\"loss\": []},\n    )\n\n    # Early stopping state\n    best_loss = float(\"inf\")\n    best_params = init_params\n    epochs_without_improvement = 0\n\n    for epoch in range(num_epochs):\n        # Split RNG key for this epoch\n        epoch_key, state.rng_key = jax.random.split(state.rng_key)\n\n        # Shuffle data\n        perm = jax.random.permutation(epoch_key, n)\n        X_shuffled = X[perm]\n        A_shuffled = A[perm]\n\n        # Train on batches\n        epoch_losses = []\n        num_batches = n // batch_size\n\n        for i in range(num_batches):\n            batch_key, epoch_key = jax.random.split(epoch_key)\n\n            # Get batch indices\n            start_idx = i * batch_size\n            end_idx = start_idx + batch_size\n            batch_indices = jnp.arange(start_idx, end_idx)\n\n            # Create training batch\n            batch = create_training_batch(X_shuffled, A_shuffled, batch_indices, batch_key)\n\n            # Training step\n            result = train_step(\n                state,\n                batch,\n                discriminator_fn,\n                optimizer,\n                loss_fn_type=loss_fn,\n                regularization_fn=regularization_fn,\n                regularization_strength=regularization_strength,\n                eps=eps,\n            )\n            state = result.state\n            epoch_losses.append(float(result.loss))\n\n        # Record epoch loss\n        mean_epoch_loss = jnp.mean(jnp.array(epoch_losses))\n        state.history[\"loss\"].append(float(mean_epoch_loss))\n        state.epoch = epoch + 1\n\n        # Early stopping logic\n        if early_stopping:\n            if mean_epoch_loss &lt; best_loss - min_delta:\n                best_loss = mean_epoch_loss\n                best_params = state.params\n                epochs_without_improvement = 0\n            else:\n                epochs_without_improvement += 1\n\n            if epochs_without_improvement &gt;= patience:\n                # Restore best parameters\n                state.params = best_params\n                break\n\n    return state.params, state.history\n</code></pre>"},{"location":"reference/stochpw/#stochpw.logistic_loss","title":"<code>logistic_loss(logits, labels)</code>","text":"<p>Binary cross-entropy loss for discriminator.</p> <p>Uses numerically stable log-sigmoid implementation.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Raw discriminator outputs</p> required <code>labels</code> <code>(Array, shape(batch_size))</code> <p>Binary labels (0 or 1)</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>Scalar loss value</p> Source code in <code>src/stochpw/training/losses.py</code> <pre><code>@jax.jit\ndef logistic_loss(logits: Array, labels: Array) -&gt; Array:\n    \"\"\"\n    Binary cross-entropy loss for discriminator.\n\n    Uses numerically stable log-sigmoid implementation.\n\n    Parameters\n    ----------\n    logits : jax.Array, shape (batch_size,)\n        Raw discriminator outputs\n    labels : jax.Array, shape (batch_size,)\n        Binary labels (0 or 1)\n\n    Returns\n    -------\n    loss : float\n        Scalar loss value\n    \"\"\"\n    # Use optax's stable implementation\n    return optax.sigmoid_binary_cross_entropy(logits, labels).mean()\n</code></pre>"},{"location":"reference/stochpw/#stochpw.lp_weight_penalty","title":"<code>lp_weight_penalty(weights, p=2.0)</code>","text":"<p>L_p penalty on weight deviations from uniform.</p> <p>Penalizes weights that deviate from 1, encouraging more uniform weighting. Different values of p produce different behaviors: - p=1: L1 penalty (sparse, robust to outliers) - p=2: L2 penalty (smooth, sensitive to large deviations)</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n))</code> <p>Importance weights</p> required <code>p</code> <code>float</code> <p>The power for the L_p norm (must be &gt;= 1)</p> <code>2.0</code> <p>Returns:</p> Name Type Description <code>penalty</code> <code>Array</code> <p>L_p penalty on weight deviations</p> Notes <p>Computed as sum(|weights - 1|^p), which penalizes deviation from uniform weights.</p> Source code in <code>src/stochpw/training/regularization.py</code> <pre><code>def lp_weight_penalty(weights: Array, p: float = 2.0) -&gt; Array:\n    \"\"\"\n    L_p penalty on weight deviations from uniform.\n\n    Penalizes weights that deviate from 1, encouraging more uniform weighting.\n    Different values of p produce different behaviors:\n    - p=1: L1 penalty (sparse, robust to outliers)\n    - p=2: L2 penalty (smooth, sensitive to large deviations)\n\n    Parameters\n    ----------\n    weights : Array, shape (n,)\n        Importance weights\n    p : float, default=2.0\n        The power for the L_p norm (must be &gt;= 1)\n\n    Returns\n    -------\n    penalty : Array\n        L_p penalty on weight deviations\n\n    Notes\n    -----\n    Computed as sum(|weights - 1|^p), which penalizes deviation\n    from uniform weights.\n    \"\"\"\n    deviations = jnp.abs(weights - 1.0)\n    return jnp.sum(deviations**p)\n</code></pre>"},{"location":"reference/stochpw/#stochpw.permute_treatment","title":"<code>permute_treatment(A, rng_key)</code>","text":"<p>Randomly permute treatment assignments.</p> <p>For multi-dimensional A, this permutes entire rows (treatment vectors) together, not individual elements.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>(Array, shape(batch_size) or (batch_size, n_treatments))</code> <p>Intervention assignments</p> required <code>rng_key</code> <code>PRNGKey</code> <p>Random key for permutation</p> required <p>Returns:</p> Name Type Description <code>A_perm</code> <code>jax.Array, same shape as A</code> <p>Permuted treatment assignments</p> Source code in <code>src/stochpw/utils.py</code> <pre><code>def permute_treatment(A: Array, rng_key: Array) -&gt; Array:\n    \"\"\"\n    Randomly permute treatment assignments.\n\n    For multi-dimensional A, this permutes entire rows (treatment vectors)\n    together, not individual elements.\n\n    Parameters\n    ----------\n    A : jax.Array, shape (batch_size,) or (batch_size, n_treatments)\n        Intervention assignments\n    rng_key : jax.random.PRNGKey\n        Random key for permutation\n\n    Returns\n    -------\n    A_perm : jax.Array, same shape as A\n        Permuted treatment assignments\n    \"\"\"\n    import jax.random as random\n\n    n_samples = A.shape[0]\n    perm_indices = random.permutation(rng_key, n_samples)\n    return A[perm_indices]\n</code></pre>"},{"location":"reference/stochpw/#stochpw.roc_curve","title":"<code>roc_curve(weights, true_labels, max_points=100)</code>","text":"<p>Compute ROC curve from weights for discriminator performance.</p> <p>Given weights w(x,a), infers eta(x,a) = w(x,a) / (1 + w(x,a)) and computes the ROC curve for discriminating between observed and permuted data.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights from permutation weighting</p> required <code>true_labels</code> <code>(Array, shape(n_samples))</code> <p>True binary labels (0=observed, 1=permuted)</p> required <code>max_points</code> <code>int</code> <p>Maximum number of points in the ROC curve (for computational efficiency)</p> <code>100</code> <p>Returns:</p> Name Type Description <code>fpr</code> <code>Array</code> <p>False positive rates at each threshold</p> <code>tpr</code> <code>Array</code> <p>True positive rates at each threshold</p> <code>thresholds</code> <code>Array</code> <p>Thresholds used to compute fpr and tpr</p> Notes <p>The ROC curve is the most important diagnostic for discriminator quality. A good discriminator will have high AUC (area under curve), indicating it can successfully distinguish between observed and permuted data.</p> <p>The discriminator probability eta is inferred from weights as:     eta(x,a) = w(x,a) / (1 + w(x,a))</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # After fitting a weighter\n&gt;&gt;&gt; weights = weighter.predict(X, A)\n&gt;&gt;&gt; # Create permuted data and labels\n&gt;&gt;&gt; weights_perm = weighter.predict(X, A_permuted)\n&gt;&gt;&gt; all_weights = jnp.concatenate([weights, weights_perm])\n&gt;&gt;&gt; labels = jnp.concatenate([jnp.zeros(len(weights)), jnp.ones(len(weights_perm))])\n&gt;&gt;&gt; fpr, tpr, thresholds = roc_curve(all_weights, labels)\n&gt;&gt;&gt; auc = jnp.trapezoid(tpr, fpr)\n</code></pre> Source code in <code>src/stochpw/diagnostics/advanced.py</code> <pre><code>def roc_curve(\n    weights: Array, true_labels: Array, max_points: int = 100\n) -&gt; tuple[Array, Array, Array]:\n    \"\"\"\n    Compute ROC curve from weights for discriminator performance.\n\n    Given weights w(x,a), infers eta(x,a) = w(x,a) / (1 + w(x,a)) and\n    computes the ROC curve for discriminating between observed and\n    permuted data.\n\n    Parameters\n    ----------\n    weights : jax.Array, shape (n_samples,)\n        Sample weights from permutation weighting\n    true_labels : jax.Array, shape (n_samples,)\n        True binary labels (0=observed, 1=permuted)\n    max_points : int, default=100\n        Maximum number of points in the ROC curve (for computational efficiency)\n\n    Returns\n    -------\n    fpr : jax.Array\n        False positive rates at each threshold\n    tpr : jax.Array\n        True positive rates at each threshold\n    thresholds : jax.Array\n        Thresholds used to compute fpr and tpr\n\n    Notes\n    -----\n    The ROC curve is the most important diagnostic for discriminator quality.\n    A good discriminator will have high AUC (area under curve), indicating\n    it can successfully distinguish between observed and permuted data.\n\n    The discriminator probability eta is inferred from weights as:\n        eta(x,a) = w(x,a) / (1 + w(x,a))\n\n    Examples\n    --------\n    &gt;&gt;&gt; # After fitting a weighter\n    &gt;&gt;&gt; weights = weighter.predict(X, A)\n    &gt;&gt;&gt; # Create permuted data and labels\n    &gt;&gt;&gt; weights_perm = weighter.predict(X, A_permuted)\n    &gt;&gt;&gt; all_weights = jnp.concatenate([weights, weights_perm])\n    &gt;&gt;&gt; labels = jnp.concatenate([jnp.zeros(len(weights)), jnp.ones(len(weights_perm))])\n    &gt;&gt;&gt; fpr, tpr, thresholds = roc_curve(all_weights, labels)\n    &gt;&gt;&gt; auc = jnp.trapezoid(tpr, fpr)\n    \"\"\"\n    # Infer eta from weights: eta = w / (1 + w)\n    eta = weights / (1.0 + weights)\n\n    # Use linearly spaced thresholds for efficiency\n    min_eta = float(jnp.min(eta))\n    max_eta = float(jnp.max(eta))\n    thresholds = jnp.linspace(max_eta + 1e-6, min_eta - 1e-6, max_points)\n\n    # Compute TPR and FPR for each threshold using vectorized operations\n    n_positive = float(jnp.sum(true_labels == 1))\n    n_negative = float(jnp.sum(true_labels == 0))\n\n    # Vectorized computation\n    # For each threshold, count predictions &gt;= threshold\n    predictions_matrix = eta[:, jnp.newaxis] &gt;= thresholds[jnp.newaxis, :]\n\n    # True positives: predictions==1 AND true_labels==1\n    tp = jnp.sum(predictions_matrix &amp; (true_labels[:, jnp.newaxis] == 1), axis=0)\n\n    # False positives: predictions==1 AND true_labels==0\n    fp = jnp.sum(predictions_matrix &amp; (true_labels[:, jnp.newaxis] == 0), axis=0)\n\n    # Compute rates\n    tpr_array = tp / n_positive if n_positive &gt; 0 else jnp.zeros_like(tp)\n    fpr_array = fp / n_negative if n_negative &gt; 0 else jnp.zeros_like(fp)\n\n    return fpr_array, tpr_array, thresholds\n</code></pre>"},{"location":"reference/stochpw/#stochpw.standardized_mean_difference","title":"<code>standardized_mean_difference(X, A, weights)</code>","text":"<p>Compute weighted standardized mean difference for each covariate.</p> <p>For binary treatment, computes SMD between weighted treatment groups. For continuous treatment, computes weighted correlation with covariates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n_samples, 1) or (n_samples,))</code> <p>Treatments</p> required <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>smd</code> <code>(Array, shape(n_features))</code> <p>SMD or correlation for each covariate</p> Source code in <code>src/stochpw/diagnostics/balance.py</code> <pre><code>def standardized_mean_difference(X: Array, A: Array, weights: Array) -&gt; Array:\n    \"\"\"\n    Compute weighted standardized mean difference for each covariate.\n\n    For binary treatment, computes SMD between weighted treatment groups.\n    For continuous treatment, computes weighted correlation with covariates.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n_samples, n_features)\n        Covariates\n    A : jax.Array, shape (n_samples, 1) or (n_samples,)\n        Treatments\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    smd : jax.Array, shape (n_features,)\n        SMD or correlation for each covariate\n    \"\"\"\n    # Ensure A is 1D for this computation\n    a_1d = A.squeeze() if A.ndim == 2 else A\n\n    # Check if A is binary\n    unique_a = jnp.unique(a_1d)\n    is_binary = len(unique_a) == 2\n\n    if is_binary:\n        # Binary treatment: compute SMD\n        a0, a1 = unique_a[0], unique_a[1]\n        mask_0 = a_1d == a0\n        mask_1 = a_1d == a1\n\n        # Weighted means\n        weights_0 = weights * mask_0\n        weights_1 = weights * mask_1\n\n        sum_weights_0 = jnp.sum(weights_0)\n        sum_weights_1 = jnp.sum(weights_1)\n\n        mean_0 = jnp.average(X, axis=0, weights=weights_0)\n        mean_1 = jnp.average(X, axis=0, weights=weights_1)\n\n        # Weighted standard deviations\n        var_0 = jnp.sum(weights_0[:, None] * (X - mean_0) ** 2, axis=0) / (sum_weights_0 + 1e-10)\n        var_1 = jnp.sum(weights_1[:, None] * (X - mean_1) ** 2, axis=0) / (sum_weights_1 + 1e-10)\n\n        # Pooled standard deviation\n        pooled_std = jnp.sqrt((var_0 + var_1) / 2)\n\n        # SMD\n        smd = (mean_1 - mean_0) / (pooled_std + 1e-10)\n\n    else:\n        # Continuous treatment: compute weighted correlation\n        # Normalize weights\n        w_norm = weights / jnp.sum(weights)\n\n        # Weighted means\n        mean_a = jnp.sum(w_norm * a_1d)\n        mean_X = jnp.sum(w_norm[:, None] * X, axis=0)\n\n        # Weighted covariance\n        cov = jnp.sum(w_norm[:, None] * (a_1d[:, None] - mean_a) * (X - mean_X), axis=0)\n\n        # Weighted standard deviations\n        std_a = jnp.sqrt(jnp.sum(w_norm * (a_1d - mean_a) ** 2))\n        std_X = jnp.sqrt(jnp.sum(w_norm[:, None] * (X - mean_X) ** 2, axis=0))\n\n        # Correlation\n        smd = cov / (std_a * std_X + 1e-10)\n\n    return smd\n</code></pre>"},{"location":"reference/stochpw/#stochpw.standardized_mean_difference_se","title":"<code>standardized_mean_difference_se(X, A, weights)</code>","text":"<p>Compute standard errors for standardized mean differences.</p> <p>Uses the bootstrap-style approximation for weighted SMD standard errors.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n_samples, 1) or (n_samples,))</code> <p>Treatments</p> required <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>se</code> <code>(Array, shape(n_features))</code> <p>Standard error for each covariate's SMD</p> Source code in <code>src/stochpw/diagnostics/balance.py</code> <pre><code>def standardized_mean_difference_se(X: Array, A: Array, weights: Array) -&gt; Array:\n    \"\"\"\n    Compute standard errors for standardized mean differences.\n\n    Uses the bootstrap-style approximation for weighted SMD standard errors.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n_samples, n_features)\n        Covariates\n    A : jax.Array, shape (n_samples, 1) or (n_samples,)\n        Treatments\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    se : jax.Array, shape (n_features,)\n        Standard error for each covariate's SMD\n    \"\"\"\n    # Ensure A is 1D\n    a_1d = A.squeeze() if A.ndim == 2 else A\n\n    # Check if A is binary\n    unique_a = jnp.unique(a_1d)\n    is_binary = len(unique_a) == 2\n\n    if is_binary:\n        # Binary treatment: bootstrap-style SE\n        a0, a1 = unique_a[0], unique_a[1]\n        mask_0 = a_1d == a0\n        mask_1 = a_1d == a1\n\n        # Effective sample sizes\n        weights_0 = weights * mask_0\n        weights_1 = weights * mask_1\n        ess_0 = jnp.sum(weights_0) ** 2 / jnp.sum(weights_0**2)\n        ess_1 = jnp.sum(weights_1) ** 2 / jnp.sum(weights_1**2)\n\n        # Approximate SE using ESS\n        # SE(SMD) \u2248 sqrt(1/n_0 + 1/n_1 + SMD\u00b2/(2*(n_0 + n_1)))\n        # Use ESS instead of n\n        smd = standardized_mean_difference(X, A, weights)\n        se = jnp.sqrt(1.0 / ess_0 + 1.0 / ess_1 + smd**2 / (2.0 * (ess_0 + ess_1)))\n\n    else:\n        # Continuous treatment: approximate SE for correlation\n        n_eff = jnp.sum(weights) ** 2 / jnp.sum(weights**2)\n        # SE(correlation) \u2248 1/sqrt(n)\n        se = jnp.ones(X.shape[1]) / jnp.sqrt(n_eff)\n\n    return se\n</code></pre>"},{"location":"reference/stochpw/#stochpw.train_step","title":"<code>train_step(state, batch, discriminator_fn, optimizer, loss_fn_type=logistic_loss, regularization_fn=None, regularization_strength=0.0, eps=1e-07)</code>","text":"<p>Single training step (JIT-compiled).</p> <p>Computes loss, gradients, and updates parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>TrainingState</code> <p>Current training state</p> required <code>batch</code> <code>TrainingBatch</code> <p>Training batch</p> required <code>discriminator_fn</code> <code>Callable</code> <p>Discriminator function (params, a, x, ax) -&gt; logits</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>Optax optimizer</p> required <code>loss_fn_type</code> <code>Callable</code> <p>Loss function (logits, labels) -&gt; loss</p> <code>logistic_loss</code> <code>regularization_fn</code> <code>Callable</code> <p>Regularization function on weights (weights) -&gt; penalty</p> <code>None</code> <code>regularization_strength</code> <code>float</code> <p>Strength of regularization penalty</p> <code>0.0</code> <code>eps</code> <code>float</code> <p>Numerical stability constant for weight computation</p> <code>1e-7</code> <p>Returns:</p> Type Description <code>TrainingStepResult</code> <p>Updated state and loss value</p> Source code in <code>src/stochpw/training/loop.py</code> <pre><code>def train_step(\n    state: TrainingState,\n    batch: TrainingBatch,\n    discriminator_fn: Callable[[PyTree, Array, Array, Array], Array],\n    optimizer: optax.GradientTransformation,\n    loss_fn_type: LossFn = logistic_loss,\n    regularization_fn: Callable[[Array], Array] | None = None,\n    regularization_strength: float = 0.0,\n    eps: float = 1e-7,\n) -&gt; TrainingStepResult:\n    \"\"\"\n    Single training step (JIT-compiled).\n\n    Computes loss, gradients, and updates parameters.\n\n    Parameters\n    ----------\n    state : TrainingState\n        Current training state\n    batch : TrainingBatch\n        Training batch\n    discriminator_fn : Callable\n        Discriminator function (params, a, x, ax) -&gt; logits\n    optimizer : optax.GradientTransformation\n        Optax optimizer\n    loss_fn_type : Callable, default=logistic_loss\n        Loss function (logits, labels) -&gt; loss\n    regularization_fn : Callable, optional\n        Regularization function on weights (weights) -&gt; penalty\n    regularization_strength : float, default=0.0\n        Strength of regularization penalty\n    eps : float, default=1e-7\n        Numerical stability constant for weight computation\n\n    Returns\n    -------\n    TrainingStepResult\n        Updated state and loss value\n    \"\"\"\n\n    def loss_fn(params: PyTree) -&gt; Array:\n        logits = discriminator_fn(params, batch.A, batch.X, batch.AX)\n        loss = loss_fn_type(logits, batch.C)\n\n        # Add weight-based regularization if specified\n        if regularization_fn is not None and regularization_strength &gt; 0:\n            # Compute weights from discriminator output (only for observed data)\n            # Filter to C=0 (observed data) for weight computation\n            observed_mask = batch.C == 0\n            observed_logits = logits[observed_mask]\n            eta = jax.nn.sigmoid(observed_logits)\n            eta_clipped = jnp.clip(eta, eps, 1 - eps)\n            weights = eta_clipped / (1 - eta_clipped)\n\n            # Apply regularization on weights\n            penalty = regularization_fn(weights)\n            loss = loss + regularization_strength * penalty\n\n        return loss\n\n    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n    updates, opt_state = optimizer.update(grads, state.opt_state, state.params)\n    params = optax.apply_updates(state.params, updates)\n\n    new_state = TrainingState(\n        params=params,  # type: ignore[arg-type]\n        opt_state=opt_state,\n        rng_key=state.rng_key,\n        epoch=state.epoch,\n        history=state.history,\n    )\n\n    return TrainingStepResult(state=new_state, loss=loss)\n</code></pre>"},{"location":"reference/stochpw/#stochpw.validate_inputs","title":"<code>validate_inputs(X, A)</code>","text":"<p>Validate and convert inputs to JAX arrays.</p> <p>Checks: - X and A have compatible shapes (same number of samples) - No NaNs or Infs - Sufficient variation in A - Converts numpy arrays to JAX arrays if needed</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(array - like, shape(n_samples) or (n_samples, n_treatments))</code> <p>Intervention/treatment assignments</p> required <p>Returns:</p> Name Type Description <code>X_jax</code> <code>Array</code> <p>Validated and converted X</p> <code>A_jax</code> <code>Array</code> <p>Validated and converted A</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid (incompatible shapes, NaNs, no variation, etc.)</p> Source code in <code>src/stochpw/utils.py</code> <pre><code>def validate_inputs(\n    X: Array | NDArray[Any],  # type: ignore[misc]\n    A: Array | NDArray[Any],  # type: ignore[misc]\n) -&gt; tuple[Array, Array]:\n    \"\"\"\n    Validate and convert inputs to JAX arrays.\n\n    Checks:\n    - X and A have compatible shapes (same number of samples)\n    - No NaNs or Infs\n    - Sufficient variation in A\n    - Converts numpy arrays to JAX arrays if needed\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Covariates\n    A : array-like, shape (n_samples,) or (n_samples, n_treatments)\n        Intervention/treatment assignments\n\n    Returns\n    -------\n    X_jax : jax.Array\n        Validated and converted X\n    A_jax : jax.Array\n        Validated and converted A\n\n    Raises\n    ------\n    ValueError\n        If inputs are invalid (incompatible shapes, NaNs, no variation, etc.)\n    \"\"\"\n    # Convert to JAX arrays if needed\n    x_jax = jnp.array(X) if isinstance(X, np.ndarray) else X\n    a_jax = jnp.array(A) if isinstance(A, np.ndarray) else A\n\n    # Check shapes\n    if x_jax.ndim != 2:\n        raise ValueError(f\"X must be 2-dimensional, got shape {x_jax.shape}\")\n\n    # Ensure A is at least 1D\n    if a_jax.ndim == 0:\n        raise ValueError(\"A must be at least 1-dimensional, got scalar\")\n    if a_jax.ndim == 1:\n        a_jax = a_jax.reshape(-1, 1)  # Make it (n, 1) for consistency\n    if a_jax.ndim &gt; 2:\n        raise ValueError(f\"A must be 1 or 2-dimensional, got shape {a_jax.shape}\")\n\n    # Check number of samples match\n    if x_jax.shape[0] != a_jax.shape[0]:\n        raise ValueError(\n            \"X and A must have same number of samples: \"\n            + f\"X has {x_jax.shape[0]}, A has {a_jax.shape[0]}\"\n        )\n\n    # Check for NaNs or Infs\n    _check_array_validity(x_jax, \"X\")\n    _check_array_validity(a_jax, \"A\")\n\n    # Check for sufficient variation in A\n    _validate_treatment_variation(a_jax)\n\n    # Check minimum sample size\n    if x_jax.shape[0] &lt; 10:\n        raise ValueError(f\"Need at least 10 samples for training, got {x_jax.shape[0]}\")\n\n    return x_jax, a_jax\n</code></pre>"},{"location":"reference/stochpw/#stochpw.weight_statistics","title":"<code>weight_statistics(weights)</code>","text":"<p>Compute comprehensive statistics about weight distribution.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>stats</code> <code>dict</code> <p>Dictionary with weight statistics: - mean: Mean weight - std: Standard deviation of weights - min: Minimum weight - max: Maximum weight - cv: Coefficient of variation (std/mean) - entropy: Entropy of normalized weights - max_ratio: Ratio of max to min weight - n_extreme: Number of weights &gt; 10x mean</p> Notes <p>Useful for diagnosing weight quality and potential issues with extreme or highly variable weights.</p> Source code in <code>src/stochpw/diagnostics/advanced.py</code> <pre><code>def weight_statistics(weights: Array) -&gt; dict[str, float]:\n    \"\"\"\n    Compute comprehensive statistics about weight distribution.\n\n    Parameters\n    ----------\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    stats : dict\n        Dictionary with weight statistics:\n        - mean: Mean weight\n        - std: Standard deviation of weights\n        - min: Minimum weight\n        - max: Maximum weight\n        - cv: Coefficient of variation (std/mean)\n        - entropy: Entropy of normalized weights\n        - max_ratio: Ratio of max to min weight\n        - n_extreme: Number of weights &gt; 10x mean\n\n    Notes\n    -----\n    Useful for diagnosing weight quality and potential issues with\n    extreme or highly variable weights.\n    \"\"\"\n    mean_w = float(jnp.mean(weights))\n    std_w = float(jnp.std(weights))\n    min_w = float(jnp.min(weights))\n    max_w = float(jnp.max(weights))\n\n    # Coefficient of variation\n    cv = std_w / mean_w if mean_w &gt; 0 else float(\"inf\")\n\n    # Entropy of normalized weights\n    w_norm = weights / jnp.sum(weights)\n    # Add small constant to avoid log(0)\n    entropy = float(-jnp.sum(w_norm * jnp.log(w_norm + 1e-10)))\n\n    # Maximum weight ratio\n    max_ratio = max_w / min_w if min_w &gt; 0 else float(\"inf\")\n\n    # Number of extreme weights (&gt; 10x mean)\n    n_extreme = int(jnp.sum(weights &gt; 10 * mean_w))\n\n    return {\n        \"mean\": mean_w,\n        \"std\": std_w,\n        \"min\": min_w,\n        \"max\": max_w,\n        \"cv\": cv,\n        \"entropy\": entropy,\n        \"max_ratio\": max_ratio,\n        \"n_extreme\": n_extreme,\n    }\n</code></pre>"},{"location":"reference/stochpw/core/","title":"core","text":"<p>Main API for permutation weighting.</p>"},{"location":"reference/stochpw/core/#stochpw.core.NotFittedError","title":"<code>NotFittedError</code>","text":"<p>               Bases: <code>Exception</code></p> <p>Exception raised when predict is called before fit.</p>"},{"location":"reference/stochpw/core/#stochpw.core.PermutationWeighter","title":"<code>PermutationWeighter(discriminator=None, optimizer=None, num_epochs=100, batch_size=256, random_state=None, loss_fn=logistic_loss, regularization_fn=None, regularization_strength=0.0, early_stopping=False, patience=10, min_delta=0.0001)</code>","text":"<p>Main class for permutation weighting (sklearn-style API).</p> <p>Permutation weighting learns importance weights by training a discriminator to distinguish between observed (X, A) pairs and permuted (X, A') pairs, where treatments are randomly shuffled to break the association with covariates.</p> <p>Parameters:</p> Name Type Description Default <code>discriminator</code> <code>BaseDiscriminator</code> <p>Discriminator model instance. If None, uses LinearDiscriminator(). Can be any subclass of BaseDiscriminator (e.g., LinearDiscriminator, MLPDiscriminator, or custom discriminator).</p> <code>None</code> <code>optimizer</code> <code>GradientTransformation</code> <p>Optax optimizer. If None, uses Adam with learning rate 1e-3.</p> <code>None</code> <code>num_epochs</code> <code>int</code> <p>Number of training epochs</p> <code>100</code> <code>batch_size</code> <code>int</code> <p>Mini-batch size for training</p> <code>256</code> <code>random_state</code> <code>int</code> <p>Random seed for reproducibility</p> <code>None</code> <code>loss_fn</code> <code>Callable</code> <p>Loss function (logits, labels) -&gt; loss. Can be logistic_loss, exponential_loss, or brier_loss.</p> <code>logistic_loss</code> <code>regularization_fn</code> <code>Callable</code> <p>Regularization function on weights (weights) -&gt; penalty. Use entropy_penalty or lp_weight_penalty (with p=1 or p=2).</p> <code>None</code> <code>regularization_strength</code> <code>float</code> <p>Strength of regularization penalty</p> <code>0.0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to use early stopping based on training loss</p> <code>False</code> <code>patience</code> <code>int</code> <p>Number of epochs to wait for improvement before early stopping</p> <code>10</code> <code>min_delta</code> <code>float</code> <p>Minimum change in loss to qualify as improvement for early stopping</p> <code>1e-4</code> <p>Attributes:</p> Name Type Description <code>params_</code> <code>dict</code> <p>Fitted discriminator parameters (set after fit)</p> <code>history_</code> <code>dict</code> <p>Training history with 'loss' key (set after fit)</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from stochpw import PermutationWeighter, LinearDiscriminator, MLPDiscriminator\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Generate synthetic data\n&gt;&gt;&gt; X = jnp.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n&gt;&gt;&gt; A = jnp.array([[0.0], [1.0], [0.0]])\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Default: Linear discriminator\n&gt;&gt;&gt; weighter = PermutationWeighter(num_epochs=50, random_state=42)\n&gt;&gt;&gt; weighter.fit(X, A)\n&gt;&gt;&gt; weights = weighter.predict(X, A)\n&gt;&gt;&gt;\n&gt;&gt;&gt; # MLP discriminator with custom architecture\n&gt;&gt;&gt; mlp_disc = MLPDiscriminator(hidden_dims=[128, 64], activation=\"tanh\")\n&gt;&gt;&gt; weighter = PermutationWeighter(discriminator=mlp_disc, num_epochs=50, random_state=42)\n&gt;&gt;&gt; weighter.fit(X, A)\n&gt;&gt;&gt; weights = weighter.predict(X, A)\n</code></pre> Source code in <code>src/stochpw/core.py</code> <pre><code>def __init__(\n    self,\n    discriminator: BaseDiscriminator | None = None,\n    optimizer: optax.GradientTransformation | None = None,\n    num_epochs: int = 100,\n    batch_size: int = 256,\n    random_state: int | None = None,\n    loss_fn: LossFn = logistic_loss,\n    regularization_fn: Callable[[Array], Array] | None = None,\n    regularization_strength: float = 0.0,\n    early_stopping: bool = False,\n    patience: int = 10,\n    min_delta: float = 1e-4,\n):\n    self.discriminator: BaseDiscriminator = (\n        discriminator if discriminator is not None else LinearDiscriminator()\n    )\n    self.optimizer: optax.GradientTransformation | None = optimizer\n    self.num_epochs: int = num_epochs\n    self.batch_size: int = batch_size\n    self.random_state: int | None = random_state\n    self.loss_fn: LossFn = loss_fn\n    self.regularization_fn: Callable[[Array], Array] | None = regularization_fn\n    self.regularization_strength: float = regularization_strength\n    self.early_stopping: bool = early_stopping\n    self.patience: int = patience\n    self.min_delta: float = min_delta\n\n    # Fitted attributes (set by fit())\n    self.params_: PyTree | None = None\n    self.history_: dict[str, list[float]] | None = None\n    self._input_dim: int | None = None\n</code></pre>"},{"location":"reference/stochpw/core/#stochpw.core.PermutationWeighter.fit","title":"<code>fit(X, A)</code>","text":"<p>Fit discriminator on data (sklearn-style).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(array - like, shape(n_samples) or (n_samples, n_treatments))</code> <p>Treatment assignments</p> required <p>Returns:</p> Name Type Description <code>self</code> <code>PermutationWeighter</code> <p>Fitted estimator (for method chaining)</p> Source code in <code>src/stochpw/core.py</code> <pre><code>def fit(\n    self,\n    X: Array | NDArray[Any],  # type: ignore[misc]\n    A: Array | NDArray[Any],  # type: ignore[misc]\n) -&gt; \"PermutationWeighter\":\n    \"\"\"\n    Fit discriminator on data (sklearn-style).\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Covariates\n    A : array-like, shape (n_samples,) or (n_samples, n_treatments)\n        Treatment assignments\n\n    Returns\n    -------\n    self : PermutationWeighter\n        Fitted estimator (for method chaining)\n    \"\"\"\n    # Validate and convert inputs\n    x_val, a_val = validate_inputs(X, A)\n\n    # Set up RNG key\n    if self.random_state is not None:\n        rng_key = jax.random.PRNGKey(self.random_state)\n    else:\n        rng_key = jax.random.PRNGKey(0)\n\n    # Determine dimensions\n    d_a = a_val.shape[1]\n    d_x = x_val.shape[1]\n\n    # Initialize discriminator parameters\n    init_key, train_key = jax.random.split(rng_key)\n    init_params = self.discriminator.init_params(init_key, d_a, d_x)\n\n    # Set up optimizer\n    if self.optimizer is None:\n        optimizer = optax.adam(1e-3)\n    else:\n        optimizer = self.optimizer\n\n    # Fit discriminator\n    self.params_, self.history_ = fit_discriminator(\n        X=x_val,\n        A=a_val,\n        discriminator_fn=self.discriminator.apply,\n        init_params=init_params,\n        optimizer=optimizer,\n        num_epochs=self.num_epochs,\n        batch_size=self.batch_size,\n        rng_key=train_key,\n        loss_fn=self.loss_fn,\n        regularization_fn=self.regularization_fn,\n        regularization_strength=self.regularization_strength,\n        early_stopping=self.early_stopping,\n        patience=self.patience,\n        min_delta=self.min_delta,\n    )\n\n    return self\n</code></pre>"},{"location":"reference/stochpw/core/#stochpw.core.PermutationWeighter.predict","title":"<code>predict(X, A)</code>","text":"<p>Predict importance weights for given data (sklearn-style).</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(array - like, shape(n_samples) or (n_samples, n_treatments))</code> <p>Treatment assignments</p> required <p>Returns:</p> Name Type Description <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Importance weights</p> <p>Raises:</p> Type Description <code>NotFittedError</code> <p>If called before fit()</p> Source code in <code>src/stochpw/core.py</code> <pre><code>def predict(\n    self,\n    X: Array | NDArray[Any],  # type: ignore[misc]\n    A: Array | NDArray[Any],  # type: ignore[misc]\n) -&gt; Array:\n    \"\"\"\n    Predict importance weights for given data (sklearn-style).\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Covariates\n    A : array-like, shape (n_samples,) or (n_samples, n_treatments)\n        Treatment assignments\n\n    Returns\n    -------\n    weights : jax.Array, shape (n_samples,)\n        Importance weights\n\n    Raises\n    ------\n    NotFittedError\n        If called before fit()\n    \"\"\"\n    if self.params_ is None:\n        raise NotFittedError(\n            \"This PermutationWeighter instance is not fitted yet. \"\n            + \"Call 'fit' with appropriate arguments before using 'predict'.\"\n        )\n\n    # Validate and convert inputs\n    x_val, a_val = validate_inputs(X, A)\n\n    # Compute interactions\n    ax = jnp.einsum(\"bi,bj-&gt;bij\", a_val, x_val).reshape(x_val.shape[0], -1)\n\n    # Extract weights\n    weights = extract_weights(self.discriminator.apply, self.params_, x_val, a_val, ax)\n\n    return weights\n</code></pre>"},{"location":"reference/stochpw/data/","title":"data","text":"<p>Data structures for permutation weighting.</p>"},{"location":"reference/stochpw/data/#stochpw.data.TrainingBatch","title":"<code>TrainingBatch(X, A, C, AX)</code>  <code>dataclass</code>","text":"<p>A batch of data for discriminator training.</p>"},{"location":"reference/stochpw/data/#stochpw.data.TrainingState","title":"<code>TrainingState(params, opt_state, rng_key, epoch, history)</code>  <code>dataclass</code>","text":"<p>State of the training process.</p>"},{"location":"reference/stochpw/data/#stochpw.data.TrainingStepResult","title":"<code>TrainingStepResult(state, loss)</code>  <code>dataclass</code>","text":"<p>Result of a single training step.</p>"},{"location":"reference/stochpw/data/#stochpw.data.WeightedData","title":"<code>WeightedData(X, A, weights)</code>  <code>dataclass</code>","text":"<p>Data with computed importance weights.</p>"},{"location":"reference/stochpw/plotting/","title":"plotting","text":"<p>Visualization utilities for permutation weighting diagnostics.</p>"},{"location":"reference/stochpw/plotting/#stochpw.plotting.plot_balance_diagnostics","title":"<code>plot_balance_diagnostics(X, A, weights, unweighted_weights=None, feature_names=None)</code>","text":"<p>Create balance diagnostic plot with standard errors.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n_samples, 1) or (n_samples,))</code> <p>Treatments</p> required <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights after permutation weighting</p> required <code>unweighted_weights</code> <code>Array</code> <p>Uniform weights for comparison. If None, uses uniform weights.</p> <code>None</code> <code>feature_names</code> <code>list of str</code> <p>Names of features for plot labels</p> <code>None</code> <p>Returns:</p> Name Type Description <code>plot</code> <code>ggplot</code> <p>SMD comparison plot with error bars</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from stochpw import PermutationWeighter\n&gt;&gt;&gt; from stochpw.plotting import plot_balance_diagnostics\n&gt;&gt;&gt; weighter = PermutationWeighter()\n&gt;&gt;&gt; weighter.fit(X, A)\n&gt;&gt;&gt; weights = weighter.predict(X, A)\n&gt;&gt;&gt; plot = plot_balance_diagnostics(X, A, weights)\n&gt;&gt;&gt; print(plot)\n</code></pre> Source code in <code>src/stochpw/plotting.py</code> <pre><code>def plot_balance_diagnostics(\n    X: Array,\n    A: Array,\n    weights: Array,\n    unweighted_weights: Array | None = None,\n    feature_names: list[str] | None = None,\n):\n    \"\"\"\n    Create balance diagnostic plot with standard errors.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n_samples, n_features)\n        Covariates\n    A : jax.Array, shape (n_samples, 1) or (n_samples,)\n        Treatments\n    weights : jax.Array, shape (n_samples,)\n        Sample weights after permutation weighting\n    unweighted_weights : jax.Array, optional\n        Uniform weights for comparison. If None, uses uniform weights.\n    feature_names : list of str, optional\n        Names of features for plot labels\n\n    Returns\n    -------\n    plot : plotnine.ggplot\n        SMD comparison plot with error bars\n\n    Examples\n    --------\n    &gt;&gt;&gt; from stochpw import PermutationWeighter\n    &gt;&gt;&gt; from stochpw.plotting import plot_balance_diagnostics\n    &gt;&gt;&gt; weighter = PermutationWeighter()\n    &gt;&gt;&gt; weighter.fit(X, A)\n    &gt;&gt;&gt; weights = weighter.predict(X, A)\n    &gt;&gt;&gt; plot = plot_balance_diagnostics(X, A, weights)\n    &gt;&gt;&gt; print(plot)\n    \"\"\"\n    if unweighted_weights is None:\n        unweighted_weights = jnp.ones(len(weights))\n\n    if feature_names is None:\n        feature_names = [f\"X{i}\" for i in range(X.shape[1])]\n\n    # Compute SMD and SE for both weighted and unweighted\n    smd_unweighted = standardized_mean_difference(X, A, unweighted_weights)\n    smd_weighted = standardized_mean_difference(X, A, weights)\n    se_unweighted = standardized_mean_difference_se(X, A, unweighted_weights)\n    se_weighted = standardized_mean_difference_se(X, A, weights)\n\n    # Create dataframe with SE\n    n_features = len(feature_names)\n    smd_df = pd.DataFrame(\n        {\n            \"feature\": feature_names * 2,\n            \"smd\": jnp.concatenate([jnp.abs(smd_unweighted), jnp.abs(smd_weighted)]),\n            \"se\": jnp.concatenate([se_unweighted, se_weighted]),\n            \"type\": [\"Unweighted\"] * n_features + [\"Weighted\"] * n_features,\n        }\n    )\n\n    # Add position for dodging\n    smd_df[\"position\"] = smd_df.groupby(\"feature\").cumcount()\n\n    from plotnine import geom_errorbar, position_dodge\n\n    p = (\n        ggplot(smd_df, aes(x=\"feature\", y=\"smd\", fill=\"type\"))\n        + geom_bar(stat=\"identity\", position=position_dodge(width=0.9), alpha=0.7)\n        + geom_errorbar(\n            aes(ymin=\"smd - 1.96 * se\", ymax=\"smd + 1.96 * se\"),\n            position=position_dodge(width=0.9),\n            width=0.25,\n        )\n        + geom_hline(yintercept=0.1, linetype=\"dashed\", color=\"red\", alpha=0.5)\n        + labs(\n            title=\"Balance Improvement (with 95% CI)\",\n            x=\"Feature\",\n            y=\"Absolute Standardized Mean Difference\",\n            fill=\"\",\n        )\n        + theme_minimal()\n        + theme(axis_text_x=element_text(rotation=45, hjust=1))\n    )\n\n    return p\n</code></pre>"},{"location":"reference/stochpw/plotting/#stochpw.plotting.plot_calibration_curve","title":"<code>plot_calibration_curve(bin_centers, true_frequencies, counts)</code>","text":"<p>Plot calibration curve for discriminator predictions.</p> <p>Parameters:</p> Name Type Description Default <code>bin_centers</code> <code>(Array, shape(num_bins))</code> <p>Center of each probability bin</p> required <code>true_frequencies</code> <code>(Array, shape(num_bins))</code> <p>Actual frequency of positive class in each bin</p> required <code>counts</code> <code>(Array, shape(num_bins))</code> <p>Number of samples in each bin</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>ggplot</code> <p>Calibration curve plot</p> Notes <p>A well-calibrated discriminator will have points close to the diagonal line.</p> Source code in <code>src/stochpw/plotting.py</code> <pre><code>def plot_calibration_curve(\n    bin_centers: Array,\n    true_frequencies: Array,\n    counts: Array,\n):\n    \"\"\"\n    Plot calibration curve for discriminator predictions.\n\n    Parameters\n    ----------\n    bin_centers : jax.Array, shape (num_bins,)\n        Center of each probability bin\n    true_frequencies : jax.Array, shape (num_bins,)\n        Actual frequency of positive class in each bin\n    counts : jax.Array, shape (num_bins,)\n        Number of samples in each bin\n\n    Returns\n    -------\n    plot : plotnine.ggplot\n        Calibration curve plot\n\n    Notes\n    -----\n    A well-calibrated discriminator will have points close to the diagonal line.\n    \"\"\"\n    # Only use bins with samples\n    mask = counts &gt; 0\n    cal_df = pd.DataFrame(\n        {\n            \"predicted\": jnp.concatenate([bin_centers[mask], jnp.array([0.0, 1.0])]),\n            \"observed\": jnp.concatenate([true_frequencies[mask], jnp.array([0.0, 1.0])]),\n            \"type\": [\"Calibration\"] * int(jnp.sum(mask)) + [\"Perfect\", \"Perfect\"],\n        }\n    )\n\n    p = (\n        ggplot(cal_df, aes(x=\"predicted\", y=\"observed\", color=\"type\"))\n        + geom_line(size=1)\n        + geom_point(data=pd.DataFrame(cal_df[cal_df[\"type\"] == \"Calibration\"]), size=3)\n        + labs(\n            title=\"Calibration Curve\",\n            x=\"Predicted Probability\",\n            y=\"True Frequency\",\n            color=\"\",\n        )\n        + theme_minimal()\n        + scale_x_continuous(limits=(0, 1))\n        + scale_y_continuous(limits=(0, 1))\n    )\n\n    return p\n</code></pre>"},{"location":"reference/stochpw/plotting/#stochpw.plotting.plot_roc_curve","title":"<code>plot_roc_curve(fpr, tpr, auc=None)</code>","text":"<p>Plot ROC curve for discriminator performance evaluation.</p> <p>This is the most important diagnostic for assessing discriminator quality. The ROC curve shows the trade-off between true positive rate and false positive rate at different classification thresholds.</p> <p>Parameters:</p> Name Type Description Default <code>fpr</code> <code>Array</code> <p>False positive rates</p> required <code>tpr</code> <code>Array</code> <p>True positive rates</p> required <code>auc</code> <code>float</code> <p>Area under the ROC curve. If not provided, will be computed.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>plot</code> <code>ggplot</code> <p>ROC curve plot</p> Notes <p>A good discriminator will have an ROC curve that bows toward the top-left corner (high TPR, low FPR) with AUC close to 1.0. An AUC of 0.5 indicates random guessing.</p> <p>The discriminator's ability to distinguish observed from permuted data directly relates to the quality of the estimated weights.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from stochpw import PermutationWeighter, roc_curve\n&gt;&gt;&gt; from stochpw.plotting import plot_roc_curve\n&gt;&gt;&gt; # After fitting\n&gt;&gt;&gt; weighter.fit(X, A)\n&gt;&gt;&gt; weights = weighter.predict(X, A)\n&gt;&gt;&gt; # Create permuted data\n&gt;&gt;&gt; A_perm = A[jax.random.permutation(key, len(A))]\n&gt;&gt;&gt; weights_perm = weighter.predict(X, A_perm)\n&gt;&gt;&gt; # Compute ROC\n&gt;&gt;&gt; all_weights = jnp.concatenate([weights, weights_perm])\n&gt;&gt;&gt; labels = jnp.concatenate([jnp.zeros(len(weights)), jnp.ones(len(weights_perm))])\n&gt;&gt;&gt; fpr, tpr, _ = roc_curve(all_weights, labels)\n&gt;&gt;&gt; auc = jnp.trapezoid(tpr, fpr)\n&gt;&gt;&gt; plot = plot_roc_curve(fpr, tpr, auc)\n&gt;&gt;&gt; print(plot)\n</code></pre> Source code in <code>src/stochpw/plotting.py</code> <pre><code>def plot_roc_curve(fpr: Array, tpr: Array, auc: float | None = None):\n    \"\"\"\n    Plot ROC curve for discriminator performance evaluation.\n\n    This is the most important diagnostic for assessing discriminator quality.\n    The ROC curve shows the trade-off between true positive rate and false\n    positive rate at different classification thresholds.\n\n    Parameters\n    ----------\n    fpr : jax.Array\n        False positive rates\n    tpr : jax.Array\n        True positive rates\n    auc : float, optional\n        Area under the ROC curve. If not provided, will be computed.\n\n    Returns\n    -------\n    plot : plotnine.ggplot\n        ROC curve plot\n\n    Notes\n    -----\n    A good discriminator will have an ROC curve that bows toward the top-left\n    corner (high TPR, low FPR) with AUC close to 1.0. An AUC of 0.5 indicates\n    random guessing.\n\n    The discriminator's ability to distinguish observed from permuted data\n    directly relates to the quality of the estimated weights.\n\n    Examples\n    --------\n    &gt;&gt;&gt; from stochpw import PermutationWeighter, roc_curve\n    &gt;&gt;&gt; from stochpw.plotting import plot_roc_curve\n    &gt;&gt;&gt; # After fitting\n    &gt;&gt;&gt; weighter.fit(X, A)\n    &gt;&gt;&gt; weights = weighter.predict(X, A)\n    &gt;&gt;&gt; # Create permuted data\n    &gt;&gt;&gt; A_perm = A[jax.random.permutation(key, len(A))]\n    &gt;&gt;&gt; weights_perm = weighter.predict(X, A_perm)\n    &gt;&gt;&gt; # Compute ROC\n    &gt;&gt;&gt; all_weights = jnp.concatenate([weights, weights_perm])\n    &gt;&gt;&gt; labels = jnp.concatenate([jnp.zeros(len(weights)), jnp.ones(len(weights_perm))])\n    &gt;&gt;&gt; fpr, tpr, _ = roc_curve(all_weights, labels)\n    &gt;&gt;&gt; auc = jnp.trapezoid(tpr, fpr)\n    &gt;&gt;&gt; plot = plot_roc_curve(fpr, tpr, auc)\n    &gt;&gt;&gt; print(plot)\n    \"\"\"\n    # Compute AUC if not provided\n    if auc is None:\n        # Use trapezoidal rule for AUC\n        auc = float(jnp.trapezoid(tpr, fpr))\n\n    # Create dataframes for actual ROC and diagonal reference\n    roc_df = pd.DataFrame(\n        {\n            \"fpr\": jnp.concatenate([fpr, jnp.array([0.0, 1.0])]),\n            \"tpr\": jnp.concatenate([tpr, jnp.array([0.0, 1.0])]),\n            \"type\": [\"ROC Curve\"] * len(fpr) + [\"Random Classifier\", \"Random Classifier\"],\n        }\n    )\n\n    title = f\"ROC Curve (AUC = {auc:.3f})\"\n\n    p = (\n        ggplot(roc_df, aes(x=\"fpr\", y=\"tpr\", color=\"type\"))\n        + geom_line(size=1)\n        + geom_point(data=pd.DataFrame(roc_df[roc_df[\"type\"] == \"ROC Curve\"]), size=2, alpha=0.6)\n        + labs(\n            title=title,\n            x=\"False Positive Rate\",\n            y=\"True Positive Rate\",\n            color=\"\",\n        )\n        + theme_minimal()\n        + scale_x_continuous(limits=(0, 1))\n        + scale_y_continuous(limits=(0, 1))\n    )\n\n    return p\n</code></pre>"},{"location":"reference/stochpw/plotting/#stochpw.plotting.plot_training_history","title":"<code>plot_training_history(history)</code>","text":"<p>Plot training loss and other metrics over epochs.</p> <p>Parameters:</p> Name Type Description Default <code>history</code> <code>dict</code> <p>Training history dictionary with 'loss' key and optional other metrics</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>ggplot</code> <p>Training history plot</p> Source code in <code>src/stochpw/plotting.py</code> <pre><code>def plot_training_history(history: dict[str, list[float]]):\n    \"\"\"\n    Plot training loss and other metrics over epochs.\n\n    Parameters\n    ----------\n    history : dict\n        Training history dictionary with 'loss' key and optional other metrics\n\n    Returns\n    -------\n    plot : plotnine.ggplot\n        Training history plot\n    \"\"\"\n    epochs = jnp.arange(1, len(history[\"loss\"]) + 1)\n    hist_df = pd.DataFrame({\"epoch\": epochs, \"loss\": jnp.array(history[\"loss\"])})\n\n    p = (\n        ggplot(hist_df, aes(x=\"epoch\", y=\"loss\"))\n        + geom_line(color=\"blue\", size=1)\n        + labs(title=\"Training History\", x=\"Epoch\", y=\"Loss\")\n        + theme_minimal()\n    )\n\n    return p\n</code></pre>"},{"location":"reference/stochpw/plotting/#stochpw.plotting.plot_weight_distribution","title":"<code>plot_weight_distribution(weights)</code>","text":"<p>Visualize weight distribution.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>plot</code> <code>ggplot</code> <p>Weight distribution histogram with ESS info</p> Source code in <code>src/stochpw/plotting.py</code> <pre><code>def plot_weight_distribution(weights: Array):\n    \"\"\"\n    Visualize weight distribution.\n\n    Parameters\n    ----------\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    plot : plotnine.ggplot\n        Weight distribution histogram with ESS info\n    \"\"\"\n    ess = effective_sample_size(weights)\n    n = len(weights)\n\n    weight_df = pd.DataFrame({\"weight\": jnp.array(weights)})\n    mean_weight = float(jnp.mean(weights))\n\n    p = (\n        ggplot(weight_df, aes(x=\"weight\"))\n        + geom_histogram(bins=50, fill=\"steelblue\", alpha=0.7, color=\"black\")\n        + geom_vline(xintercept=mean_weight, linetype=\"dashed\", color=\"red\")\n        + labs(\n            title=f\"Weight Distribution (ESS: {ess:.0f}/{n}, Ratio: {ess / n:.2%})\",\n            x=\"Weight\",\n            y=\"Frequency\",\n        )\n        + theme_minimal()\n    )\n\n    return p\n</code></pre>"},{"location":"reference/stochpw/types/","title":"types","text":"<p>Type definitions for stochpw.</p> <p>This module provides type aliases for JAX-specific types to improve type safety and reduce reliance on <code>Any</code> types throughout the codebase.</p>"},{"location":"reference/stochpw/utils/","title":"utils","text":"<p>Utility functions for input validation and data handling.</p>"},{"location":"reference/stochpw/utils/#stochpw.utils.permute_treatment","title":"<code>permute_treatment(A, rng_key)</code>","text":"<p>Randomly permute treatment assignments.</p> <p>For multi-dimensional A, this permutes entire rows (treatment vectors) together, not individual elements.</p> <p>Parameters:</p> Name Type Description Default <code>A</code> <code>(Array, shape(batch_size) or (batch_size, n_treatments))</code> <p>Intervention assignments</p> required <code>rng_key</code> <code>PRNGKey</code> <p>Random key for permutation</p> required <p>Returns:</p> Name Type Description <code>A_perm</code> <code>jax.Array, same shape as A</code> <p>Permuted treatment assignments</p> Source code in <code>src/stochpw/utils.py</code> <pre><code>def permute_treatment(A: Array, rng_key: Array) -&gt; Array:\n    \"\"\"\n    Randomly permute treatment assignments.\n\n    For multi-dimensional A, this permutes entire rows (treatment vectors)\n    together, not individual elements.\n\n    Parameters\n    ----------\n    A : jax.Array, shape (batch_size,) or (batch_size, n_treatments)\n        Intervention assignments\n    rng_key : jax.random.PRNGKey\n        Random key for permutation\n\n    Returns\n    -------\n    A_perm : jax.Array, same shape as A\n        Permuted treatment assignments\n    \"\"\"\n    import jax.random as random\n\n    n_samples = A.shape[0]\n    perm_indices = random.permutation(rng_key, n_samples)\n    return A[perm_indices]\n</code></pre>"},{"location":"reference/stochpw/utils/#stochpw.utils.validate_inputs","title":"<code>validate_inputs(X, A)</code>","text":"<p>Validate and convert inputs to JAX arrays.</p> <p>Checks: - X and A have compatible shapes (same number of samples) - No NaNs or Infs - Sufficient variation in A - Converts numpy arrays to JAX arrays if needed</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(array - like, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(array - like, shape(n_samples) or (n_samples, n_treatments))</code> <p>Intervention/treatment assignments</p> required <p>Returns:</p> Name Type Description <code>X_jax</code> <code>Array</code> <p>Validated and converted X</p> <code>A_jax</code> <code>Array</code> <p>Validated and converted A</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If inputs are invalid (incompatible shapes, NaNs, no variation, etc.)</p> Source code in <code>src/stochpw/utils.py</code> <pre><code>def validate_inputs(\n    X: Array | NDArray[Any],  # type: ignore[misc]\n    A: Array | NDArray[Any],  # type: ignore[misc]\n) -&gt; tuple[Array, Array]:\n    \"\"\"\n    Validate and convert inputs to JAX arrays.\n\n    Checks:\n    - X and A have compatible shapes (same number of samples)\n    - No NaNs or Infs\n    - Sufficient variation in A\n    - Converts numpy arrays to JAX arrays if needed\n\n    Parameters\n    ----------\n    X : array-like, shape (n_samples, n_features)\n        Covariates\n    A : array-like, shape (n_samples,) or (n_samples, n_treatments)\n        Intervention/treatment assignments\n\n    Returns\n    -------\n    X_jax : jax.Array\n        Validated and converted X\n    A_jax : jax.Array\n        Validated and converted A\n\n    Raises\n    ------\n    ValueError\n        If inputs are invalid (incompatible shapes, NaNs, no variation, etc.)\n    \"\"\"\n    # Convert to JAX arrays if needed\n    x_jax = jnp.array(X) if isinstance(X, np.ndarray) else X\n    a_jax = jnp.array(A) if isinstance(A, np.ndarray) else A\n\n    # Check shapes\n    if x_jax.ndim != 2:\n        raise ValueError(f\"X must be 2-dimensional, got shape {x_jax.shape}\")\n\n    # Ensure A is at least 1D\n    if a_jax.ndim == 0:\n        raise ValueError(\"A must be at least 1-dimensional, got scalar\")\n    if a_jax.ndim == 1:\n        a_jax = a_jax.reshape(-1, 1)  # Make it (n, 1) for consistency\n    if a_jax.ndim &gt; 2:\n        raise ValueError(f\"A must be 1 or 2-dimensional, got shape {a_jax.shape}\")\n\n    # Check number of samples match\n    if x_jax.shape[0] != a_jax.shape[0]:\n        raise ValueError(\n            \"X and A must have same number of samples: \"\n            + f\"X has {x_jax.shape[0]}, A has {a_jax.shape[0]}\"\n        )\n\n    # Check for NaNs or Infs\n    _check_array_validity(x_jax, \"X\")\n    _check_array_validity(a_jax, \"A\")\n\n    # Check for sufficient variation in A\n    _validate_treatment_variation(a_jax)\n\n    # Check minimum sample size\n    if x_jax.shape[0] &lt; 10:\n        raise ValueError(f\"Need at least 10 samples for training, got {x_jax.shape[0]}\")\n\n    return x_jax, a_jax\n</code></pre>"},{"location":"reference/stochpw/weights/","title":"weights","text":"<p>Weight extraction utilities for permutation weighting.</p>"},{"location":"reference/stochpw/weights/#stochpw.weights.extract_weights","title":"<code>extract_weights(discriminator_fn, params, X, A, AX, eps=1e-07)</code>","text":"<p>Extract importance weights from trained discriminator.</p> <p>Converts discriminator probabilities to density ratio weights using the formula: w(a, x) = (1 - \u03b7(a, x)) / \u03b7(a, x) where \u03b7(a, x) = p(C=1 | a, x) is the probability from the permuted distribution.</p> <p>Parameters:</p> Name Type Description Default <code>discriminator_fn</code> <code>Callable</code> <p>Discriminator function (params, a, x) -&gt; logits</p> required <code>params</code> <code>dict</code> <p>Trained discriminator parameters</p> required <code>X</code> <code>(Array, shape(n, d_x))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n, d_a))</code> <p>Treatments</p> required <code>eps</code> <code>float</code> <p>Numerical stability constant (default: 1e-7)</p> <code>1e-07</code> <p>Returns:</p> Name Type Description <code>weights</code> <code>(Array, shape(n))</code> <p>Importance weights</p> Notes <p>The discriminator outputs logits for p(C=1 | a, x), where C=1 indicates the permuted distribution. The weight formula w = (1 - \u03b7) / \u03b7 gives the density ratio dP/dQ where P is the observed distribution and Q is the balanced (permuted) distribution. This up-weights observed pairs that look like they came from the observed distribution (low \u03b7).</p> Source code in <code>src/stochpw/weights.py</code> <pre><code>def extract_weights(\n    discriminator_fn: Callable[[PyTree, Array, Array, Array], Array],\n    params: PyTree,\n    X: Array,\n    A: Array,\n    AX: Array,\n    eps: float = 1e-7,\n) -&gt; Array:\n    \"\"\"\n    Extract importance weights from trained discriminator.\n\n    Converts discriminator probabilities to density ratio weights using\n    the formula: w(a, x) = (1 - \u03b7(a, x)) / \u03b7(a, x)\n    where \u03b7(a, x) = p(C=1 | a, x) is the probability from the permuted distribution.\n\n    Parameters\n    ----------\n    discriminator_fn : Callable\n        Discriminator function (params, a, x) -&gt; logits\n    params : dict\n        Trained discriminator parameters\n    X : jax.Array, shape (n, d_x)\n        Covariates\n    A : jax.Array, shape (n, d_a)\n        Treatments\n    eps : float, optional\n        Numerical stability constant (default: 1e-7)\n\n    Returns\n    -------\n    weights : jax.Array, shape (n,)\n        Importance weights\n\n    Notes\n    -----\n    The discriminator outputs logits for p(C=1 | a, x), where C=1 indicates\n    the permuted distribution. The weight formula w = (1 - \u03b7) / \u03b7 gives the\n    density ratio dP/dQ where P is the observed distribution and Q is the\n    balanced (permuted) distribution. This up-weights observed pairs that\n    look like they came from the observed distribution (low \u03b7).\n    \"\"\"\n    # Get discriminator probabilities\n    logits = discriminator_fn(params, A, X, AX)\n    eta = jax.nn.sigmoid(logits)\n\n    # Clip to avoid division issues\n    eta_clipped = jnp.clip(eta, eps, 1 - eps)\n\n    # Convert to density ratio: w = \u03b7 / (1 - \u03b7)\n    # Up-weight units that look PERMUTED (high \u03b7) to make observed distribution balanced\n    weights = eta_clipped / (1 - eta_clipped)\n\n    return weights\n</code></pre>"},{"location":"reference/stochpw/diagnostics/","title":"diagnostics","text":"<p>Diagnostic utilities for assessing balance and weight quality.</p>"},{"location":"reference/stochpw/diagnostics/#stochpw.diagnostics.balance_report","title":"<code>balance_report(X, A, weights)</code>","text":"<p>Generate comprehensive balance report.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n_samples, 1) or (n_samples,))</code> <p>Treatments</p> required <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>report</code> <code>dict</code> <p>Comprehensive balance report with: - smd: Array of SMD per covariate - max_smd: Maximum absolute SMD across covariates - mean_smd: Mean absolute SMD across covariates - ess: Effective sample size - ess_ratio: ESS / n_samples - weight_stats: Dictionary of weight distribution statistics - n_samples: Number of samples - n_features: Number of features - treatment_type: 'binary' or 'continuous'</p> Notes <p>This function provides a complete overview of balance quality after weighting, useful for reporting and model diagnostics.</p> Source code in <code>src/stochpw/diagnostics/advanced.py</code> <pre><code>def balance_report(\n    X: Array, A: Array, weights: Array\n) -&gt; dict[str, float | Array | dict[str, float] | str | int]:\n    \"\"\"\n    Generate comprehensive balance report.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n_samples, n_features)\n        Covariates\n    A : jax.Array, shape (n_samples, 1) or (n_samples,)\n        Treatments\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    report : dict\n        Comprehensive balance report with:\n        - smd: Array of SMD per covariate\n        - max_smd: Maximum absolute SMD across covariates\n        - mean_smd: Mean absolute SMD across covariates\n        - ess: Effective sample size\n        - ess_ratio: ESS / n_samples\n        - weight_stats: Dictionary of weight distribution statistics\n        - n_samples: Number of samples\n        - n_features: Number of features\n        - treatment_type: 'binary' or 'continuous'\n\n    Notes\n    -----\n    This function provides a complete overview of balance quality after\n    weighting, useful for reporting and model diagnostics.\n    \"\"\"\n    from .balance import standardized_mean_difference\n    from .weights import effective_sample_size as ess_fn\n\n    # Ensure A is 1D for type detection\n    A_flat = A.squeeze() if A.ndim == 2 else A\n\n    # Detect treatment type\n    unique_a = jnp.unique(A_flat)\n    is_binary = len(unique_a) == 2\n    treatment_type = \"binary\" if is_binary else \"continuous\"\n\n    # Compute SMD\n    smd = standardized_mean_difference(X, A, weights)\n    max_smd = float(jnp.max(jnp.abs(smd)))\n    mean_smd = float(jnp.mean(jnp.abs(smd)))\n\n    # Compute ESS\n    ess = float(ess_fn(weights))\n    n_samples = len(weights)\n    ess_ratio = ess / n_samples\n\n    # Weight statistics\n    w_stats = weight_statistics(weights)\n\n    return {\n        \"smd\": smd,\n        \"max_smd\": max_smd,\n        \"mean_smd\": mean_smd,\n        \"ess\": ess,\n        \"ess_ratio\": ess_ratio,\n        \"weight_stats\": w_stats,\n        \"n_samples\": n_samples,\n        \"n_features\": X.shape[1],\n        \"treatment_type\": treatment_type,\n    }\n</code></pre>"},{"location":"reference/stochpw/diagnostics/#stochpw.diagnostics.calibration_curve","title":"<code>calibration_curve(discriminator_probs, true_labels, num_bins=10)</code>","text":"<p>Compute calibration curve for discriminator predictions.</p> <p>A well-calibrated discriminator should have predicted probabilities that match the true frequencies of the labels.</p> <p>Parameters:</p> Name Type Description Default <code>discriminator_probs</code> <code>(Array, shape(n_samples))</code> <p>Predicted probabilities from discriminator (values between 0 and 1)</p> required <code>true_labels</code> <code>(Array, shape(n_samples))</code> <p>True binary labels (0 or 1)</p> required <code>num_bins</code> <code>int</code> <p>Number of bins to divide probability range into</p> <code>10</code> <p>Returns:</p> Name Type Description <code>bin_centers</code> <code>(Array, shape(num_bins))</code> <p>Center of each probability bin</p> <code>true_frequencies</code> <code>(Array, shape(num_bins))</code> <p>Actual frequency of positive class in each bin</p> <code>counts</code> <code>(Array, shape(num_bins))</code> <p>Number of samples in each bin</p> Notes <p>Perfect calibration means true_frequencies == bin_centers for all bins.</p> Source code in <code>src/stochpw/diagnostics/advanced.py</code> <pre><code>def calibration_curve(\n    discriminator_probs: Array, true_labels: Array, num_bins: int = 10\n) -&gt; tuple[Array, Array, Array]:\n    \"\"\"\n    Compute calibration curve for discriminator predictions.\n\n    A well-calibrated discriminator should have predicted probabilities\n    that match the true frequencies of the labels.\n\n    Parameters\n    ----------\n    discriminator_probs : jax.Array, shape (n_samples,)\n        Predicted probabilities from discriminator (values between 0 and 1)\n    true_labels : jax.Array, shape (n_samples,)\n        True binary labels (0 or 1)\n    num_bins : int, default=10\n        Number of bins to divide probability range into\n\n    Returns\n    -------\n    bin_centers : jax.Array, shape (num_bins,)\n        Center of each probability bin\n    true_frequencies : jax.Array, shape (num_bins,)\n        Actual frequency of positive class in each bin\n    counts : jax.Array, shape (num_bins,)\n        Number of samples in each bin\n\n    Notes\n    -----\n    Perfect calibration means true_frequencies == bin_centers for all bins.\n    \"\"\"\n    # Create bins\n    bin_edges = jnp.linspace(0, 1, num_bins + 1)\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n    # Digitize predictions into bins\n    bin_indices = jnp.digitize(discriminator_probs, bin_edges[1:-1])\n\n    # Compute true frequency and count per bin\n    true_frequencies = jnp.zeros(num_bins)\n    counts = jnp.zeros(num_bins)\n\n    for i in range(num_bins):\n        mask = bin_indices == i\n        count = jnp.sum(mask)\n        counts = counts.at[i].set(count)\n\n        if count &gt; 0:\n            freq = jnp.sum(true_labels[mask]) / count\n            true_frequencies = true_frequencies.at[i].set(freq)\n        else:\n            # For empty bins, use bin center as default\n            true_frequencies = true_frequencies.at[i].set(bin_centers[i])\n\n    return bin_centers, true_frequencies, counts\n</code></pre>"},{"location":"reference/stochpw/diagnostics/#stochpw.diagnostics.effective_sample_size","title":"<code>effective_sample_size(weights)</code>","text":"<p>Compute effective sample size (ESS).</p> <p>ESS = (sum w)^2 / sum(w^2)</p> <p>Lower values indicate more extreme weights (fewer \"effective\" samples). ESS = n means uniform weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>ess</code> <code>Array(scalar)</code> <p>Effective sample size</p> Source code in <code>src/stochpw/diagnostics/weights.py</code> <pre><code>@jax.jit\ndef effective_sample_size(weights: Array) -&gt; Array:\n    \"\"\"\n    Compute effective sample size (ESS).\n\n    ESS = (sum w)^2 / sum(w^2)\n\n    Lower values indicate more extreme weights (fewer \"effective\" samples).\n    ESS = n means uniform weights.\n\n    Parameters\n    ----------\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    ess : jax.Array (scalar)\n        Effective sample size\n    \"\"\"\n    return jnp.sum(weights) ** 2 / jnp.sum(weights**2)\n</code></pre>"},{"location":"reference/stochpw/diagnostics/#stochpw.diagnostics.roc_curve","title":"<code>roc_curve(weights, true_labels, max_points=100)</code>","text":"<p>Compute ROC curve from weights for discriminator performance.</p> <p>Given weights w(x,a), infers eta(x,a) = w(x,a) / (1 + w(x,a)) and computes the ROC curve for discriminating between observed and permuted data.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights from permutation weighting</p> required <code>true_labels</code> <code>(Array, shape(n_samples))</code> <p>True binary labels (0=observed, 1=permuted)</p> required <code>max_points</code> <code>int</code> <p>Maximum number of points in the ROC curve (for computational efficiency)</p> <code>100</code> <p>Returns:</p> Name Type Description <code>fpr</code> <code>Array</code> <p>False positive rates at each threshold</p> <code>tpr</code> <code>Array</code> <p>True positive rates at each threshold</p> <code>thresholds</code> <code>Array</code> <p>Thresholds used to compute fpr and tpr</p> Notes <p>The ROC curve is the most important diagnostic for discriminator quality. A good discriminator will have high AUC (area under curve), indicating it can successfully distinguish between observed and permuted data.</p> <p>The discriminator probability eta is inferred from weights as:     eta(x,a) = w(x,a) / (1 + w(x,a))</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # After fitting a weighter\n&gt;&gt;&gt; weights = weighter.predict(X, A)\n&gt;&gt;&gt; # Create permuted data and labels\n&gt;&gt;&gt; weights_perm = weighter.predict(X, A_permuted)\n&gt;&gt;&gt; all_weights = jnp.concatenate([weights, weights_perm])\n&gt;&gt;&gt; labels = jnp.concatenate([jnp.zeros(len(weights)), jnp.ones(len(weights_perm))])\n&gt;&gt;&gt; fpr, tpr, thresholds = roc_curve(all_weights, labels)\n&gt;&gt;&gt; auc = jnp.trapezoid(tpr, fpr)\n</code></pre> Source code in <code>src/stochpw/diagnostics/advanced.py</code> <pre><code>def roc_curve(\n    weights: Array, true_labels: Array, max_points: int = 100\n) -&gt; tuple[Array, Array, Array]:\n    \"\"\"\n    Compute ROC curve from weights for discriminator performance.\n\n    Given weights w(x,a), infers eta(x,a) = w(x,a) / (1 + w(x,a)) and\n    computes the ROC curve for discriminating between observed and\n    permuted data.\n\n    Parameters\n    ----------\n    weights : jax.Array, shape (n_samples,)\n        Sample weights from permutation weighting\n    true_labels : jax.Array, shape (n_samples,)\n        True binary labels (0=observed, 1=permuted)\n    max_points : int, default=100\n        Maximum number of points in the ROC curve (for computational efficiency)\n\n    Returns\n    -------\n    fpr : jax.Array\n        False positive rates at each threshold\n    tpr : jax.Array\n        True positive rates at each threshold\n    thresholds : jax.Array\n        Thresholds used to compute fpr and tpr\n\n    Notes\n    -----\n    The ROC curve is the most important diagnostic for discriminator quality.\n    A good discriminator will have high AUC (area under curve), indicating\n    it can successfully distinguish between observed and permuted data.\n\n    The discriminator probability eta is inferred from weights as:\n        eta(x,a) = w(x,a) / (1 + w(x,a))\n\n    Examples\n    --------\n    &gt;&gt;&gt; # After fitting a weighter\n    &gt;&gt;&gt; weights = weighter.predict(X, A)\n    &gt;&gt;&gt; # Create permuted data and labels\n    &gt;&gt;&gt; weights_perm = weighter.predict(X, A_permuted)\n    &gt;&gt;&gt; all_weights = jnp.concatenate([weights, weights_perm])\n    &gt;&gt;&gt; labels = jnp.concatenate([jnp.zeros(len(weights)), jnp.ones(len(weights_perm))])\n    &gt;&gt;&gt; fpr, tpr, thresholds = roc_curve(all_weights, labels)\n    &gt;&gt;&gt; auc = jnp.trapezoid(tpr, fpr)\n    \"\"\"\n    # Infer eta from weights: eta = w / (1 + w)\n    eta = weights / (1.0 + weights)\n\n    # Use linearly spaced thresholds for efficiency\n    min_eta = float(jnp.min(eta))\n    max_eta = float(jnp.max(eta))\n    thresholds = jnp.linspace(max_eta + 1e-6, min_eta - 1e-6, max_points)\n\n    # Compute TPR and FPR for each threshold using vectorized operations\n    n_positive = float(jnp.sum(true_labels == 1))\n    n_negative = float(jnp.sum(true_labels == 0))\n\n    # Vectorized computation\n    # For each threshold, count predictions &gt;= threshold\n    predictions_matrix = eta[:, jnp.newaxis] &gt;= thresholds[jnp.newaxis, :]\n\n    # True positives: predictions==1 AND true_labels==1\n    tp = jnp.sum(predictions_matrix &amp; (true_labels[:, jnp.newaxis] == 1), axis=0)\n\n    # False positives: predictions==1 AND true_labels==0\n    fp = jnp.sum(predictions_matrix &amp; (true_labels[:, jnp.newaxis] == 0), axis=0)\n\n    # Compute rates\n    tpr_array = tp / n_positive if n_positive &gt; 0 else jnp.zeros_like(tp)\n    fpr_array = fp / n_negative if n_negative &gt; 0 else jnp.zeros_like(fp)\n\n    return fpr_array, tpr_array, thresholds\n</code></pre>"},{"location":"reference/stochpw/diagnostics/#stochpw.diagnostics.standardized_mean_difference","title":"<code>standardized_mean_difference(X, A, weights)</code>","text":"<p>Compute weighted standardized mean difference for each covariate.</p> <p>For binary treatment, computes SMD between weighted treatment groups. For continuous treatment, computes weighted correlation with covariates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n_samples, 1) or (n_samples,))</code> <p>Treatments</p> required <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>smd</code> <code>(Array, shape(n_features))</code> <p>SMD or correlation for each covariate</p> Source code in <code>src/stochpw/diagnostics/balance.py</code> <pre><code>def standardized_mean_difference(X: Array, A: Array, weights: Array) -&gt; Array:\n    \"\"\"\n    Compute weighted standardized mean difference for each covariate.\n\n    For binary treatment, computes SMD between weighted treatment groups.\n    For continuous treatment, computes weighted correlation with covariates.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n_samples, n_features)\n        Covariates\n    A : jax.Array, shape (n_samples, 1) or (n_samples,)\n        Treatments\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    smd : jax.Array, shape (n_features,)\n        SMD or correlation for each covariate\n    \"\"\"\n    # Ensure A is 1D for this computation\n    a_1d = A.squeeze() if A.ndim == 2 else A\n\n    # Check if A is binary\n    unique_a = jnp.unique(a_1d)\n    is_binary = len(unique_a) == 2\n\n    if is_binary:\n        # Binary treatment: compute SMD\n        a0, a1 = unique_a[0], unique_a[1]\n        mask_0 = a_1d == a0\n        mask_1 = a_1d == a1\n\n        # Weighted means\n        weights_0 = weights * mask_0\n        weights_1 = weights * mask_1\n\n        sum_weights_0 = jnp.sum(weights_0)\n        sum_weights_1 = jnp.sum(weights_1)\n\n        mean_0 = jnp.average(X, axis=0, weights=weights_0)\n        mean_1 = jnp.average(X, axis=0, weights=weights_1)\n\n        # Weighted standard deviations\n        var_0 = jnp.sum(weights_0[:, None] * (X - mean_0) ** 2, axis=0) / (sum_weights_0 + 1e-10)\n        var_1 = jnp.sum(weights_1[:, None] * (X - mean_1) ** 2, axis=0) / (sum_weights_1 + 1e-10)\n\n        # Pooled standard deviation\n        pooled_std = jnp.sqrt((var_0 + var_1) / 2)\n\n        # SMD\n        smd = (mean_1 - mean_0) / (pooled_std + 1e-10)\n\n    else:\n        # Continuous treatment: compute weighted correlation\n        # Normalize weights\n        w_norm = weights / jnp.sum(weights)\n\n        # Weighted means\n        mean_a = jnp.sum(w_norm * a_1d)\n        mean_X = jnp.sum(w_norm[:, None] * X, axis=0)\n\n        # Weighted covariance\n        cov = jnp.sum(w_norm[:, None] * (a_1d[:, None] - mean_a) * (X - mean_X), axis=0)\n\n        # Weighted standard deviations\n        std_a = jnp.sqrt(jnp.sum(w_norm * (a_1d - mean_a) ** 2))\n        std_X = jnp.sqrt(jnp.sum(w_norm[:, None] * (X - mean_X) ** 2, axis=0))\n\n        # Correlation\n        smd = cov / (std_a * std_X + 1e-10)\n\n    return smd\n</code></pre>"},{"location":"reference/stochpw/diagnostics/#stochpw.diagnostics.standardized_mean_difference_se","title":"<code>standardized_mean_difference_se(X, A, weights)</code>","text":"<p>Compute standard errors for standardized mean differences.</p> <p>Uses the bootstrap-style approximation for weighted SMD standard errors.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n_samples, 1) or (n_samples,))</code> <p>Treatments</p> required <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>se</code> <code>(Array, shape(n_features))</code> <p>Standard error for each covariate's SMD</p> Source code in <code>src/stochpw/diagnostics/balance.py</code> <pre><code>def standardized_mean_difference_se(X: Array, A: Array, weights: Array) -&gt; Array:\n    \"\"\"\n    Compute standard errors for standardized mean differences.\n\n    Uses the bootstrap-style approximation for weighted SMD standard errors.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n_samples, n_features)\n        Covariates\n    A : jax.Array, shape (n_samples, 1) or (n_samples,)\n        Treatments\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    se : jax.Array, shape (n_features,)\n        Standard error for each covariate's SMD\n    \"\"\"\n    # Ensure A is 1D\n    a_1d = A.squeeze() if A.ndim == 2 else A\n\n    # Check if A is binary\n    unique_a = jnp.unique(a_1d)\n    is_binary = len(unique_a) == 2\n\n    if is_binary:\n        # Binary treatment: bootstrap-style SE\n        a0, a1 = unique_a[0], unique_a[1]\n        mask_0 = a_1d == a0\n        mask_1 = a_1d == a1\n\n        # Effective sample sizes\n        weights_0 = weights * mask_0\n        weights_1 = weights * mask_1\n        ess_0 = jnp.sum(weights_0) ** 2 / jnp.sum(weights_0**2)\n        ess_1 = jnp.sum(weights_1) ** 2 / jnp.sum(weights_1**2)\n\n        # Approximate SE using ESS\n        # SE(SMD) \u2248 sqrt(1/n_0 + 1/n_1 + SMD\u00b2/(2*(n_0 + n_1)))\n        # Use ESS instead of n\n        smd = standardized_mean_difference(X, A, weights)\n        se = jnp.sqrt(1.0 / ess_0 + 1.0 / ess_1 + smd**2 / (2.0 * (ess_0 + ess_1)))\n\n    else:\n        # Continuous treatment: approximate SE for correlation\n        n_eff = jnp.sum(weights) ** 2 / jnp.sum(weights**2)\n        # SE(correlation) \u2248 1/sqrt(n)\n        se = jnp.ones(X.shape[1]) / jnp.sqrt(n_eff)\n\n    return se\n</code></pre>"},{"location":"reference/stochpw/diagnostics/#stochpw.diagnostics.weight_statistics","title":"<code>weight_statistics(weights)</code>","text":"<p>Compute comprehensive statistics about weight distribution.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>stats</code> <code>dict</code> <p>Dictionary with weight statistics: - mean: Mean weight - std: Standard deviation of weights - min: Minimum weight - max: Maximum weight - cv: Coefficient of variation (std/mean) - entropy: Entropy of normalized weights - max_ratio: Ratio of max to min weight - n_extreme: Number of weights &gt; 10x mean</p> Notes <p>Useful for diagnosing weight quality and potential issues with extreme or highly variable weights.</p> Source code in <code>src/stochpw/diagnostics/advanced.py</code> <pre><code>def weight_statistics(weights: Array) -&gt; dict[str, float]:\n    \"\"\"\n    Compute comprehensive statistics about weight distribution.\n\n    Parameters\n    ----------\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    stats : dict\n        Dictionary with weight statistics:\n        - mean: Mean weight\n        - std: Standard deviation of weights\n        - min: Minimum weight\n        - max: Maximum weight\n        - cv: Coefficient of variation (std/mean)\n        - entropy: Entropy of normalized weights\n        - max_ratio: Ratio of max to min weight\n        - n_extreme: Number of weights &gt; 10x mean\n\n    Notes\n    -----\n    Useful for diagnosing weight quality and potential issues with\n    extreme or highly variable weights.\n    \"\"\"\n    mean_w = float(jnp.mean(weights))\n    std_w = float(jnp.std(weights))\n    min_w = float(jnp.min(weights))\n    max_w = float(jnp.max(weights))\n\n    # Coefficient of variation\n    cv = std_w / mean_w if mean_w &gt; 0 else float(\"inf\")\n\n    # Entropy of normalized weights\n    w_norm = weights / jnp.sum(weights)\n    # Add small constant to avoid log(0)\n    entropy = float(-jnp.sum(w_norm * jnp.log(w_norm + 1e-10)))\n\n    # Maximum weight ratio\n    max_ratio = max_w / min_w if min_w &gt; 0 else float(\"inf\")\n\n    # Number of extreme weights (&gt; 10x mean)\n    n_extreme = int(jnp.sum(weights &gt; 10 * mean_w))\n\n    return {\n        \"mean\": mean_w,\n        \"std\": std_w,\n        \"min\": min_w,\n        \"max\": max_w,\n        \"cv\": cv,\n        \"entropy\": entropy,\n        \"max_ratio\": max_ratio,\n        \"n_extreme\": n_extreme,\n    }\n</code></pre>"},{"location":"reference/stochpw/diagnostics/advanced/","title":"advanced","text":"<p>Advanced balance diagnostics.</p>"},{"location":"reference/stochpw/diagnostics/advanced/#stochpw.diagnostics.advanced.balance_report","title":"<code>balance_report(X, A, weights)</code>","text":"<p>Generate comprehensive balance report.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n_samples, 1) or (n_samples,))</code> <p>Treatments</p> required <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>report</code> <code>dict</code> <p>Comprehensive balance report with: - smd: Array of SMD per covariate - max_smd: Maximum absolute SMD across covariates - mean_smd: Mean absolute SMD across covariates - ess: Effective sample size - ess_ratio: ESS / n_samples - weight_stats: Dictionary of weight distribution statistics - n_samples: Number of samples - n_features: Number of features - treatment_type: 'binary' or 'continuous'</p> Notes <p>This function provides a complete overview of balance quality after weighting, useful for reporting and model diagnostics.</p> Source code in <code>src/stochpw/diagnostics/advanced.py</code> <pre><code>def balance_report(\n    X: Array, A: Array, weights: Array\n) -&gt; dict[str, float | Array | dict[str, float] | str | int]:\n    \"\"\"\n    Generate comprehensive balance report.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n_samples, n_features)\n        Covariates\n    A : jax.Array, shape (n_samples, 1) or (n_samples,)\n        Treatments\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    report : dict\n        Comprehensive balance report with:\n        - smd: Array of SMD per covariate\n        - max_smd: Maximum absolute SMD across covariates\n        - mean_smd: Mean absolute SMD across covariates\n        - ess: Effective sample size\n        - ess_ratio: ESS / n_samples\n        - weight_stats: Dictionary of weight distribution statistics\n        - n_samples: Number of samples\n        - n_features: Number of features\n        - treatment_type: 'binary' or 'continuous'\n\n    Notes\n    -----\n    This function provides a complete overview of balance quality after\n    weighting, useful for reporting and model diagnostics.\n    \"\"\"\n    from .balance import standardized_mean_difference\n    from .weights import effective_sample_size as ess_fn\n\n    # Ensure A is 1D for type detection\n    A_flat = A.squeeze() if A.ndim == 2 else A\n\n    # Detect treatment type\n    unique_a = jnp.unique(A_flat)\n    is_binary = len(unique_a) == 2\n    treatment_type = \"binary\" if is_binary else \"continuous\"\n\n    # Compute SMD\n    smd = standardized_mean_difference(X, A, weights)\n    max_smd = float(jnp.max(jnp.abs(smd)))\n    mean_smd = float(jnp.mean(jnp.abs(smd)))\n\n    # Compute ESS\n    ess = float(ess_fn(weights))\n    n_samples = len(weights)\n    ess_ratio = ess / n_samples\n\n    # Weight statistics\n    w_stats = weight_statistics(weights)\n\n    return {\n        \"smd\": smd,\n        \"max_smd\": max_smd,\n        \"mean_smd\": mean_smd,\n        \"ess\": ess,\n        \"ess_ratio\": ess_ratio,\n        \"weight_stats\": w_stats,\n        \"n_samples\": n_samples,\n        \"n_features\": X.shape[1],\n        \"treatment_type\": treatment_type,\n    }\n</code></pre>"},{"location":"reference/stochpw/diagnostics/advanced/#stochpw.diagnostics.advanced.calibration_curve","title":"<code>calibration_curve(discriminator_probs, true_labels, num_bins=10)</code>","text":"<p>Compute calibration curve for discriminator predictions.</p> <p>A well-calibrated discriminator should have predicted probabilities that match the true frequencies of the labels.</p> <p>Parameters:</p> Name Type Description Default <code>discriminator_probs</code> <code>(Array, shape(n_samples))</code> <p>Predicted probabilities from discriminator (values between 0 and 1)</p> required <code>true_labels</code> <code>(Array, shape(n_samples))</code> <p>True binary labels (0 or 1)</p> required <code>num_bins</code> <code>int</code> <p>Number of bins to divide probability range into</p> <code>10</code> <p>Returns:</p> Name Type Description <code>bin_centers</code> <code>(Array, shape(num_bins))</code> <p>Center of each probability bin</p> <code>true_frequencies</code> <code>(Array, shape(num_bins))</code> <p>Actual frequency of positive class in each bin</p> <code>counts</code> <code>(Array, shape(num_bins))</code> <p>Number of samples in each bin</p> Notes <p>Perfect calibration means true_frequencies == bin_centers for all bins.</p> Source code in <code>src/stochpw/diagnostics/advanced.py</code> <pre><code>def calibration_curve(\n    discriminator_probs: Array, true_labels: Array, num_bins: int = 10\n) -&gt; tuple[Array, Array, Array]:\n    \"\"\"\n    Compute calibration curve for discriminator predictions.\n\n    A well-calibrated discriminator should have predicted probabilities\n    that match the true frequencies of the labels.\n\n    Parameters\n    ----------\n    discriminator_probs : jax.Array, shape (n_samples,)\n        Predicted probabilities from discriminator (values between 0 and 1)\n    true_labels : jax.Array, shape (n_samples,)\n        True binary labels (0 or 1)\n    num_bins : int, default=10\n        Number of bins to divide probability range into\n\n    Returns\n    -------\n    bin_centers : jax.Array, shape (num_bins,)\n        Center of each probability bin\n    true_frequencies : jax.Array, shape (num_bins,)\n        Actual frequency of positive class in each bin\n    counts : jax.Array, shape (num_bins,)\n        Number of samples in each bin\n\n    Notes\n    -----\n    Perfect calibration means true_frequencies == bin_centers for all bins.\n    \"\"\"\n    # Create bins\n    bin_edges = jnp.linspace(0, 1, num_bins + 1)\n    bin_centers = (bin_edges[:-1] + bin_edges[1:]) / 2\n\n    # Digitize predictions into bins\n    bin_indices = jnp.digitize(discriminator_probs, bin_edges[1:-1])\n\n    # Compute true frequency and count per bin\n    true_frequencies = jnp.zeros(num_bins)\n    counts = jnp.zeros(num_bins)\n\n    for i in range(num_bins):\n        mask = bin_indices == i\n        count = jnp.sum(mask)\n        counts = counts.at[i].set(count)\n\n        if count &gt; 0:\n            freq = jnp.sum(true_labels[mask]) / count\n            true_frequencies = true_frequencies.at[i].set(freq)\n        else:\n            # For empty bins, use bin center as default\n            true_frequencies = true_frequencies.at[i].set(bin_centers[i])\n\n    return bin_centers, true_frequencies, counts\n</code></pre>"},{"location":"reference/stochpw/diagnostics/advanced/#stochpw.diagnostics.advanced.roc_curve","title":"<code>roc_curve(weights, true_labels, max_points=100)</code>","text":"<p>Compute ROC curve from weights for discriminator performance.</p> <p>Given weights w(x,a), infers eta(x,a) = w(x,a) / (1 + w(x,a)) and computes the ROC curve for discriminating between observed and permuted data.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights from permutation weighting</p> required <code>true_labels</code> <code>(Array, shape(n_samples))</code> <p>True binary labels (0=observed, 1=permuted)</p> required <code>max_points</code> <code>int</code> <p>Maximum number of points in the ROC curve (for computational efficiency)</p> <code>100</code> <p>Returns:</p> Name Type Description <code>fpr</code> <code>Array</code> <p>False positive rates at each threshold</p> <code>tpr</code> <code>Array</code> <p>True positive rates at each threshold</p> <code>thresholds</code> <code>Array</code> <p>Thresholds used to compute fpr and tpr</p> Notes <p>The ROC curve is the most important diagnostic for discriminator quality. A good discriminator will have high AUC (area under curve), indicating it can successfully distinguish between observed and permuted data.</p> <p>The discriminator probability eta is inferred from weights as:     eta(x,a) = w(x,a) / (1 + w(x,a))</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; # After fitting a weighter\n&gt;&gt;&gt; weights = weighter.predict(X, A)\n&gt;&gt;&gt; # Create permuted data and labels\n&gt;&gt;&gt; weights_perm = weighter.predict(X, A_permuted)\n&gt;&gt;&gt; all_weights = jnp.concatenate([weights, weights_perm])\n&gt;&gt;&gt; labels = jnp.concatenate([jnp.zeros(len(weights)), jnp.ones(len(weights_perm))])\n&gt;&gt;&gt; fpr, tpr, thresholds = roc_curve(all_weights, labels)\n&gt;&gt;&gt; auc = jnp.trapezoid(tpr, fpr)\n</code></pre> Source code in <code>src/stochpw/diagnostics/advanced.py</code> <pre><code>def roc_curve(\n    weights: Array, true_labels: Array, max_points: int = 100\n) -&gt; tuple[Array, Array, Array]:\n    \"\"\"\n    Compute ROC curve from weights for discriminator performance.\n\n    Given weights w(x,a), infers eta(x,a) = w(x,a) / (1 + w(x,a)) and\n    computes the ROC curve for discriminating between observed and\n    permuted data.\n\n    Parameters\n    ----------\n    weights : jax.Array, shape (n_samples,)\n        Sample weights from permutation weighting\n    true_labels : jax.Array, shape (n_samples,)\n        True binary labels (0=observed, 1=permuted)\n    max_points : int, default=100\n        Maximum number of points in the ROC curve (for computational efficiency)\n\n    Returns\n    -------\n    fpr : jax.Array\n        False positive rates at each threshold\n    tpr : jax.Array\n        True positive rates at each threshold\n    thresholds : jax.Array\n        Thresholds used to compute fpr and tpr\n\n    Notes\n    -----\n    The ROC curve is the most important diagnostic for discriminator quality.\n    A good discriminator will have high AUC (area under curve), indicating\n    it can successfully distinguish between observed and permuted data.\n\n    The discriminator probability eta is inferred from weights as:\n        eta(x,a) = w(x,a) / (1 + w(x,a))\n\n    Examples\n    --------\n    &gt;&gt;&gt; # After fitting a weighter\n    &gt;&gt;&gt; weights = weighter.predict(X, A)\n    &gt;&gt;&gt; # Create permuted data and labels\n    &gt;&gt;&gt; weights_perm = weighter.predict(X, A_permuted)\n    &gt;&gt;&gt; all_weights = jnp.concatenate([weights, weights_perm])\n    &gt;&gt;&gt; labels = jnp.concatenate([jnp.zeros(len(weights)), jnp.ones(len(weights_perm))])\n    &gt;&gt;&gt; fpr, tpr, thresholds = roc_curve(all_weights, labels)\n    &gt;&gt;&gt; auc = jnp.trapezoid(tpr, fpr)\n    \"\"\"\n    # Infer eta from weights: eta = w / (1 + w)\n    eta = weights / (1.0 + weights)\n\n    # Use linearly spaced thresholds for efficiency\n    min_eta = float(jnp.min(eta))\n    max_eta = float(jnp.max(eta))\n    thresholds = jnp.linspace(max_eta + 1e-6, min_eta - 1e-6, max_points)\n\n    # Compute TPR and FPR for each threshold using vectorized operations\n    n_positive = float(jnp.sum(true_labels == 1))\n    n_negative = float(jnp.sum(true_labels == 0))\n\n    # Vectorized computation\n    # For each threshold, count predictions &gt;= threshold\n    predictions_matrix = eta[:, jnp.newaxis] &gt;= thresholds[jnp.newaxis, :]\n\n    # True positives: predictions==1 AND true_labels==1\n    tp = jnp.sum(predictions_matrix &amp; (true_labels[:, jnp.newaxis] == 1), axis=0)\n\n    # False positives: predictions==1 AND true_labels==0\n    fp = jnp.sum(predictions_matrix &amp; (true_labels[:, jnp.newaxis] == 0), axis=0)\n\n    # Compute rates\n    tpr_array = tp / n_positive if n_positive &gt; 0 else jnp.zeros_like(tp)\n    fpr_array = fp / n_negative if n_negative &gt; 0 else jnp.zeros_like(fp)\n\n    return fpr_array, tpr_array, thresholds\n</code></pre>"},{"location":"reference/stochpw/diagnostics/advanced/#stochpw.diagnostics.advanced.weight_statistics","title":"<code>weight_statistics(weights)</code>","text":"<p>Compute comprehensive statistics about weight distribution.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>stats</code> <code>dict</code> <p>Dictionary with weight statistics: - mean: Mean weight - std: Standard deviation of weights - min: Minimum weight - max: Maximum weight - cv: Coefficient of variation (std/mean) - entropy: Entropy of normalized weights - max_ratio: Ratio of max to min weight - n_extreme: Number of weights &gt; 10x mean</p> Notes <p>Useful for diagnosing weight quality and potential issues with extreme or highly variable weights.</p> Source code in <code>src/stochpw/diagnostics/advanced.py</code> <pre><code>def weight_statistics(weights: Array) -&gt; dict[str, float]:\n    \"\"\"\n    Compute comprehensive statistics about weight distribution.\n\n    Parameters\n    ----------\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    stats : dict\n        Dictionary with weight statistics:\n        - mean: Mean weight\n        - std: Standard deviation of weights\n        - min: Minimum weight\n        - max: Maximum weight\n        - cv: Coefficient of variation (std/mean)\n        - entropy: Entropy of normalized weights\n        - max_ratio: Ratio of max to min weight\n        - n_extreme: Number of weights &gt; 10x mean\n\n    Notes\n    -----\n    Useful for diagnosing weight quality and potential issues with\n    extreme or highly variable weights.\n    \"\"\"\n    mean_w = float(jnp.mean(weights))\n    std_w = float(jnp.std(weights))\n    min_w = float(jnp.min(weights))\n    max_w = float(jnp.max(weights))\n\n    # Coefficient of variation\n    cv = std_w / mean_w if mean_w &gt; 0 else float(\"inf\")\n\n    # Entropy of normalized weights\n    w_norm = weights / jnp.sum(weights)\n    # Add small constant to avoid log(0)\n    entropy = float(-jnp.sum(w_norm * jnp.log(w_norm + 1e-10)))\n\n    # Maximum weight ratio\n    max_ratio = max_w / min_w if min_w &gt; 0 else float(\"inf\")\n\n    # Number of extreme weights (&gt; 10x mean)\n    n_extreme = int(jnp.sum(weights &gt; 10 * mean_w))\n\n    return {\n        \"mean\": mean_w,\n        \"std\": std_w,\n        \"min\": min_w,\n        \"max\": max_w,\n        \"cv\": cv,\n        \"entropy\": entropy,\n        \"max_ratio\": max_ratio,\n        \"n_extreme\": n_extreme,\n    }\n</code></pre>"},{"location":"reference/stochpw/diagnostics/balance/","title":"balance","text":"<p>Balance diagnostics for treatment-covariate distributions.</p>"},{"location":"reference/stochpw/diagnostics/balance/#stochpw.diagnostics.balance.standardized_mean_difference","title":"<code>standardized_mean_difference(X, A, weights)</code>","text":"<p>Compute weighted standardized mean difference for each covariate.</p> <p>For binary treatment, computes SMD between weighted treatment groups. For continuous treatment, computes weighted correlation with covariates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n_samples, 1) or (n_samples,))</code> <p>Treatments</p> required <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>smd</code> <code>(Array, shape(n_features))</code> <p>SMD or correlation for each covariate</p> Source code in <code>src/stochpw/diagnostics/balance.py</code> <pre><code>def standardized_mean_difference(X: Array, A: Array, weights: Array) -&gt; Array:\n    \"\"\"\n    Compute weighted standardized mean difference for each covariate.\n\n    For binary treatment, computes SMD between weighted treatment groups.\n    For continuous treatment, computes weighted correlation with covariates.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n_samples, n_features)\n        Covariates\n    A : jax.Array, shape (n_samples, 1) or (n_samples,)\n        Treatments\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    smd : jax.Array, shape (n_features,)\n        SMD or correlation for each covariate\n    \"\"\"\n    # Ensure A is 1D for this computation\n    a_1d = A.squeeze() if A.ndim == 2 else A\n\n    # Check if A is binary\n    unique_a = jnp.unique(a_1d)\n    is_binary = len(unique_a) == 2\n\n    if is_binary:\n        # Binary treatment: compute SMD\n        a0, a1 = unique_a[0], unique_a[1]\n        mask_0 = a_1d == a0\n        mask_1 = a_1d == a1\n\n        # Weighted means\n        weights_0 = weights * mask_0\n        weights_1 = weights * mask_1\n\n        sum_weights_0 = jnp.sum(weights_0)\n        sum_weights_1 = jnp.sum(weights_1)\n\n        mean_0 = jnp.average(X, axis=0, weights=weights_0)\n        mean_1 = jnp.average(X, axis=0, weights=weights_1)\n\n        # Weighted standard deviations\n        var_0 = jnp.sum(weights_0[:, None] * (X - mean_0) ** 2, axis=0) / (sum_weights_0 + 1e-10)\n        var_1 = jnp.sum(weights_1[:, None] * (X - mean_1) ** 2, axis=0) / (sum_weights_1 + 1e-10)\n\n        # Pooled standard deviation\n        pooled_std = jnp.sqrt((var_0 + var_1) / 2)\n\n        # SMD\n        smd = (mean_1 - mean_0) / (pooled_std + 1e-10)\n\n    else:\n        # Continuous treatment: compute weighted correlation\n        # Normalize weights\n        w_norm = weights / jnp.sum(weights)\n\n        # Weighted means\n        mean_a = jnp.sum(w_norm * a_1d)\n        mean_X = jnp.sum(w_norm[:, None] * X, axis=0)\n\n        # Weighted covariance\n        cov = jnp.sum(w_norm[:, None] * (a_1d[:, None] - mean_a) * (X - mean_X), axis=0)\n\n        # Weighted standard deviations\n        std_a = jnp.sqrt(jnp.sum(w_norm * (a_1d - mean_a) ** 2))\n        std_X = jnp.sqrt(jnp.sum(w_norm[:, None] * (X - mean_X) ** 2, axis=0))\n\n        # Correlation\n        smd = cov / (std_a * std_X + 1e-10)\n\n    return smd\n</code></pre>"},{"location":"reference/stochpw/diagnostics/balance/#stochpw.diagnostics.balance.standardized_mean_difference_se","title":"<code>standardized_mean_difference_se(X, A, weights)</code>","text":"<p>Compute standard errors for standardized mean differences.</p> <p>Uses the bootstrap-style approximation for weighted SMD standard errors.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n_samples, n_features))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n_samples, 1) or (n_samples,))</code> <p>Treatments</p> required <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>se</code> <code>(Array, shape(n_features))</code> <p>Standard error for each covariate's SMD</p> Source code in <code>src/stochpw/diagnostics/balance.py</code> <pre><code>def standardized_mean_difference_se(X: Array, A: Array, weights: Array) -&gt; Array:\n    \"\"\"\n    Compute standard errors for standardized mean differences.\n\n    Uses the bootstrap-style approximation for weighted SMD standard errors.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n_samples, n_features)\n        Covariates\n    A : jax.Array, shape (n_samples, 1) or (n_samples,)\n        Treatments\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    se : jax.Array, shape (n_features,)\n        Standard error for each covariate's SMD\n    \"\"\"\n    # Ensure A is 1D\n    a_1d = A.squeeze() if A.ndim == 2 else A\n\n    # Check if A is binary\n    unique_a = jnp.unique(a_1d)\n    is_binary = len(unique_a) == 2\n\n    if is_binary:\n        # Binary treatment: bootstrap-style SE\n        a0, a1 = unique_a[0], unique_a[1]\n        mask_0 = a_1d == a0\n        mask_1 = a_1d == a1\n\n        # Effective sample sizes\n        weights_0 = weights * mask_0\n        weights_1 = weights * mask_1\n        ess_0 = jnp.sum(weights_0) ** 2 / jnp.sum(weights_0**2)\n        ess_1 = jnp.sum(weights_1) ** 2 / jnp.sum(weights_1**2)\n\n        # Approximate SE using ESS\n        # SE(SMD) \u2248 sqrt(1/n_0 + 1/n_1 + SMD\u00b2/(2*(n_0 + n_1)))\n        # Use ESS instead of n\n        smd = standardized_mean_difference(X, A, weights)\n        se = jnp.sqrt(1.0 / ess_0 + 1.0 / ess_1 + smd**2 / (2.0 * (ess_0 + ess_1)))\n\n    else:\n        # Continuous treatment: approximate SE for correlation\n        n_eff = jnp.sum(weights) ** 2 / jnp.sum(weights**2)\n        # SE(correlation) \u2248 1/sqrt(n)\n        se = jnp.ones(X.shape[1]) / jnp.sqrt(n_eff)\n\n    return se\n</code></pre>"},{"location":"reference/stochpw/diagnostics/weights/","title":"weights","text":"<p>Weight quality diagnostics.</p>"},{"location":"reference/stochpw/diagnostics/weights/#stochpw.diagnostics.weights.effective_sample_size","title":"<code>effective_sample_size(weights)</code>","text":"<p>Compute effective sample size (ESS).</p> <p>ESS = (sum w)^2 / sum(w^2)</p> <p>Lower values indicate more extreme weights (fewer \"effective\" samples). ESS = n means uniform weights.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n_samples))</code> <p>Sample weights</p> required <p>Returns:</p> Name Type Description <code>ess</code> <code>Array(scalar)</code> <p>Effective sample size</p> Source code in <code>src/stochpw/diagnostics/weights.py</code> <pre><code>@jax.jit\ndef effective_sample_size(weights: Array) -&gt; Array:\n    \"\"\"\n    Compute effective sample size (ESS).\n\n    ESS = (sum w)^2 / sum(w^2)\n\n    Lower values indicate more extreme weights (fewer \"effective\" samples).\n    ESS = n means uniform weights.\n\n    Parameters\n    ----------\n    weights : jax.Array, shape (n_samples,)\n        Sample weights\n\n    Returns\n    -------\n    ess : jax.Array (scalar)\n        Effective sample size\n    \"\"\"\n    return jnp.sum(weights) ** 2 / jnp.sum(weights**2)\n</code></pre>"},{"location":"reference/stochpw/models/","title":"models","text":"<p>Discriminator models for permutation weighting.</p>"},{"location":"reference/stochpw/models/#stochpw.models.BaseDiscriminator","title":"<code>BaseDiscriminator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for discriminator models in permutation weighting.</p> <p>All discriminator models must implement the <code>init_params</code> and <code>apply</code> methods to define how parameters are initialized and how logits are computed from treatments (A), covariates (X), and their interactions (AX).</p> <p>The discriminator's role is to distinguish between: - Observed pairs (X, A) with label C=0 - Permuted pairs (X, A') with label C=1</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyDiscriminator(BaseDiscriminator):\n...     def init_params(self, rng_key, d_a, d_x):\n...         # Initialize parameters\n...         return {\"w\": jax.random.normal(rng_key, (d_a + d_x,))}\n...\n...     def apply(self, params, a, x, ax):\n...         # Compute logits\n...         return jnp.dot(jnp.concatenate([a, x], axis=-1), params[\"w\"])\n</code></pre>"},{"location":"reference/stochpw/models/#stochpw.models.BaseDiscriminator.__call__","title":"<code>__call__(params, a, x, ax)</code>","text":"<p>Convenience method to call apply.</p> <p>Allows discriminator to be called as: discriminator(params, a, x, ax).</p> Source code in <code>src/stochpw/models/base.py</code> <pre><code>def __call__(self, params: PyTree, a: Array, x: Array, ax: Array) -&gt; Array:\n    \"\"\"\n    Convenience method to call apply.\n\n    Allows discriminator to be called as: discriminator(params, a, x, ax).\n    \"\"\"\n    return self.apply(params, a, x, ax)\n</code></pre>"},{"location":"reference/stochpw/models/#stochpw.models.BaseDiscriminator.apply","title":"<code>apply(params, a, x, ax)</code>  <code>abstractmethod</code>","text":"<p>Compute discriminator logits.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>PyTree</code> <p>Model parameters (output of init_params)</p> required <code>a</code> <code>(Array, shape(batch_size, d_a) or (batch_size,))</code> <p>Treatment assignments</p> required <code>x</code> <code>(Array, shape(batch_size, d_x))</code> <p>Covariates</p> required <code>ax</code> <code>(Array, shape(batch_size, d_a * d_x))</code> <p>Pre-computed first-order interactions A \u2297 X</p> required <p>Returns:</p> Name Type Description <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Discriminator logits for p(C=1 | a, x)</p> Notes <p>The logits represent the raw output before sigmoid activation. They are used in the logistic loss: BCE(sigmoid(logits), labels).</p> Source code in <code>src/stochpw/models/base.py</code> <pre><code>@abstractmethod\ndef apply(self, params: PyTree, a: Array, x: Array, ax: Array) -&gt; Array:\n    \"\"\"\n    Compute discriminator logits.\n\n    Parameters\n    ----------\n    params : PyTree\n        Model parameters (output of init_params)\n    a : jax.Array, shape (batch_size, d_a) or (batch_size,)\n        Treatment assignments\n    x : jax.Array, shape (batch_size, d_x)\n        Covariates\n    ax : jax.Array, shape (batch_size, d_a * d_x)\n        Pre-computed first-order interactions A \u2297 X\n\n    Returns\n    -------\n    logits : jax.Array, shape (batch_size,)\n        Discriminator logits for p(C=1 | a, x)\n\n    Notes\n    -----\n    The logits represent the raw output before sigmoid activation.\n    They are used in the logistic loss: BCE(sigmoid(logits), labels).\n    \"\"\"\n    raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"reference/stochpw/models/#stochpw.models.BaseDiscriminator.init_params","title":"<code>init_params(rng_key, d_a, d_x)</code>  <code>abstractmethod</code>","text":"<p>Initialize discriminator parameters.</p> <p>Parameters:</p> Name Type Description Default <code>rng_key</code> <code>Array</code> <p>Random key for parameter initialization</p> required <code>d_a</code> <code>int</code> <p>Dimension of treatment vector</p> required <code>d_x</code> <code>int</code> <p>Dimension of covariate vector</p> required <p>Returns:</p> Name Type Description <code>params</code> <code>PyTree</code> <p>Initialized parameters (any JAX-compatible PyTree structure)</p> Source code in <code>src/stochpw/models/base.py</code> <pre><code>@abstractmethod\ndef init_params(self, rng_key: Array, d_a: int, d_x: int) -&gt; PyTree:\n    \"\"\"\n    Initialize discriminator parameters.\n\n    Parameters\n    ----------\n    rng_key : jax.Array\n        Random key for parameter initialization\n    d_a : int\n        Dimension of treatment vector\n    d_x : int\n        Dimension of covariate vector\n\n    Returns\n    -------\n    params : PyTree\n        Initialized parameters (any JAX-compatible PyTree structure)\n    \"\"\"\n    raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"reference/stochpw/models/#stochpw.models.LinearDiscriminator","title":"<code>LinearDiscriminator</code>","text":"<p>               Bases: <code>BaseDiscriminator</code></p> <p>Linear discriminator using A, X, and A*X interactions.</p> <p>The discriminator computes logits as:     logit = w_a^T A + w_x^T X + w_ax^T (A \u2297 X) + b</p> <p>This allows the model to learn from: - Marginal treatment effects (w_a) - Marginal covariate effects (w_x) - Treatment-covariate interactions (w_ax)</p> <p>The explicit inclusion of A*X interactions is critical for linear models because within-batch permutation ensures P(A) and P(X) are identical in observed vs permuted batches. Only the joint distribution P(A,X) differs, which linear models need explicit interaction terms to capture.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from stochpw.models import LinearDiscriminator\n&gt;&gt;&gt; import jax\n&gt;&gt;&gt;\n&gt;&gt;&gt; discriminator = LinearDiscriminator()\n&gt;&gt;&gt; params = discriminator.init_params(jax.random.PRNGKey(0), d_a=1, d_x=3)\n&gt;&gt;&gt; # params contains: w_a, w_x, w_ax, b\n</code></pre>"},{"location":"reference/stochpw/models/#stochpw.models.LinearDiscriminator.apply","title":"<code>apply(params, a, x, ax)</code>","text":"<p>Compute linear discriminator logits using A, X, and A*X.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters with keys 'w_a', 'w_x', 'w_ax', and 'b'</p> required <code>a</code> <code>(Array, shape(batch_size, d_a) or (batch_size,))</code> <p>Treatment assignments</p> required <code>x</code> <code>(Array, shape(batch_size, d_x))</code> <p>Covariates</p> required <code>ax</code> <code>(Array, shape(batch_size, d_a * d_x))</code> <p>Pre-computed first-order interactions A \u2297 X</p> required <p>Returns:</p> Name Type Description <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Discriminator logits for p(C=1 | a, x)</p> Source code in <code>src/stochpw/models/linear.py</code> <pre><code>@override\ndef apply(self, params: PyTree, a: Array, x: Array, ax: Array) -&gt; Array:  # type: ignore[override]\n    \"\"\"\n    Compute linear discriminator logits using A, X, and A*X.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters with keys 'w_a', 'w_x', 'w_ax', and 'b'\n    a : jax.Array, shape (batch_size, d_a) or (batch_size,)\n        Treatment assignments\n    x : jax.Array, shape (batch_size, d_x)\n        Covariates\n    ax : jax.Array, shape (batch_size, d_a * d_x)\n        Pre-computed first-order interactions A \u2297 X\n\n    Returns\n    -------\n    logits : jax.Array, shape (batch_size,)\n        Discriminator logits for p(C=1 | a, x)\n    \"\"\"\n    # Ensure a is 2D\n    if a.ndim == 1:\n        a = a.reshape(-1, 1)\n\n    # Cast params to expected dict type for type checker\n    params_dict = cast(LinearParams, params)\n\n    # Linear transformation: w_a^T A + w_x^T X + w_ax^T (A*X) + b\n    logits = (\n        jnp.dot(a, params_dict[\"w_a\"])\n        + jnp.dot(x, params_dict[\"w_x\"])\n        + jnp.dot(ax, params_dict[\"w_ax\"])\n        + params_dict[\"b\"]\n    )\n\n    return logits\n</code></pre>"},{"location":"reference/stochpw/models/#stochpw.models.LinearDiscriminator.init_params","title":"<code>init_params(rng_key, d_a, d_x)</code>","text":"<p>Initialize linear discriminator parameters.</p> <p>Uses Xavier/Glorot initialization for weights and small random initialization for bias.</p> <p>Parameters:</p> Name Type Description Default <code>rng_key</code> <code>Array</code> <p>Random key for parameter initialization</p> required <code>d_a</code> <code>int</code> <p>Dimension of treatment vector</p> required <code>d_x</code> <code>int</code> <p>Dimension of covariate vector</p> required <p>Returns:</p> Name Type Description <code>params</code> <code>dict</code> <p>Dictionary with keys: - 'w_a': Array of shape (d_a,) - treatment weights - 'w_x': Array of shape (d_x,) - covariate weights - 'w_ax': Array of shape (d_a * d_x,) - interaction weights - 'b': scalar - bias term</p> Source code in <code>src/stochpw/models/linear.py</code> <pre><code>@override\ndef init_params(self, rng_key: Array, d_a: int, d_x: int) -&gt; LinearParams:\n    \"\"\"\n    Initialize linear discriminator parameters.\n\n    Uses Xavier/Glorot initialization for weights and small random\n    initialization for bias.\n\n    Parameters\n    ----------\n    rng_key : jax.Array\n        Random key for parameter initialization\n    d_a : int\n        Dimension of treatment vector\n    d_x : int\n        Dimension of covariate vector\n\n    Returns\n    -------\n    params : dict\n        Dictionary with keys:\n        - 'w_a': Array of shape (d_a,) - treatment weights\n        - 'w_x': Array of shape (d_x,) - covariate weights\n        - 'w_ax': Array of shape (d_a * d_x,) - interaction weights\n        - 'b': scalar - bias term\n    \"\"\"\n    interaction_dim = d_a * d_x\n    total_dim = d_a + d_x + interaction_dim\n\n    w_key, b_key = jax.random.split(rng_key)\n\n    # Xavier/Glorot initialization\n    std = jnp.sqrt(2.0 / total_dim)\n    w_a = jax.random.normal(jax.random.fold_in(w_key, 0), (d_a,)) * std\n    w_x = jax.random.normal(jax.random.fold_in(w_key, 1), (d_x,)) * std\n    w_ax = jax.random.normal(jax.random.fold_in(w_key, 2), (interaction_dim,)) * std\n    b = jax.random.normal(b_key, ()) * 0.01\n\n    return {\"w_a\": w_a, \"w_x\": w_x, \"w_ax\": w_ax, \"b\": b}\n</code></pre>"},{"location":"reference/stochpw/models/#stochpw.models.MLPDiscriminator","title":"<code>MLPDiscriminator(hidden_dims=None, activation='relu')</code>","text":"<p>               Bases: <code>BaseDiscriminator</code></p> <p>Multi-layer perceptron (MLP) discriminator using A, X, and A*X interactions.</p> <p>The MLP processes concatenated features [A, X, A*X] through configurable hidden layers with specified activation functions, outputting a scalar logit.</p> <p>This provides more expressive power than linear discriminators for capturing complex relationships between treatments and covariates.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_dims</code> <code>list[int]</code> <p>List of hidden layer sizes. Default is [64, 32]</p> <code>None</code> <code>activation</code> <code>(relu, tanh, elu, sigmoid)</code> <p>Activation function to use between layers. Default is 'relu'</p> <code>'relu'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from stochpw.models import MLPDiscriminator\n&gt;&gt;&gt; import jax\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Default: 2-layer MLP with ReLU\n&gt;&gt;&gt; discriminator = MLPDiscriminator()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Custom: 3-layer MLP with tanh\n&gt;&gt;&gt; discriminator = MLPDiscriminator(hidden_dims=[128, 64, 32], activation=\"tanh\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; params = discriminator.init_params(jax.random.PRNGKey(0), d_a=1, d_x=3)\n</code></pre> Source code in <code>src/stochpw/models/mlp.py</code> <pre><code>def __init__(\n    self,\n    hidden_dims: list[int] | None = None,\n    activation: ActivationType = \"relu\",\n):\n    if hidden_dims is None:\n        hidden_dims = [64, 32]\n    self.hidden_dims: list[int] = hidden_dims\n    self.activation: ActivationType = activation\n    self._activation_fn: Callable[[Array], Array] = _get_activation(activation)\n    self._use_he_init: bool = activation in (\"relu\", \"elu\")\n</code></pre>"},{"location":"reference/stochpw/models/#stochpw.models.MLPDiscriminator.apply","title":"<code>apply(params, a, x, ax)</code>","text":"<p>Compute MLP discriminator logits using A, X, and A*X.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters with key 'layers' containing list of layer dicts</p> required <code>a</code> <code>(Array, shape(batch_size, d_a) or (batch_size,))</code> <p>Treatment assignments</p> required <code>x</code> <code>(Array, shape(batch_size, d_x))</code> <p>Covariates</p> required <code>ax</code> <code>(Array, shape(batch_size, d_a * d_x))</code> <p>Pre-computed first-order interactions A \u2297 X</p> required <p>Returns:</p> Name Type Description <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Discriminator logits for p(C=1 | a, x)</p> Source code in <code>src/stochpw/models/mlp.py</code> <pre><code>@override\ndef apply(self, params: PyTree, a: Array, x: Array, ax: Array) -&gt; Array:  # type: ignore[override]\n    \"\"\"\n    Compute MLP discriminator logits using A, X, and A*X.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters with key 'layers' containing list of layer dicts\n    a : jax.Array, shape (batch_size, d_a) or (batch_size,)\n        Treatment assignments\n    x : jax.Array, shape (batch_size, d_x)\n        Covariates\n    ax : jax.Array, shape (batch_size, d_a * d_x)\n        Pre-computed first-order interactions A \u2297 X\n\n    Returns\n    -------\n    logits : jax.Array, shape (batch_size,)\n        Discriminator logits for p(C=1 | a, x)\n    \"\"\"\n    # Ensure a is 2D\n    if a.ndim == 1:\n        a = a.reshape(-1, 1)\n\n    # Concatenate all features: [A, X, A*X]\n    h = jnp.concatenate([a, x, ax], axis=-1)\n\n    # Cast params to expected dict type for type checker\n    params_dict = cast(MLPParams, params)\n\n    # Forward pass through hidden layers\n    for i, layer in enumerate(params_dict[\"layers\"]):\n        h = jnp.dot(h, layer[\"w\"]) + layer[\"b\"]\n\n        # Apply activation to all layers except the last (output layer)\n        if i &lt; len(params_dict[\"layers\"]) - 1:\n            h = self._activation_fn(h)\n\n    # Output is shape (batch_size, 1), squeeze to (batch_size,)\n    logits = h.squeeze(-1)\n\n    return logits\n</code></pre>"},{"location":"reference/stochpw/models/#stochpw.models.MLPDiscriminator.init_params","title":"<code>init_params(rng_key, d_a, d_x)</code>","text":"<p>Initialize MLP discriminator parameters.</p> <p>Uses He initialization for ReLU-family activations and Xavier initialization for tanh/sigmoid activations. Biases are initialized to zero.</p> <p>Parameters:</p> Name Type Description Default <code>rng_key</code> <code>Array</code> <p>Random key for parameter initialization</p> required <code>d_a</code> <code>int</code> <p>Dimension of treatment vector</p> required <code>d_x</code> <code>int</code> <p>Dimension of covariate vector</p> required <p>Returns:</p> Name Type Description <code>params</code> <code>dict</code> <p>Dictionary with key 'layers' containing a list of layer dicts, each with keys 'w' (weight matrix) and 'b' (bias vector)</p> Source code in <code>src/stochpw/models/mlp.py</code> <pre><code>@override\ndef init_params(self, rng_key: Array, d_a: int, d_x: int) -&gt; MLPParams:\n    \"\"\"\n    Initialize MLP discriminator parameters.\n\n    Uses He initialization for ReLU-family activations and Xavier\n    initialization for tanh/sigmoid activations. Biases are initialized to zero.\n\n    Parameters\n    ----------\n    rng_key : jax.Array\n        Random key for parameter initialization\n    d_a : int\n        Dimension of treatment vector\n    d_x : int\n        Dimension of covariate vector\n\n    Returns\n    -------\n    params : dict\n        Dictionary with key 'layers' containing a list of layer dicts,\n        each with keys 'w' (weight matrix) and 'b' (bias vector)\n    \"\"\"\n    interaction_dim = d_a * d_x\n    input_dim = d_a + d_x + interaction_dim\n\n    params = {\"layers\": []}\n    layer_dims = [input_dim] + self.hidden_dims + [1]  # Output is scalar logit\n    current_key = rng_key\n\n    for i in range(len(layer_dims) - 1):\n        current_key, layer_key = jax.random.split(current_key)\n        w_key, _b_key = jax.random.split(layer_key)\n\n        in_dim = layer_dims[i]\n        out_dim = layer_dims[i + 1]\n\n        # He initialization for ReLU, Xavier for others\n        if self._use_he_init:\n            std = jnp.sqrt(2.0 / in_dim)\n        else:\n            std = jnp.sqrt(2.0 / (in_dim + out_dim))\n\n        w = jax.random.normal(w_key, (in_dim, out_dim)) * std\n        b = jnp.zeros((out_dim,))  # Initialize biases to zero\n\n        params[\"layers\"].append({\"w\": w, \"b\": b})\n\n    return params\n</code></pre>"},{"location":"reference/stochpw/models/base/","title":"base","text":"<p>Abstract base class for discriminator models.</p>"},{"location":"reference/stochpw/models/base/#stochpw.models.base.BaseDiscriminator","title":"<code>BaseDiscriminator</code>","text":"<p>               Bases: <code>ABC</code></p> <p>Abstract base class for discriminator models in permutation weighting.</p> <p>All discriminator models must implement the <code>init_params</code> and <code>apply</code> methods to define how parameters are initialized and how logits are computed from treatments (A), covariates (X), and their interactions (AX).</p> <p>The discriminator's role is to distinguish between: - Observed pairs (X, A) with label C=0 - Permuted pairs (X, A') with label C=1</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; class MyDiscriminator(BaseDiscriminator):\n...     def init_params(self, rng_key, d_a, d_x):\n...         # Initialize parameters\n...         return {\"w\": jax.random.normal(rng_key, (d_a + d_x,))}\n...\n...     def apply(self, params, a, x, ax):\n...         # Compute logits\n...         return jnp.dot(jnp.concatenate([a, x], axis=-1), params[\"w\"])\n</code></pre>"},{"location":"reference/stochpw/models/base/#stochpw.models.base.BaseDiscriminator.__call__","title":"<code>__call__(params, a, x, ax)</code>","text":"<p>Convenience method to call apply.</p> <p>Allows discriminator to be called as: discriminator(params, a, x, ax).</p> Source code in <code>src/stochpw/models/base.py</code> <pre><code>def __call__(self, params: PyTree, a: Array, x: Array, ax: Array) -&gt; Array:\n    \"\"\"\n    Convenience method to call apply.\n\n    Allows discriminator to be called as: discriminator(params, a, x, ax).\n    \"\"\"\n    return self.apply(params, a, x, ax)\n</code></pre>"},{"location":"reference/stochpw/models/base/#stochpw.models.base.BaseDiscriminator.apply","title":"<code>apply(params, a, x, ax)</code>  <code>abstractmethod</code>","text":"<p>Compute discriminator logits.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>PyTree</code> <p>Model parameters (output of init_params)</p> required <code>a</code> <code>(Array, shape(batch_size, d_a) or (batch_size,))</code> <p>Treatment assignments</p> required <code>x</code> <code>(Array, shape(batch_size, d_x))</code> <p>Covariates</p> required <code>ax</code> <code>(Array, shape(batch_size, d_a * d_x))</code> <p>Pre-computed first-order interactions A \u2297 X</p> required <p>Returns:</p> Name Type Description <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Discriminator logits for p(C=1 | a, x)</p> Notes <p>The logits represent the raw output before sigmoid activation. They are used in the logistic loss: BCE(sigmoid(logits), labels).</p> Source code in <code>src/stochpw/models/base.py</code> <pre><code>@abstractmethod\ndef apply(self, params: PyTree, a: Array, x: Array, ax: Array) -&gt; Array:\n    \"\"\"\n    Compute discriminator logits.\n\n    Parameters\n    ----------\n    params : PyTree\n        Model parameters (output of init_params)\n    a : jax.Array, shape (batch_size, d_a) or (batch_size,)\n        Treatment assignments\n    x : jax.Array, shape (batch_size, d_x)\n        Covariates\n    ax : jax.Array, shape (batch_size, d_a * d_x)\n        Pre-computed first-order interactions A \u2297 X\n\n    Returns\n    -------\n    logits : jax.Array, shape (batch_size,)\n        Discriminator logits for p(C=1 | a, x)\n\n    Notes\n    -----\n    The logits represent the raw output before sigmoid activation.\n    They are used in the logistic loss: BCE(sigmoid(logits), labels).\n    \"\"\"\n    raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"reference/stochpw/models/base/#stochpw.models.base.BaseDiscriminator.init_params","title":"<code>init_params(rng_key, d_a, d_x)</code>  <code>abstractmethod</code>","text":"<p>Initialize discriminator parameters.</p> <p>Parameters:</p> Name Type Description Default <code>rng_key</code> <code>Array</code> <p>Random key for parameter initialization</p> required <code>d_a</code> <code>int</code> <p>Dimension of treatment vector</p> required <code>d_x</code> <code>int</code> <p>Dimension of covariate vector</p> required <p>Returns:</p> Name Type Description <code>params</code> <code>PyTree</code> <p>Initialized parameters (any JAX-compatible PyTree structure)</p> Source code in <code>src/stochpw/models/base.py</code> <pre><code>@abstractmethod\ndef init_params(self, rng_key: Array, d_a: int, d_x: int) -&gt; PyTree:\n    \"\"\"\n    Initialize discriminator parameters.\n\n    Parameters\n    ----------\n    rng_key : jax.Array\n        Random key for parameter initialization\n    d_a : int\n        Dimension of treatment vector\n    d_x : int\n        Dimension of covariate vector\n\n    Returns\n    -------\n    params : PyTree\n        Initialized parameters (any JAX-compatible PyTree structure)\n    \"\"\"\n    raise NotImplementedError  # pragma: no cover\n</code></pre>"},{"location":"reference/stochpw/models/linear/","title":"linear","text":"<p>Linear discriminator for permutation weighting.</p>"},{"location":"reference/stochpw/models/linear/#stochpw.models.linear.LinearDiscriminator","title":"<code>LinearDiscriminator</code>","text":"<p>               Bases: <code>BaseDiscriminator</code></p> <p>Linear discriminator using A, X, and A*X interactions.</p> <p>The discriminator computes logits as:     logit = w_a^T A + w_x^T X + w_ax^T (A \u2297 X) + b</p> <p>This allows the model to learn from: - Marginal treatment effects (w_a) - Marginal covariate effects (w_x) - Treatment-covariate interactions (w_ax)</p> <p>The explicit inclusion of A*X interactions is critical for linear models because within-batch permutation ensures P(A) and P(X) are identical in observed vs permuted batches. Only the joint distribution P(A,X) differs, which linear models need explicit interaction terms to capture.</p> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from stochpw.models import LinearDiscriminator\n&gt;&gt;&gt; import jax\n&gt;&gt;&gt;\n&gt;&gt;&gt; discriminator = LinearDiscriminator()\n&gt;&gt;&gt; params = discriminator.init_params(jax.random.PRNGKey(0), d_a=1, d_x=3)\n&gt;&gt;&gt; # params contains: w_a, w_x, w_ax, b\n</code></pre>"},{"location":"reference/stochpw/models/linear/#stochpw.models.linear.LinearDiscriminator.apply","title":"<code>apply(params, a, x, ax)</code>","text":"<p>Compute linear discriminator logits using A, X, and A*X.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters with keys 'w_a', 'w_x', 'w_ax', and 'b'</p> required <code>a</code> <code>(Array, shape(batch_size, d_a) or (batch_size,))</code> <p>Treatment assignments</p> required <code>x</code> <code>(Array, shape(batch_size, d_x))</code> <p>Covariates</p> required <code>ax</code> <code>(Array, shape(batch_size, d_a * d_x))</code> <p>Pre-computed first-order interactions A \u2297 X</p> required <p>Returns:</p> Name Type Description <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Discriminator logits for p(C=1 | a, x)</p> Source code in <code>src/stochpw/models/linear.py</code> <pre><code>@override\ndef apply(self, params: PyTree, a: Array, x: Array, ax: Array) -&gt; Array:  # type: ignore[override]\n    \"\"\"\n    Compute linear discriminator logits using A, X, and A*X.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters with keys 'w_a', 'w_x', 'w_ax', and 'b'\n    a : jax.Array, shape (batch_size, d_a) or (batch_size,)\n        Treatment assignments\n    x : jax.Array, shape (batch_size, d_x)\n        Covariates\n    ax : jax.Array, shape (batch_size, d_a * d_x)\n        Pre-computed first-order interactions A \u2297 X\n\n    Returns\n    -------\n    logits : jax.Array, shape (batch_size,)\n        Discriminator logits for p(C=1 | a, x)\n    \"\"\"\n    # Ensure a is 2D\n    if a.ndim == 1:\n        a = a.reshape(-1, 1)\n\n    # Cast params to expected dict type for type checker\n    params_dict = cast(LinearParams, params)\n\n    # Linear transformation: w_a^T A + w_x^T X + w_ax^T (A*X) + b\n    logits = (\n        jnp.dot(a, params_dict[\"w_a\"])\n        + jnp.dot(x, params_dict[\"w_x\"])\n        + jnp.dot(ax, params_dict[\"w_ax\"])\n        + params_dict[\"b\"]\n    )\n\n    return logits\n</code></pre>"},{"location":"reference/stochpw/models/linear/#stochpw.models.linear.LinearDiscriminator.init_params","title":"<code>init_params(rng_key, d_a, d_x)</code>","text":"<p>Initialize linear discriminator parameters.</p> <p>Uses Xavier/Glorot initialization for weights and small random initialization for bias.</p> <p>Parameters:</p> Name Type Description Default <code>rng_key</code> <code>Array</code> <p>Random key for parameter initialization</p> required <code>d_a</code> <code>int</code> <p>Dimension of treatment vector</p> required <code>d_x</code> <code>int</code> <p>Dimension of covariate vector</p> required <p>Returns:</p> Name Type Description <code>params</code> <code>dict</code> <p>Dictionary with keys: - 'w_a': Array of shape (d_a,) - treatment weights - 'w_x': Array of shape (d_x,) - covariate weights - 'w_ax': Array of shape (d_a * d_x,) - interaction weights - 'b': scalar - bias term</p> Source code in <code>src/stochpw/models/linear.py</code> <pre><code>@override\ndef init_params(self, rng_key: Array, d_a: int, d_x: int) -&gt; LinearParams:\n    \"\"\"\n    Initialize linear discriminator parameters.\n\n    Uses Xavier/Glorot initialization for weights and small random\n    initialization for bias.\n\n    Parameters\n    ----------\n    rng_key : jax.Array\n        Random key for parameter initialization\n    d_a : int\n        Dimension of treatment vector\n    d_x : int\n        Dimension of covariate vector\n\n    Returns\n    -------\n    params : dict\n        Dictionary with keys:\n        - 'w_a': Array of shape (d_a,) - treatment weights\n        - 'w_x': Array of shape (d_x,) - covariate weights\n        - 'w_ax': Array of shape (d_a * d_x,) - interaction weights\n        - 'b': scalar - bias term\n    \"\"\"\n    interaction_dim = d_a * d_x\n    total_dim = d_a + d_x + interaction_dim\n\n    w_key, b_key = jax.random.split(rng_key)\n\n    # Xavier/Glorot initialization\n    std = jnp.sqrt(2.0 / total_dim)\n    w_a = jax.random.normal(jax.random.fold_in(w_key, 0), (d_a,)) * std\n    w_x = jax.random.normal(jax.random.fold_in(w_key, 1), (d_x,)) * std\n    w_ax = jax.random.normal(jax.random.fold_in(w_key, 2), (interaction_dim,)) * std\n    b = jax.random.normal(b_key, ()) * 0.01\n\n    return {\"w_a\": w_a, \"w_x\": w_x, \"w_ax\": w_ax, \"b\": b}\n</code></pre>"},{"location":"reference/stochpw/models/mlp/","title":"mlp","text":"<p>Multi-layer perceptron discriminator for permutation weighting.</p>"},{"location":"reference/stochpw/models/mlp/#stochpw.models.mlp.MLPDiscriminator","title":"<code>MLPDiscriminator(hidden_dims=None, activation='relu')</code>","text":"<p>               Bases: <code>BaseDiscriminator</code></p> <p>Multi-layer perceptron (MLP) discriminator using A, X, and A*X interactions.</p> <p>The MLP processes concatenated features [A, X, A*X] through configurable hidden layers with specified activation functions, outputting a scalar logit.</p> <p>This provides more expressive power than linear discriminators for capturing complex relationships between treatments and covariates.</p> <p>Parameters:</p> Name Type Description Default <code>hidden_dims</code> <code>list[int]</code> <p>List of hidden layer sizes. Default is [64, 32]</p> <code>None</code> <code>activation</code> <code>(relu, tanh, elu, sigmoid)</code> <p>Activation function to use between layers. Default is 'relu'</p> <code>'relu'</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from stochpw.models import MLPDiscriminator\n&gt;&gt;&gt; import jax\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Default: 2-layer MLP with ReLU\n&gt;&gt;&gt; discriminator = MLPDiscriminator()\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Custom: 3-layer MLP with tanh\n&gt;&gt;&gt; discriminator = MLPDiscriminator(hidden_dims=[128, 64, 32], activation=\"tanh\")\n&gt;&gt;&gt;\n&gt;&gt;&gt; params = discriminator.init_params(jax.random.PRNGKey(0), d_a=1, d_x=3)\n</code></pre> Source code in <code>src/stochpw/models/mlp.py</code> <pre><code>def __init__(\n    self,\n    hidden_dims: list[int] | None = None,\n    activation: ActivationType = \"relu\",\n):\n    if hidden_dims is None:\n        hidden_dims = [64, 32]\n    self.hidden_dims: list[int] = hidden_dims\n    self.activation: ActivationType = activation\n    self._activation_fn: Callable[[Array], Array] = _get_activation(activation)\n    self._use_he_init: bool = activation in (\"relu\", \"elu\")\n</code></pre>"},{"location":"reference/stochpw/models/mlp/#stochpw.models.mlp.MLPDiscriminator.apply","title":"<code>apply(params, a, x, ax)</code>","text":"<p>Compute MLP discriminator logits using A, X, and A*X.</p> <p>Parameters:</p> Name Type Description Default <code>params</code> <code>dict</code> <p>Parameters with key 'layers' containing list of layer dicts</p> required <code>a</code> <code>(Array, shape(batch_size, d_a) or (batch_size,))</code> <p>Treatment assignments</p> required <code>x</code> <code>(Array, shape(batch_size, d_x))</code> <p>Covariates</p> required <code>ax</code> <code>(Array, shape(batch_size, d_a * d_x))</code> <p>Pre-computed first-order interactions A \u2297 X</p> required <p>Returns:</p> Name Type Description <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Discriminator logits for p(C=1 | a, x)</p> Source code in <code>src/stochpw/models/mlp.py</code> <pre><code>@override\ndef apply(self, params: PyTree, a: Array, x: Array, ax: Array) -&gt; Array:  # type: ignore[override]\n    \"\"\"\n    Compute MLP discriminator logits using A, X, and A*X.\n\n    Parameters\n    ----------\n    params : dict\n        Parameters with key 'layers' containing list of layer dicts\n    a : jax.Array, shape (batch_size, d_a) or (batch_size,)\n        Treatment assignments\n    x : jax.Array, shape (batch_size, d_x)\n        Covariates\n    ax : jax.Array, shape (batch_size, d_a * d_x)\n        Pre-computed first-order interactions A \u2297 X\n\n    Returns\n    -------\n    logits : jax.Array, shape (batch_size,)\n        Discriminator logits for p(C=1 | a, x)\n    \"\"\"\n    # Ensure a is 2D\n    if a.ndim == 1:\n        a = a.reshape(-1, 1)\n\n    # Concatenate all features: [A, X, A*X]\n    h = jnp.concatenate([a, x, ax], axis=-1)\n\n    # Cast params to expected dict type for type checker\n    params_dict = cast(MLPParams, params)\n\n    # Forward pass through hidden layers\n    for i, layer in enumerate(params_dict[\"layers\"]):\n        h = jnp.dot(h, layer[\"w\"]) + layer[\"b\"]\n\n        # Apply activation to all layers except the last (output layer)\n        if i &lt; len(params_dict[\"layers\"]) - 1:\n            h = self._activation_fn(h)\n\n    # Output is shape (batch_size, 1), squeeze to (batch_size,)\n    logits = h.squeeze(-1)\n\n    return logits\n</code></pre>"},{"location":"reference/stochpw/models/mlp/#stochpw.models.mlp.MLPDiscriminator.init_params","title":"<code>init_params(rng_key, d_a, d_x)</code>","text":"<p>Initialize MLP discriminator parameters.</p> <p>Uses He initialization for ReLU-family activations and Xavier initialization for tanh/sigmoid activations. Biases are initialized to zero.</p> <p>Parameters:</p> Name Type Description Default <code>rng_key</code> <code>Array</code> <p>Random key for parameter initialization</p> required <code>d_a</code> <code>int</code> <p>Dimension of treatment vector</p> required <code>d_x</code> <code>int</code> <p>Dimension of covariate vector</p> required <p>Returns:</p> Name Type Description <code>params</code> <code>dict</code> <p>Dictionary with key 'layers' containing a list of layer dicts, each with keys 'w' (weight matrix) and 'b' (bias vector)</p> Source code in <code>src/stochpw/models/mlp.py</code> <pre><code>@override\ndef init_params(self, rng_key: Array, d_a: int, d_x: int) -&gt; MLPParams:\n    \"\"\"\n    Initialize MLP discriminator parameters.\n\n    Uses He initialization for ReLU-family activations and Xavier\n    initialization for tanh/sigmoid activations. Biases are initialized to zero.\n\n    Parameters\n    ----------\n    rng_key : jax.Array\n        Random key for parameter initialization\n    d_a : int\n        Dimension of treatment vector\n    d_x : int\n        Dimension of covariate vector\n\n    Returns\n    -------\n    params : dict\n        Dictionary with key 'layers' containing a list of layer dicts,\n        each with keys 'w' (weight matrix) and 'b' (bias vector)\n    \"\"\"\n    interaction_dim = d_a * d_x\n    input_dim = d_a + d_x + interaction_dim\n\n    params = {\"layers\": []}\n    layer_dims = [input_dim] + self.hidden_dims + [1]  # Output is scalar logit\n    current_key = rng_key\n\n    for i in range(len(layer_dims) - 1):\n        current_key, layer_key = jax.random.split(current_key)\n        w_key, _b_key = jax.random.split(layer_key)\n\n        in_dim = layer_dims[i]\n        out_dim = layer_dims[i + 1]\n\n        # He initialization for ReLU, Xavier for others\n        if self._use_he_init:\n            std = jnp.sqrt(2.0 / in_dim)\n        else:\n            std = jnp.sqrt(2.0 / (in_dim + out_dim))\n\n        w = jax.random.normal(w_key, (in_dim, out_dim)) * std\n        b = jnp.zeros((out_dim,))  # Initialize biases to zero\n\n        params[\"layers\"].append({\"w\": w, \"b\": b})\n\n    return params\n</code></pre>"},{"location":"reference/stochpw/training/","title":"training","text":"<p>Training utilities for permutation weighting discriminators.</p>"},{"location":"reference/stochpw/training/#stochpw.training.brier_loss","title":"<code>brier_loss(logits, labels)</code>","text":"<p>Brier score loss for probabilistic predictions.</p> <p>The Brier score is the mean squared error between predicted probabilities and true labels. It's a proper scoring rule that encourages well-calibrated predictions.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Raw discriminator outputs</p> required <code>labels</code> <code>(Array, shape(batch_size))</code> <p>Binary labels (0 or 1)</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>Scalar loss value</p> Notes <p>The Brier score is defined as:     BS = (1/n) * \u03a3(p_i - y_i)\u00b2 where p_i = \u03c3(logits_i) is the predicted probability and y_i is the true label.</p> Source code in <code>src/stochpw/training/losses.py</code> <pre><code>@jax.jit\ndef brier_loss(logits: Array, labels: Array) -&gt; Array:\n    \"\"\"\n    Brier score loss for probabilistic predictions.\n\n    The Brier score is the mean squared error between predicted probabilities\n    and true labels. It's a proper scoring rule that encourages well-calibrated\n    predictions.\n\n    Parameters\n    ----------\n    logits : jax.Array, shape (batch_size,)\n        Raw discriminator outputs\n    labels : jax.Array, shape (batch_size,)\n        Binary labels (0 or 1)\n\n    Returns\n    -------\n    loss : float\n        Scalar loss value\n\n    Notes\n    -----\n    The Brier score is defined as:\n        BS = (1/n) * \u03a3(p_i - y_i)\u00b2\n    where p_i = \u03c3(logits_i) is the predicted probability and y_i is the true label.\n    \"\"\"\n    # Convert logits to probabilities\n    probs = jax.nn.sigmoid(logits)\n    # Brier score: mean squared error\n    return jnp.mean((probs - labels) ** 2)\n</code></pre>"},{"location":"reference/stochpw/training/#stochpw.training.create_training_batch","title":"<code>create_training_batch(X, A, batch_indices, rng_key)</code>","text":"<p>Create a training batch with observed and permuted pairs.</p> <p>Includes first-order interactions (A*X) which are critical for the discriminator to learn the association between treatment and covariates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n, d_x))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n, d_a))</code> <p>Treatments</p> required <code>batch_indices</code> <code>(Array, shape(batch_size))</code> <p>Indices for this batch</p> required <code>rng_key</code> <code>PRNGKey</code> <p>PRNG key for permutation</p> required <p>Returns:</p> Type Description <code>TrainingBatch</code> <p>Batch with concatenated observed and permuted data, including interactions</p> Source code in <code>src/stochpw/training/batch.py</code> <pre><code>def create_training_batch(\n    X: Array, A: Array, batch_indices: Array, rng_key: Array\n) -&gt; TrainingBatch:\n    \"\"\"\n    Create a training batch with observed and permuted pairs.\n\n    Includes first-order interactions (A*X) which are critical for the\n    discriminator to learn the association between treatment and covariates.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n, d_x)\n        Covariates\n    A : jax.Array, shape (n, d_a)\n        Treatments\n    batch_indices : jax.Array, shape (batch_size,)\n        Indices for this batch\n    rng_key : jax.random.PRNGKey\n        PRNG key for permutation\n\n    Returns\n    -------\n    TrainingBatch\n        Batch with concatenated observed and permuted data, including interactions\n    \"\"\"\n    # Sample observed batch\n    X_obs = X[batch_indices]\n    A_obs = A[batch_indices]\n\n    # Create permuted batch by shuffling treatments WITHIN the batch\n    # This creates the product distribution P(A)P(X) within the batch\n    batch_size = len(batch_indices)\n    X_perm = X_obs  # Same covariates (not shuffled)\n    A_perm = permute_treatment(A_obs, rng_key)  # Shuffle treatments within batch\n\n    # Compute interactions: outer product A \u2297 X\n    # For each sample, creates all A_i * X_j combinations\n    interactions_obs = jnp.einsum(\"bi,bj-&gt;bij\", A_obs, X_obs).reshape(batch_size, -1)\n    interactions_perm = jnp.einsum(\"bi,bj-&gt;bij\", A_perm, X_perm).reshape(batch_size, -1)\n\n    # Concatenate and label\n    X_batch = jnp.concatenate([X_obs, X_perm])\n    A_batch = jnp.concatenate([A_obs, A_perm])\n    interactions_batch = jnp.concatenate([interactions_obs, interactions_perm])\n    C_batch = jnp.concatenate(\n        [\n            jnp.zeros(batch_size),  # Observed: C=0\n            jnp.ones(batch_size),  # Permuted: C=1\n        ]\n    )\n\n    return TrainingBatch(X=X_batch, A=A_batch, C=C_batch, AX=interactions_batch)\n</code></pre>"},{"location":"reference/stochpw/training/#stochpw.training.entropy_penalty","title":"<code>entropy_penalty(weights, eps=1e-07)</code>","text":"<p>Entropy regularization on weights.</p> <p>Penalizes weights that diverge from uniform, encouraging smoother reweighting and better effective sample size.</p> <p>The entropy of normalized weights is computed as:     H = -sum(p * log(p)) where p = weights / sum(weights)</p> <p>We return -H (negative entropy) as a penalty, since lower entropy (more peaked weights) should be penalized.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n))</code> <p>Importance weights</p> required <code>eps</code> <code>float</code> <p>Small constant for numerical stability</p> <code>1e-7</code> <p>Returns:</p> Name Type Description <code>penalty</code> <code>Array</code> <p>Negative entropy penalty (higher = more concentrated weights)</p> Notes <ul> <li>Uniform weights have maximum entropy</li> <li>Highly concentrated weights have low entropy (high penalty)</li> <li>Encourages effective sample size close to n</li> </ul> Source code in <code>src/stochpw/training/regularization.py</code> <pre><code>def entropy_penalty(weights: Array, eps: float = 1e-7) -&gt; Array:\n    \"\"\"\n    Entropy regularization on weights.\n\n    Penalizes weights that diverge from uniform, encouraging smoother\n    reweighting and better effective sample size.\n\n    The entropy of normalized weights is computed as:\n        H = -sum(p * log(p)) where p = weights / sum(weights)\n\n    We return -H (negative entropy) as a penalty, since lower entropy\n    (more peaked weights) should be penalized.\n\n    Parameters\n    ----------\n    weights : Array, shape (n,)\n        Importance weights\n    eps : float, default=1e-7\n        Small constant for numerical stability\n\n    Returns\n    -------\n    penalty : Array\n        Negative entropy penalty (higher = more concentrated weights)\n\n    Notes\n    -----\n    - Uniform weights have maximum entropy\n    - Highly concentrated weights have low entropy (high penalty)\n    - Encourages effective sample size close to n\n    \"\"\"\n    # Normalize weights to probability distribution\n    p = weights / (jnp.sum(weights) + eps)\n    p = jnp.clip(p, eps, 1.0)  # Avoid log(0)\n\n    # Compute entropy: H = -sum(p * log(p))\n    entropy = -jnp.sum(p * jnp.log(p))\n\n    # Return negative entropy as penalty (we want to maximize entropy)\n    return -entropy\n</code></pre>"},{"location":"reference/stochpw/training/#stochpw.training.exponential_loss","title":"<code>exponential_loss(logits, labels)</code>","text":"<p>Exponential loss for density ratio estimation.</p> <p>This is a proper scoring rule that can be more robust than logistic loss for density ratio estimation. It directly optimizes the exponential of the log density ratio.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Raw discriminator outputs</p> required <code>labels</code> <code>(Array, shape(batch_size))</code> <p>Binary labels (0 or 1)</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>Scalar loss value</p> Notes <p>The exponential loss is defined as:     L = E[exp(-y * f(x))] where y \u2208 {-1, +1} and f(x) are the logits. We convert labels from {0, 1} to {-1, +1} for this formulation.</p> Source code in <code>src/stochpw/training/losses.py</code> <pre><code>@jax.jit\ndef exponential_loss(logits: Array, labels: Array) -&gt; Array:\n    \"\"\"\n    Exponential loss for density ratio estimation.\n\n    This is a proper scoring rule that can be more robust than logistic loss\n    for density ratio estimation. It directly optimizes the exponential of\n    the log density ratio.\n\n    Parameters\n    ----------\n    logits : jax.Array, shape (batch_size,)\n        Raw discriminator outputs\n    labels : jax.Array, shape (batch_size,)\n        Binary labels (0 or 1)\n\n    Returns\n    -------\n    loss : float\n        Scalar loss value\n\n    Notes\n    -----\n    The exponential loss is defined as:\n        L = E[exp(-y * f(x))]\n    where y \u2208 {-1, +1} and f(x) are the logits.\n    We convert labels from {0, 1} to {-1, +1} for this formulation.\n    \"\"\"\n    # Convert labels from {0, 1} to {-1, +1}\n    y = 2 * labels - 1\n    # Exponential loss: E[exp(-y * logits)]\n    return jnp.mean(jnp.exp(-y * logits))\n</code></pre>"},{"location":"reference/stochpw/training/#stochpw.training.fit_discriminator","title":"<code>fit_discriminator(X, A, discriminator_fn, init_params, optimizer, num_epochs, batch_size, rng_key, loss_fn=logistic_loss, regularization_fn=None, regularization_strength=0.0, early_stopping=False, patience=10, min_delta=0.0001, eps=1e-07)</code>","text":"<p>Complete training loop for discriminator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n, d_x))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n, d_a))</code> <p>Treatments</p> required <code>discriminator_fn</code> <code>Callable</code> <p>Discriminator function (params, a, x, ax) -&gt; logits</p> required <code>init_params</code> <code>dict</code> <p>Initial parameters</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>Optax optimizer</p> required <code>num_epochs</code> <code>int</code> <p>Number of training epochs</p> required <code>batch_size</code> <code>int</code> <p>Mini-batch size</p> required <code>rng_key</code> <code>PRNGKey</code> <p>Random key for reproducibility</p> required <code>loss_fn</code> <code>Callable</code> <p>Loss function (logits, labels) -&gt; loss</p> <code>logistic_loss</code> <code>regularization_fn</code> <code>Callable</code> <p>Regularization function on weights (weights) -&gt; penalty</p> <code>None</code> <code>regularization_strength</code> <code>float</code> <p>Strength of regularization penalty</p> <code>0.0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to use early stopping based on validation loss</p> <code>False</code> <code>patience</code> <code>int</code> <p>Number of epochs to wait for improvement before stopping</p> <code>10</code> <code>min_delta</code> <code>float</code> <p>Minimum change in loss to qualify as improvement</p> <code>1e-4</code> <code>eps</code> <code>float</code> <p>Numerical stability constant for weight computation</p> <code>1e-7</code> <p>Returns:</p> Name Type Description <code>params</code> <code>dict</code> <p>Fitted discriminator parameters</p> <code>history</code> <code>dict</code> <p>Training history with keys 'loss' (list of losses per epoch)</p> Source code in <code>src/stochpw/training/loop.py</code> <pre><code>def fit_discriminator(\n    X: Array,\n    A: Array,\n    discriminator_fn: Callable[[PyTree, Array, Array, Array], Array],\n    init_params: PyTree,\n    optimizer: optax.GradientTransformation,\n    num_epochs: int,\n    batch_size: int,\n    rng_key: Array,\n    loss_fn: LossFn = logistic_loss,\n    regularization_fn: Callable[[Array], Array] | None = None,\n    regularization_strength: float = 0.0,\n    early_stopping: bool = False,\n    patience: int = 10,\n    min_delta: float = 1e-4,\n    eps: float = 1e-7,\n) -&gt; tuple[PyTree, dict[str, list[float]]]:\n    \"\"\"\n    Complete training loop for discriminator.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n, d_x)\n        Covariates\n    A : jax.Array, shape (n, d_a)\n        Treatments\n    discriminator_fn : Callable\n        Discriminator function (params, a, x, ax) -&gt; logits\n    init_params : dict\n        Initial parameters\n    optimizer : optax.GradientTransformation\n        Optax optimizer\n    num_epochs : int\n        Number of training epochs\n    batch_size : int\n        Mini-batch size\n    rng_key : jax.random.PRNGKey\n        Random key for reproducibility\n    loss_fn : Callable, default=logistic_loss\n        Loss function (logits, labels) -&gt; loss\n    regularization_fn : Callable, optional\n        Regularization function on weights (weights) -&gt; penalty\n    regularization_strength : float, default=0.0\n        Strength of regularization penalty\n    early_stopping : bool, default=False\n        Whether to use early stopping based on validation loss\n    patience : int, default=10\n        Number of epochs to wait for improvement before stopping\n    min_delta : float, default=1e-4\n        Minimum change in loss to qualify as improvement\n    eps : float, default=1e-7\n        Numerical stability constant for weight computation\n\n    Returns\n    -------\n    params : dict\n        Fitted discriminator parameters\n    history : dict\n        Training history with keys 'loss' (list of losses per epoch)\n    \"\"\"\n    n = X.shape[0]\n    opt_state = optimizer.init(init_params)\n\n    # Initialize state\n    state = TrainingState(\n        params=init_params,\n        opt_state=opt_state,\n        rng_key=rng_key,\n        epoch=0,\n        history={\"loss\": []},\n    )\n\n    # Early stopping state\n    best_loss = float(\"inf\")\n    best_params = init_params\n    epochs_without_improvement = 0\n\n    for epoch in range(num_epochs):\n        # Split RNG key for this epoch\n        epoch_key, state.rng_key = jax.random.split(state.rng_key)\n\n        # Shuffle data\n        perm = jax.random.permutation(epoch_key, n)\n        X_shuffled = X[perm]\n        A_shuffled = A[perm]\n\n        # Train on batches\n        epoch_losses = []\n        num_batches = n // batch_size\n\n        for i in range(num_batches):\n            batch_key, epoch_key = jax.random.split(epoch_key)\n\n            # Get batch indices\n            start_idx = i * batch_size\n            end_idx = start_idx + batch_size\n            batch_indices = jnp.arange(start_idx, end_idx)\n\n            # Create training batch\n            batch = create_training_batch(X_shuffled, A_shuffled, batch_indices, batch_key)\n\n            # Training step\n            result = train_step(\n                state,\n                batch,\n                discriminator_fn,\n                optimizer,\n                loss_fn_type=loss_fn,\n                regularization_fn=regularization_fn,\n                regularization_strength=regularization_strength,\n                eps=eps,\n            )\n            state = result.state\n            epoch_losses.append(float(result.loss))\n\n        # Record epoch loss\n        mean_epoch_loss = jnp.mean(jnp.array(epoch_losses))\n        state.history[\"loss\"].append(float(mean_epoch_loss))\n        state.epoch = epoch + 1\n\n        # Early stopping logic\n        if early_stopping:\n            if mean_epoch_loss &lt; best_loss - min_delta:\n                best_loss = mean_epoch_loss\n                best_params = state.params\n                epochs_without_improvement = 0\n            else:\n                epochs_without_improvement += 1\n\n            if epochs_without_improvement &gt;= patience:\n                # Restore best parameters\n                state.params = best_params\n                break\n\n    return state.params, state.history\n</code></pre>"},{"location":"reference/stochpw/training/#stochpw.training.logistic_loss","title":"<code>logistic_loss(logits, labels)</code>","text":"<p>Binary cross-entropy loss for discriminator.</p> <p>Uses numerically stable log-sigmoid implementation.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Raw discriminator outputs</p> required <code>labels</code> <code>(Array, shape(batch_size))</code> <p>Binary labels (0 or 1)</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>Scalar loss value</p> Source code in <code>src/stochpw/training/losses.py</code> <pre><code>@jax.jit\ndef logistic_loss(logits: Array, labels: Array) -&gt; Array:\n    \"\"\"\n    Binary cross-entropy loss for discriminator.\n\n    Uses numerically stable log-sigmoid implementation.\n\n    Parameters\n    ----------\n    logits : jax.Array, shape (batch_size,)\n        Raw discriminator outputs\n    labels : jax.Array, shape (batch_size,)\n        Binary labels (0 or 1)\n\n    Returns\n    -------\n    loss : float\n        Scalar loss value\n    \"\"\"\n    # Use optax's stable implementation\n    return optax.sigmoid_binary_cross_entropy(logits, labels).mean()\n</code></pre>"},{"location":"reference/stochpw/training/#stochpw.training.lp_weight_penalty","title":"<code>lp_weight_penalty(weights, p=2.0)</code>","text":"<p>L_p penalty on weight deviations from uniform.</p> <p>Penalizes weights that deviate from 1, encouraging more uniform weighting. Different values of p produce different behaviors: - p=1: L1 penalty (sparse, robust to outliers) - p=2: L2 penalty (smooth, sensitive to large deviations)</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n))</code> <p>Importance weights</p> required <code>p</code> <code>float</code> <p>The power for the L_p norm (must be &gt;= 1)</p> <code>2.0</code> <p>Returns:</p> Name Type Description <code>penalty</code> <code>Array</code> <p>L_p penalty on weight deviations</p> Notes <p>Computed as sum(|weights - 1|^p), which penalizes deviation from uniform weights.</p> Source code in <code>src/stochpw/training/regularization.py</code> <pre><code>def lp_weight_penalty(weights: Array, p: float = 2.0) -&gt; Array:\n    \"\"\"\n    L_p penalty on weight deviations from uniform.\n\n    Penalizes weights that deviate from 1, encouraging more uniform weighting.\n    Different values of p produce different behaviors:\n    - p=1: L1 penalty (sparse, robust to outliers)\n    - p=2: L2 penalty (smooth, sensitive to large deviations)\n\n    Parameters\n    ----------\n    weights : Array, shape (n,)\n        Importance weights\n    p : float, default=2.0\n        The power for the L_p norm (must be &gt;= 1)\n\n    Returns\n    -------\n    penalty : Array\n        L_p penalty on weight deviations\n\n    Notes\n    -----\n    Computed as sum(|weights - 1|^p), which penalizes deviation\n    from uniform weights.\n    \"\"\"\n    deviations = jnp.abs(weights - 1.0)\n    return jnp.sum(deviations**p)\n</code></pre>"},{"location":"reference/stochpw/training/#stochpw.training.train_step","title":"<code>train_step(state, batch, discriminator_fn, optimizer, loss_fn_type=logistic_loss, regularization_fn=None, regularization_strength=0.0, eps=1e-07)</code>","text":"<p>Single training step (JIT-compiled).</p> <p>Computes loss, gradients, and updates parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>TrainingState</code> <p>Current training state</p> required <code>batch</code> <code>TrainingBatch</code> <p>Training batch</p> required <code>discriminator_fn</code> <code>Callable</code> <p>Discriminator function (params, a, x, ax) -&gt; logits</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>Optax optimizer</p> required <code>loss_fn_type</code> <code>Callable</code> <p>Loss function (logits, labels) -&gt; loss</p> <code>logistic_loss</code> <code>regularization_fn</code> <code>Callable</code> <p>Regularization function on weights (weights) -&gt; penalty</p> <code>None</code> <code>regularization_strength</code> <code>float</code> <p>Strength of regularization penalty</p> <code>0.0</code> <code>eps</code> <code>float</code> <p>Numerical stability constant for weight computation</p> <code>1e-7</code> <p>Returns:</p> Type Description <code>TrainingStepResult</code> <p>Updated state and loss value</p> Source code in <code>src/stochpw/training/loop.py</code> <pre><code>def train_step(\n    state: TrainingState,\n    batch: TrainingBatch,\n    discriminator_fn: Callable[[PyTree, Array, Array, Array], Array],\n    optimizer: optax.GradientTransformation,\n    loss_fn_type: LossFn = logistic_loss,\n    regularization_fn: Callable[[Array], Array] | None = None,\n    regularization_strength: float = 0.0,\n    eps: float = 1e-7,\n) -&gt; TrainingStepResult:\n    \"\"\"\n    Single training step (JIT-compiled).\n\n    Computes loss, gradients, and updates parameters.\n\n    Parameters\n    ----------\n    state : TrainingState\n        Current training state\n    batch : TrainingBatch\n        Training batch\n    discriminator_fn : Callable\n        Discriminator function (params, a, x, ax) -&gt; logits\n    optimizer : optax.GradientTransformation\n        Optax optimizer\n    loss_fn_type : Callable, default=logistic_loss\n        Loss function (logits, labels) -&gt; loss\n    regularization_fn : Callable, optional\n        Regularization function on weights (weights) -&gt; penalty\n    regularization_strength : float, default=0.0\n        Strength of regularization penalty\n    eps : float, default=1e-7\n        Numerical stability constant for weight computation\n\n    Returns\n    -------\n    TrainingStepResult\n        Updated state and loss value\n    \"\"\"\n\n    def loss_fn(params: PyTree) -&gt; Array:\n        logits = discriminator_fn(params, batch.A, batch.X, batch.AX)\n        loss = loss_fn_type(logits, batch.C)\n\n        # Add weight-based regularization if specified\n        if regularization_fn is not None and regularization_strength &gt; 0:\n            # Compute weights from discriminator output (only for observed data)\n            # Filter to C=0 (observed data) for weight computation\n            observed_mask = batch.C == 0\n            observed_logits = logits[observed_mask]\n            eta = jax.nn.sigmoid(observed_logits)\n            eta_clipped = jnp.clip(eta, eps, 1 - eps)\n            weights = eta_clipped / (1 - eta_clipped)\n\n            # Apply regularization on weights\n            penalty = regularization_fn(weights)\n            loss = loss + regularization_strength * penalty\n\n        return loss\n\n    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n    updates, opt_state = optimizer.update(grads, state.opt_state, state.params)\n    params = optax.apply_updates(state.params, updates)\n\n    new_state = TrainingState(\n        params=params,  # type: ignore[arg-type]\n        opt_state=opt_state,\n        rng_key=state.rng_key,\n        epoch=state.epoch,\n        history=state.history,\n    )\n\n    return TrainingStepResult(state=new_state, loss=loss)\n</code></pre>"},{"location":"reference/stochpw/training/batch/","title":"batch","text":"<p>Batch creation for permutation weighting training.</p>"},{"location":"reference/stochpw/training/batch/#stochpw.training.batch.create_training_batch","title":"<code>create_training_batch(X, A, batch_indices, rng_key)</code>","text":"<p>Create a training batch with observed and permuted pairs.</p> <p>Includes first-order interactions (A*X) which are critical for the discriminator to learn the association between treatment and covariates.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n, d_x))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n, d_a))</code> <p>Treatments</p> required <code>batch_indices</code> <code>(Array, shape(batch_size))</code> <p>Indices for this batch</p> required <code>rng_key</code> <code>PRNGKey</code> <p>PRNG key for permutation</p> required <p>Returns:</p> Type Description <code>TrainingBatch</code> <p>Batch with concatenated observed and permuted data, including interactions</p> Source code in <code>src/stochpw/training/batch.py</code> <pre><code>def create_training_batch(\n    X: Array, A: Array, batch_indices: Array, rng_key: Array\n) -&gt; TrainingBatch:\n    \"\"\"\n    Create a training batch with observed and permuted pairs.\n\n    Includes first-order interactions (A*X) which are critical for the\n    discriminator to learn the association between treatment and covariates.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n, d_x)\n        Covariates\n    A : jax.Array, shape (n, d_a)\n        Treatments\n    batch_indices : jax.Array, shape (batch_size,)\n        Indices for this batch\n    rng_key : jax.random.PRNGKey\n        PRNG key for permutation\n\n    Returns\n    -------\n    TrainingBatch\n        Batch with concatenated observed and permuted data, including interactions\n    \"\"\"\n    # Sample observed batch\n    X_obs = X[batch_indices]\n    A_obs = A[batch_indices]\n\n    # Create permuted batch by shuffling treatments WITHIN the batch\n    # This creates the product distribution P(A)P(X) within the batch\n    batch_size = len(batch_indices)\n    X_perm = X_obs  # Same covariates (not shuffled)\n    A_perm = permute_treatment(A_obs, rng_key)  # Shuffle treatments within batch\n\n    # Compute interactions: outer product A \u2297 X\n    # For each sample, creates all A_i * X_j combinations\n    interactions_obs = jnp.einsum(\"bi,bj-&gt;bij\", A_obs, X_obs).reshape(batch_size, -1)\n    interactions_perm = jnp.einsum(\"bi,bj-&gt;bij\", A_perm, X_perm).reshape(batch_size, -1)\n\n    # Concatenate and label\n    X_batch = jnp.concatenate([X_obs, X_perm])\n    A_batch = jnp.concatenate([A_obs, A_perm])\n    interactions_batch = jnp.concatenate([interactions_obs, interactions_perm])\n    C_batch = jnp.concatenate(\n        [\n            jnp.zeros(batch_size),  # Observed: C=0\n            jnp.ones(batch_size),  # Permuted: C=1\n        ]\n    )\n\n    return TrainingBatch(X=X_batch, A=A_batch, C=C_batch, AX=interactions_batch)\n</code></pre>"},{"location":"reference/stochpw/training/loop/","title":"loop","text":"<p>Training loop for permutation weighting discriminators.</p>"},{"location":"reference/stochpw/training/loop/#stochpw.training.loop.fit_discriminator","title":"<code>fit_discriminator(X, A, discriminator_fn, init_params, optimizer, num_epochs, batch_size, rng_key, loss_fn=logistic_loss, regularization_fn=None, regularization_strength=0.0, early_stopping=False, patience=10, min_delta=0.0001, eps=1e-07)</code>","text":"<p>Complete training loop for discriminator.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>(Array, shape(n, d_x))</code> <p>Covariates</p> required <code>A</code> <code>(Array, shape(n, d_a))</code> <p>Treatments</p> required <code>discriminator_fn</code> <code>Callable</code> <p>Discriminator function (params, a, x, ax) -&gt; logits</p> required <code>init_params</code> <code>dict</code> <p>Initial parameters</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>Optax optimizer</p> required <code>num_epochs</code> <code>int</code> <p>Number of training epochs</p> required <code>batch_size</code> <code>int</code> <p>Mini-batch size</p> required <code>rng_key</code> <code>PRNGKey</code> <p>Random key for reproducibility</p> required <code>loss_fn</code> <code>Callable</code> <p>Loss function (logits, labels) -&gt; loss</p> <code>logistic_loss</code> <code>regularization_fn</code> <code>Callable</code> <p>Regularization function on weights (weights) -&gt; penalty</p> <code>None</code> <code>regularization_strength</code> <code>float</code> <p>Strength of regularization penalty</p> <code>0.0</code> <code>early_stopping</code> <code>bool</code> <p>Whether to use early stopping based on validation loss</p> <code>False</code> <code>patience</code> <code>int</code> <p>Number of epochs to wait for improvement before stopping</p> <code>10</code> <code>min_delta</code> <code>float</code> <p>Minimum change in loss to qualify as improvement</p> <code>1e-4</code> <code>eps</code> <code>float</code> <p>Numerical stability constant for weight computation</p> <code>1e-7</code> <p>Returns:</p> Name Type Description <code>params</code> <code>dict</code> <p>Fitted discriminator parameters</p> <code>history</code> <code>dict</code> <p>Training history with keys 'loss' (list of losses per epoch)</p> Source code in <code>src/stochpw/training/loop.py</code> <pre><code>def fit_discriminator(\n    X: Array,\n    A: Array,\n    discriminator_fn: Callable[[PyTree, Array, Array, Array], Array],\n    init_params: PyTree,\n    optimizer: optax.GradientTransformation,\n    num_epochs: int,\n    batch_size: int,\n    rng_key: Array,\n    loss_fn: LossFn = logistic_loss,\n    regularization_fn: Callable[[Array], Array] | None = None,\n    regularization_strength: float = 0.0,\n    early_stopping: bool = False,\n    patience: int = 10,\n    min_delta: float = 1e-4,\n    eps: float = 1e-7,\n) -&gt; tuple[PyTree, dict[str, list[float]]]:\n    \"\"\"\n    Complete training loop for discriminator.\n\n    Parameters\n    ----------\n    X : jax.Array, shape (n, d_x)\n        Covariates\n    A : jax.Array, shape (n, d_a)\n        Treatments\n    discriminator_fn : Callable\n        Discriminator function (params, a, x, ax) -&gt; logits\n    init_params : dict\n        Initial parameters\n    optimizer : optax.GradientTransformation\n        Optax optimizer\n    num_epochs : int\n        Number of training epochs\n    batch_size : int\n        Mini-batch size\n    rng_key : jax.random.PRNGKey\n        Random key for reproducibility\n    loss_fn : Callable, default=logistic_loss\n        Loss function (logits, labels) -&gt; loss\n    regularization_fn : Callable, optional\n        Regularization function on weights (weights) -&gt; penalty\n    regularization_strength : float, default=0.0\n        Strength of regularization penalty\n    early_stopping : bool, default=False\n        Whether to use early stopping based on validation loss\n    patience : int, default=10\n        Number of epochs to wait for improvement before stopping\n    min_delta : float, default=1e-4\n        Minimum change in loss to qualify as improvement\n    eps : float, default=1e-7\n        Numerical stability constant for weight computation\n\n    Returns\n    -------\n    params : dict\n        Fitted discriminator parameters\n    history : dict\n        Training history with keys 'loss' (list of losses per epoch)\n    \"\"\"\n    n = X.shape[0]\n    opt_state = optimizer.init(init_params)\n\n    # Initialize state\n    state = TrainingState(\n        params=init_params,\n        opt_state=opt_state,\n        rng_key=rng_key,\n        epoch=0,\n        history={\"loss\": []},\n    )\n\n    # Early stopping state\n    best_loss = float(\"inf\")\n    best_params = init_params\n    epochs_without_improvement = 0\n\n    for epoch in range(num_epochs):\n        # Split RNG key for this epoch\n        epoch_key, state.rng_key = jax.random.split(state.rng_key)\n\n        # Shuffle data\n        perm = jax.random.permutation(epoch_key, n)\n        X_shuffled = X[perm]\n        A_shuffled = A[perm]\n\n        # Train on batches\n        epoch_losses = []\n        num_batches = n // batch_size\n\n        for i in range(num_batches):\n            batch_key, epoch_key = jax.random.split(epoch_key)\n\n            # Get batch indices\n            start_idx = i * batch_size\n            end_idx = start_idx + batch_size\n            batch_indices = jnp.arange(start_idx, end_idx)\n\n            # Create training batch\n            batch = create_training_batch(X_shuffled, A_shuffled, batch_indices, batch_key)\n\n            # Training step\n            result = train_step(\n                state,\n                batch,\n                discriminator_fn,\n                optimizer,\n                loss_fn_type=loss_fn,\n                regularization_fn=regularization_fn,\n                regularization_strength=regularization_strength,\n                eps=eps,\n            )\n            state = result.state\n            epoch_losses.append(float(result.loss))\n\n        # Record epoch loss\n        mean_epoch_loss = jnp.mean(jnp.array(epoch_losses))\n        state.history[\"loss\"].append(float(mean_epoch_loss))\n        state.epoch = epoch + 1\n\n        # Early stopping logic\n        if early_stopping:\n            if mean_epoch_loss &lt; best_loss - min_delta:\n                best_loss = mean_epoch_loss\n                best_params = state.params\n                epochs_without_improvement = 0\n            else:\n                epochs_without_improvement += 1\n\n            if epochs_without_improvement &gt;= patience:\n                # Restore best parameters\n                state.params = best_params\n                break\n\n    return state.params, state.history\n</code></pre>"},{"location":"reference/stochpw/training/loop/#stochpw.training.loop.train_step","title":"<code>train_step(state, batch, discriminator_fn, optimizer, loss_fn_type=logistic_loss, regularization_fn=None, regularization_strength=0.0, eps=1e-07)</code>","text":"<p>Single training step (JIT-compiled).</p> <p>Computes loss, gradients, and updates parameters.</p> <p>Parameters:</p> Name Type Description Default <code>state</code> <code>TrainingState</code> <p>Current training state</p> required <code>batch</code> <code>TrainingBatch</code> <p>Training batch</p> required <code>discriminator_fn</code> <code>Callable</code> <p>Discriminator function (params, a, x, ax) -&gt; logits</p> required <code>optimizer</code> <code>GradientTransformation</code> <p>Optax optimizer</p> required <code>loss_fn_type</code> <code>Callable</code> <p>Loss function (logits, labels) -&gt; loss</p> <code>logistic_loss</code> <code>regularization_fn</code> <code>Callable</code> <p>Regularization function on weights (weights) -&gt; penalty</p> <code>None</code> <code>regularization_strength</code> <code>float</code> <p>Strength of regularization penalty</p> <code>0.0</code> <code>eps</code> <code>float</code> <p>Numerical stability constant for weight computation</p> <code>1e-7</code> <p>Returns:</p> Type Description <code>TrainingStepResult</code> <p>Updated state and loss value</p> Source code in <code>src/stochpw/training/loop.py</code> <pre><code>def train_step(\n    state: TrainingState,\n    batch: TrainingBatch,\n    discriminator_fn: Callable[[PyTree, Array, Array, Array], Array],\n    optimizer: optax.GradientTransformation,\n    loss_fn_type: LossFn = logistic_loss,\n    regularization_fn: Callable[[Array], Array] | None = None,\n    regularization_strength: float = 0.0,\n    eps: float = 1e-7,\n) -&gt; TrainingStepResult:\n    \"\"\"\n    Single training step (JIT-compiled).\n\n    Computes loss, gradients, and updates parameters.\n\n    Parameters\n    ----------\n    state : TrainingState\n        Current training state\n    batch : TrainingBatch\n        Training batch\n    discriminator_fn : Callable\n        Discriminator function (params, a, x, ax) -&gt; logits\n    optimizer : optax.GradientTransformation\n        Optax optimizer\n    loss_fn_type : Callable, default=logistic_loss\n        Loss function (logits, labels) -&gt; loss\n    regularization_fn : Callable, optional\n        Regularization function on weights (weights) -&gt; penalty\n    regularization_strength : float, default=0.0\n        Strength of regularization penalty\n    eps : float, default=1e-7\n        Numerical stability constant for weight computation\n\n    Returns\n    -------\n    TrainingStepResult\n        Updated state and loss value\n    \"\"\"\n\n    def loss_fn(params: PyTree) -&gt; Array:\n        logits = discriminator_fn(params, batch.A, batch.X, batch.AX)\n        loss = loss_fn_type(logits, batch.C)\n\n        # Add weight-based regularization if specified\n        if regularization_fn is not None and regularization_strength &gt; 0:\n            # Compute weights from discriminator output (only for observed data)\n            # Filter to C=0 (observed data) for weight computation\n            observed_mask = batch.C == 0\n            observed_logits = logits[observed_mask]\n            eta = jax.nn.sigmoid(observed_logits)\n            eta_clipped = jnp.clip(eta, eps, 1 - eps)\n            weights = eta_clipped / (1 - eta_clipped)\n\n            # Apply regularization on weights\n            penalty = regularization_fn(weights)\n            loss = loss + regularization_strength * penalty\n\n        return loss\n\n    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n    updates, opt_state = optimizer.update(grads, state.opt_state, state.params)\n    params = optax.apply_updates(state.params, updates)\n\n    new_state = TrainingState(\n        params=params,  # type: ignore[arg-type]\n        opt_state=opt_state,\n        rng_key=state.rng_key,\n        epoch=state.epoch,\n        history=state.history,\n    )\n\n    return TrainingStepResult(state=new_state, loss=loss)\n</code></pre>"},{"location":"reference/stochpw/training/losses/","title":"losses","text":"<p>Loss functions for discriminator training.</p>"},{"location":"reference/stochpw/training/losses/#stochpw.training.losses.brier_loss","title":"<code>brier_loss(logits, labels)</code>","text":"<p>Brier score loss for probabilistic predictions.</p> <p>The Brier score is the mean squared error between predicted probabilities and true labels. It's a proper scoring rule that encourages well-calibrated predictions.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Raw discriminator outputs</p> required <code>labels</code> <code>(Array, shape(batch_size))</code> <p>Binary labels (0 or 1)</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>Scalar loss value</p> Notes <p>The Brier score is defined as:     BS = (1/n) * \u03a3(p_i - y_i)\u00b2 where p_i = \u03c3(logits_i) is the predicted probability and y_i is the true label.</p> Source code in <code>src/stochpw/training/losses.py</code> <pre><code>@jax.jit\ndef brier_loss(logits: Array, labels: Array) -&gt; Array:\n    \"\"\"\n    Brier score loss for probabilistic predictions.\n\n    The Brier score is the mean squared error between predicted probabilities\n    and true labels. It's a proper scoring rule that encourages well-calibrated\n    predictions.\n\n    Parameters\n    ----------\n    logits : jax.Array, shape (batch_size,)\n        Raw discriminator outputs\n    labels : jax.Array, shape (batch_size,)\n        Binary labels (0 or 1)\n\n    Returns\n    -------\n    loss : float\n        Scalar loss value\n\n    Notes\n    -----\n    The Brier score is defined as:\n        BS = (1/n) * \u03a3(p_i - y_i)\u00b2\n    where p_i = \u03c3(logits_i) is the predicted probability and y_i is the true label.\n    \"\"\"\n    # Convert logits to probabilities\n    probs = jax.nn.sigmoid(logits)\n    # Brier score: mean squared error\n    return jnp.mean((probs - labels) ** 2)\n</code></pre>"},{"location":"reference/stochpw/training/losses/#stochpw.training.losses.exponential_loss","title":"<code>exponential_loss(logits, labels)</code>","text":"<p>Exponential loss for density ratio estimation.</p> <p>This is a proper scoring rule that can be more robust than logistic loss for density ratio estimation. It directly optimizes the exponential of the log density ratio.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Raw discriminator outputs</p> required <code>labels</code> <code>(Array, shape(batch_size))</code> <p>Binary labels (0 or 1)</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>Scalar loss value</p> Notes <p>The exponential loss is defined as:     L = E[exp(-y * f(x))] where y \u2208 {-1, +1} and f(x) are the logits. We convert labels from {0, 1} to {-1, +1} for this formulation.</p> Source code in <code>src/stochpw/training/losses.py</code> <pre><code>@jax.jit\ndef exponential_loss(logits: Array, labels: Array) -&gt; Array:\n    \"\"\"\n    Exponential loss for density ratio estimation.\n\n    This is a proper scoring rule that can be more robust than logistic loss\n    for density ratio estimation. It directly optimizes the exponential of\n    the log density ratio.\n\n    Parameters\n    ----------\n    logits : jax.Array, shape (batch_size,)\n        Raw discriminator outputs\n    labels : jax.Array, shape (batch_size,)\n        Binary labels (0 or 1)\n\n    Returns\n    -------\n    loss : float\n        Scalar loss value\n\n    Notes\n    -----\n    The exponential loss is defined as:\n        L = E[exp(-y * f(x))]\n    where y \u2208 {-1, +1} and f(x) are the logits.\n    We convert labels from {0, 1} to {-1, +1} for this formulation.\n    \"\"\"\n    # Convert labels from {0, 1} to {-1, +1}\n    y = 2 * labels - 1\n    # Exponential loss: E[exp(-y * logits)]\n    return jnp.mean(jnp.exp(-y * logits))\n</code></pre>"},{"location":"reference/stochpw/training/losses/#stochpw.training.losses.logistic_loss","title":"<code>logistic_loss(logits, labels)</code>","text":"<p>Binary cross-entropy loss for discriminator.</p> <p>Uses numerically stable log-sigmoid implementation.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>(Array, shape(batch_size))</code> <p>Raw discriminator outputs</p> required <code>labels</code> <code>(Array, shape(batch_size))</code> <p>Binary labels (0 or 1)</p> required <p>Returns:</p> Name Type Description <code>loss</code> <code>float</code> <p>Scalar loss value</p> Source code in <code>src/stochpw/training/losses.py</code> <pre><code>@jax.jit\ndef logistic_loss(logits: Array, labels: Array) -&gt; Array:\n    \"\"\"\n    Binary cross-entropy loss for discriminator.\n\n    Uses numerically stable log-sigmoid implementation.\n\n    Parameters\n    ----------\n    logits : jax.Array, shape (batch_size,)\n        Raw discriminator outputs\n    labels : jax.Array, shape (batch_size,)\n        Binary labels (0 or 1)\n\n    Returns\n    -------\n    loss : float\n        Scalar loss value\n    \"\"\"\n    # Use optax's stable implementation\n    return optax.sigmoid_binary_cross_entropy(logits, labels).mean()\n</code></pre>"},{"location":"reference/stochpw/training/regularization/","title":"regularization","text":"<p>Regularization functions for permutation weighting.</p> <p>This module provides regularization functions that operate on the output weights rather than model parameters, encouraging desirable properties of the reweighting scheme.</p>"},{"location":"reference/stochpw/training/regularization/#stochpw.training.regularization.entropy_penalty","title":"<code>entropy_penalty(weights, eps=1e-07)</code>","text":"<p>Entropy regularization on weights.</p> <p>Penalizes weights that diverge from uniform, encouraging smoother reweighting and better effective sample size.</p> <p>The entropy of normalized weights is computed as:     H = -sum(p * log(p)) where p = weights / sum(weights)</p> <p>We return -H (negative entropy) as a penalty, since lower entropy (more peaked weights) should be penalized.</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n))</code> <p>Importance weights</p> required <code>eps</code> <code>float</code> <p>Small constant for numerical stability</p> <code>1e-7</code> <p>Returns:</p> Name Type Description <code>penalty</code> <code>Array</code> <p>Negative entropy penalty (higher = more concentrated weights)</p> Notes <ul> <li>Uniform weights have maximum entropy</li> <li>Highly concentrated weights have low entropy (high penalty)</li> <li>Encourages effective sample size close to n</li> </ul> Source code in <code>src/stochpw/training/regularization.py</code> <pre><code>def entropy_penalty(weights: Array, eps: float = 1e-7) -&gt; Array:\n    \"\"\"\n    Entropy regularization on weights.\n\n    Penalizes weights that diverge from uniform, encouraging smoother\n    reweighting and better effective sample size.\n\n    The entropy of normalized weights is computed as:\n        H = -sum(p * log(p)) where p = weights / sum(weights)\n\n    We return -H (negative entropy) as a penalty, since lower entropy\n    (more peaked weights) should be penalized.\n\n    Parameters\n    ----------\n    weights : Array, shape (n,)\n        Importance weights\n    eps : float, default=1e-7\n        Small constant for numerical stability\n\n    Returns\n    -------\n    penalty : Array\n        Negative entropy penalty (higher = more concentrated weights)\n\n    Notes\n    -----\n    - Uniform weights have maximum entropy\n    - Highly concentrated weights have low entropy (high penalty)\n    - Encourages effective sample size close to n\n    \"\"\"\n    # Normalize weights to probability distribution\n    p = weights / (jnp.sum(weights) + eps)\n    p = jnp.clip(p, eps, 1.0)  # Avoid log(0)\n\n    # Compute entropy: H = -sum(p * log(p))\n    entropy = -jnp.sum(p * jnp.log(p))\n\n    # Return negative entropy as penalty (we want to maximize entropy)\n    return -entropy\n</code></pre>"},{"location":"reference/stochpw/training/regularization/#stochpw.training.regularization.lp_weight_penalty","title":"<code>lp_weight_penalty(weights, p=2.0)</code>","text":"<p>L_p penalty on weight deviations from uniform.</p> <p>Penalizes weights that deviate from 1, encouraging more uniform weighting. Different values of p produce different behaviors: - p=1: L1 penalty (sparse, robust to outliers) - p=2: L2 penalty (smooth, sensitive to large deviations)</p> <p>Parameters:</p> Name Type Description Default <code>weights</code> <code>(Array, shape(n))</code> <p>Importance weights</p> required <code>p</code> <code>float</code> <p>The power for the L_p norm (must be &gt;= 1)</p> <code>2.0</code> <p>Returns:</p> Name Type Description <code>penalty</code> <code>Array</code> <p>L_p penalty on weight deviations</p> Notes <p>Computed as sum(|weights - 1|^p), which penalizes deviation from uniform weights.</p> Source code in <code>src/stochpw/training/regularization.py</code> <pre><code>def lp_weight_penalty(weights: Array, p: float = 2.0) -&gt; Array:\n    \"\"\"\n    L_p penalty on weight deviations from uniform.\n\n    Penalizes weights that deviate from 1, encouraging more uniform weighting.\n    Different values of p produce different behaviors:\n    - p=1: L1 penalty (sparse, robust to outliers)\n    - p=2: L2 penalty (smooth, sensitive to large deviations)\n\n    Parameters\n    ----------\n    weights : Array, shape (n,)\n        Importance weights\n    p : float, default=2.0\n        The power for the L_p norm (must be &gt;= 1)\n\n    Returns\n    -------\n    penalty : Array\n        L_p penalty on weight deviations\n\n    Notes\n    -----\n    Computed as sum(|weights - 1|^p), which penalizes deviation\n    from uniform weights.\n    \"\"\"\n    deviations = jnp.abs(weights - 1.0)\n    return jnp.sum(deviations**p)\n</code></pre>"}]}